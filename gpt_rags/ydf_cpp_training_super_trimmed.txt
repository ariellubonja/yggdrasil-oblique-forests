# ===== README.md (trimmed to training sections) =====

**YDF** (Yggdrasil Decision Forests) is a library to train, evaluate, interpret,
and serve Random Forest, Gradient Boosted Decision Trees, CART and Isolation
forest models.

See the [documentation](https://ydf.readthedocs.org/) for more information on
YDF.

train_ds = pd.read_csv(ds_path + "adult_train.csv")
test_ds = pd.read_csv(ds_path + "adult_test.csv")

# Train a Gradient Boosted Trees model
model = ydf.GradientBoostedTreesLearner(label="income").train(train_ds)

# Look at a model (input features, training logs, structure, etc.)
model.describe()

auto dataset_path = "csv:train.csv";

// List columns in training dataset
DataSpecification spec;
CreateDataSpec(dataset_path, false, {}, &spec);

// Create a training configuration
TrainingConfig train_config;
train_config.set_learner("RANDOM_FOREST");
train_config.set_task(Task::CLASSIFICATION);
train_config.set_label("my_label");

// Train model
std::unique_ptr<AbstractLearner> learner;
GetLearner(train_config, &learner);
auto model = learner->Train(dataset_path, spec);

// Export model
SaveModel("my_model", model.get());
```

(based on [examples/beginner.cc](examples/beginner.cc))

[Getting Started tutorial ðŸ§­](https://ydf.readthedocs.io/en/stable/tutorial/getting_started/).




// ===== FILE: yggdrasil_decision_forests/learner/abstract_learner.cc =====






absl::Status AbstractLearner::LinkTrainingConfig(
  // Label.
  // Anomaly detection is the only task that can have or not have labels.

  // CV group.

  // Ranking group.

  // Uplift treatment.

  // Weights.

  // List the model input features.

  // Remove the label from the input features.
  auto it_label_result =

  // Remove the rank group from the input features.
    auto it_ranking_group_result =

  // Remove the uplift treatment from the input features.
    auto it_uplift_treatment_result =

  // Remove the cv group from the input features.
    auto it_cv_group_result =

  // Weights
    auto it_weight_result =

  // Remove features that only contain missing values.
    bool is_fully_missing =

    // Not fully missing values, but that are like missing statistically.



  // Index numerical features

  // Allocate per-attributes array

  // Monotonicity constraints
        // Build error message.



// Non status; dataset in memory.

// Non status; dataset on disk.

// API; dataset in memory.




// Impl; dataset in memory.
  // This method should always be implemented by learners.

// API; dataset on disk.


      auto model, TrainWithStatusImpl(typed_path, data_spec, typed_valid_path));



// Impl; dataset on disk.
  // If training on disk is not implemented, we load the dataset and use
  // training from memory.

  // List the columns used for the training.
  // Only these columns will be loaded.
  auto dataset_loading_config = OptimalDatasetLoadingConfig(link_config);




absl::Status CheckGenericHyperParameterSpecification(



absl::Status AbstractLearner::CheckConfiguration(

    // Note: ANOMALY_DETECTION is the only task that does not need a label.


  // Check the type of the label column.
  // Check the label don't contains NaN.


absl::Status AbstractLearner::SetHyperParameters(

absl::Status AbstractLearner::SetHyperParametersImpl(












  // Make sure the computation distribution is supported.

  // Initialize the folds.

  // Get the label column specification.

  // Protects "aggregated_evaluation".
  absl::Status status_train_and_evaluate;

  // Trains and evaluates a single model on the "fold_idx".

        // Extract the training and testing dataset.
        // Train a model.
        auto model = learner.TrainWithStatus(training_dataset).value();
        // Evaluate the model.
        auto status_append = model->AppendEvaluation(
        // Aggregate the evaluations.



void InitializeModelWithAbstractTrainingConfig(





void InitializeModelMetadataWithAbstractTrainingConfig(

absl::Status AbstractLearner::CheckCapabilities() const {


  // Maximum training duration.

  // Maximum model size.

  // Monotonic constraints



absl::Status CopyProblemDefinition(const proto::TrainingConfig& src,










  // Filter the examples with zero weight.


  // Owner
    auto opt_username = utils::UserName();

  // Date

  // UID

  // Framework


// ===== FILE: yggdrasil_decision_forests/learner/abstract_learner.h =====


// Abstract classes for learners a.k.a model trainers.





class AbstractLearner {


  // Trains a model using the dataset stored on disk at the path "typed_path".
  //
  // A learner might use distributed training, or load the dataset in memory and
  // fallback to in-memory training.
  //
  // A typed path is a dataset with a format prefix. prefix format. For example,
  // "csv:/tmp/dataset.csv". The path supports sharding, globbing and comma
  // separation. See the "Dataset path and format" section of the user manual
  // for more details: go/ydf_documentation/user_manual.md#dataset-path-and-format
  //
  // Algorithms with the "use_validation_dataset" capability use a validation
  // dataset for training. If "typed_valid_path" is provided, it will be used
  // for validation. If "typed_valid_path" is not provided, a validation dataset
  // will be extracted from the training dataset. If the algorithm does not have
  // the "use_validation_dataset" capability, "typed_valid_path" is ignored.
  //
  // This method is virtual for historical reasons with external codebase.
  // Internally or in any new code, this method should not be overridden.

  // Trains a model using the dataset stored on memory .
  //
  // Algorithms with the "use_validation_dataset" capability use a validation
  // dataset for training. If "valid_dataset" is provided, it will be used
  // for validation. If "valid_dataset" is not provided, a validation dataset
  // will be extracted from the training dataset. If the algorithm does not have
  // the "use_validation_dataset" capability, "valid_dataset" is ignored.
  //
  // This method is virtual for historical reasons with external codebase.
  // Internally or in any new code, this method should not be overridden.

  // [Deprecated] Similar as TrainWithStatus, but fails (CHECK) in case of
  // error.

  // [Deprecated] Similar as TrainWithStatus, but fails (CHECK) in case of
  // error.

  // Obtains the linked training configuration i.e. match feature names to
  // feature idxs from a training configuration and a dataspec.
  //
  // Input feature with only missing values are removed with a warning message.

  // Accessor to the training configuration. Contains the definition of the task
  // (e.g. input features, label, weights) as well as the hyper parameters of
  // the learning.

  // Update the training config hyper parameters with a generic hyper parameter
  // definition. Hyper parameter fields non defined in "generic_hyper_params"
  // are not modified.

  // The function for learners to override when setting hyper-parameters.

  // Get a description of the generic hyper-parameters supported by the learner.

  // Returns a list of hyper-parameter sets that outperforms the default
  // hyper-parameters (either generally or in specific scenarios). Like default
  // hyper-parameters, existing pre-defined hyper-parameters cannot change.
  // However new versions can be added with a same name.

  // Pre-defined space of hyper-parameters to be automatically optimized.
  // Returns a failing status if the learner does not provide a pre-defined
  // space of hyper-parameter to optimize.

  // Accessor to the deployment configuration.

  void set_log_directory(absl::string_view log_path) {

  // Detects configuration errors and warnings.

  // Gets the capabilities of the learning algorithm.

  // Checks if the training config is compatible with the learner capabilities.
  absl::Status CheckCapabilities() const;

  // Register a trigger to stop the training. Later if a trigger is set to true
  // at any time during training (e.g. during an interrupt caused by user
  // control+C one may want to set it to true), the training algorithm will
  // gracefully interrupt. This is done by polling, so expect a little latency
  // to respond to the trigger setting.
  // If the training is interrupted, the output model is valid but partially (or
  // not at all) trained. If trigger==nullptr (default behavior), the training
  // cannot be stopped, and will continue until finished.
  void set_stop_training_trigger(std::atomic<bool>* trigger) {

  // Implementation of the "TrainWithStatus" function. Callers should call
  // "TrainWithStatus". Learners should implement "TrainWithStatusImpl" (either
  // both versions, to support both distributed and in-memory training) or only
  // the in-memory version below.

  // Implementation of the "TrainWithStatus" function. Callers should call
  // "TrainWithStatus". Learners should implement this "TrainWithStatusImpl"
  // function.
  //
  // This method is not a pure virtual function for historical reasons.

  // Training configuration. Contains the hyper parameters of the learner.

  // Deployment configuration. Defines the computing resources to use for the
  // training.

  // If non empty, this directory can be used by the learner to export any
  // training log data. Such data can include (non exhaustive) texts, tables and
  // plots. The learner is not guarantied to populate the log directory. It is
  // the responsibility of the learner to create this directory if it does not
  // exist.

  // If set, the training should stop is "*stop_training_trigger_" is true.
  // If the training is interrupted, the output model is valid but partially (or
  // not at all) trained. If flag==nullptr (default behavior), the flag is
  // ignored.



// Generic hyper parameter names.

// The values kTrue and kFalse represent boolean values of a categorical
// hyper-parameter.
// TODO: Add direct support for boolean hyper-parameter. Note: users are
// generally not using those hyper-parameter directly. Instead generic
// hyper-parameter are used in several sub-modules (automatic generation of
// documentation, generation of python code, hyper-parameter tuning, etc.).

// Check a set of hyper parameter against an hyper parameter specification.
absl::Status CheckGenericHyperParameterSpecification(

// Evaluates a learner using a fold generator.
//
// Note: To evaluate a pre-trained model, see "AbstractModel::Evaluate".
//
// For each fold group, a model is trained and evaluate. The final evaluation is
// the average evaluation of all the models.
//
// If the fold generator contains multiple fold groups (e.g. cross-validation),
// the different models will be trained and evaluated in parallel using the
// "deployment_evaluation" specification. By default, 6 models will be evaluated
// in parallel on the local machine.
//
// "deployment_evaluation" specifies the computing resources for the evaluation.
// It does not impact the computing resources training (contained in
// "learner.deployment()").
//
// If your learner is already parallelized, training and evaluating a single
// models at a time might be best i.e. deployment_evaluation = { .num_threads =
// 1 }.



// Initialize the abstract model fields
void InitializeModelWithAbstractTrainingConfig(

// Copies or set with the default values the metadata of the model.
void InitializeModelMetadataWithAbstractTrainingConfig(

// Copies the part of the training configuration related to the problem
// definition. Fails if the "dst" already existing configuration is not
// compatible or contradictory.
absl::Status CopyProblemDefinition(const proto::TrainingConfig& src,

// Create a dataset loading configuration adapted to the training configuration
// link. Skill unused features and examples with zero weights.

// Returns a copy of the metadata from the training config with appropriate
// default values if no metadata is included.



// ===== FILE: yggdrasil_decision_forests/learner/abstract_learner_test.cc =====

















  // "B" and "C" only have missing values.
























// Creates a classification model always returning class "1". Create a dataset
// with 4 x 1000 observations: 2 labels of class "2" and two labels of class "1"
// x 1000. Run a 10 fold cross-validation.
  class FakeClassificationModel : public FakeModel {

    void Predict(const dataset::VerticalDataset& dataset,

  class FakeLearner : public AbstractLearner {

      auto model = std::make_unique<FakeClassificationModel>();






  class FakeLearner : public AbstractLearner {


    void set_tested_capability(bool value) {




  // Set the hparam without actual support for it.

  // Add support for the hparam.

  // Check the hparam value.

  // Remove the hparam value i.e. return to the default logic.


// ===== FILE: yggdrasil_decision_forests/learner/export_doc.cc =====






// Creates part of the markdown table showing the available generic hyper
// parameters.
//
// Creates the last two columns ("default value" and "range") for a numerical
// hyper-parameter (i.e. integer or real).
template <typename T>


// Sanitize markdown text to be injected into a markdown table.




  // Index of "v" in "ordering". Returns "ordering.size()" is "v" is not in
  // "ordering".
  auto get_index_in_ordering = [&](const std::string& v) {

  // Sort the learners.

        auto sub_content,


  // Check if the set of generic hyper-parameters is empty.
  // Note: "kHParamMaximumTrainingDurationSeconds" is added to most learners
  // automatically.

  bool no_generic_hparams = true;

    // The name of the proto filename, source filename, c++ name_space,
    // learner_key, and extension are all "learner_key".
    // Caveat: The learner_key uses upper cases.


  // Introduction to the learner.

  // List of all the protos.












  // Sort the field alphabetically.

  // For each hyper-parameter.
    // Hyper parameter name.






// ===== FILE: yggdrasil_decision_forests/learner/export_doc.h =====


// Export the learner's documentation (e.g. hparams) to various documentation
// formats.





// Signature of a function to create the url to the code source code.

// Creates the Markdown documentation of the linked learners.
//
// Args:
//   learners: Name of the learners to export. Those learners need to be linked.
//     Use "AllRegisteredLearners()" for the list of all linked learners.
//   order: Order in which to display the learners. Remaining learners are
//     ordered alphabetically after the ones specified in "order".

// Create the Markdown documentation for a set of hyper-parameters.



// ===== FILE: yggdrasil_decision_forests/learner/export_doc_main.cc =====


// Generic the Markdown documentation of the learning algorithms.
// The result of this binary should be exported to: documentation/learners.md
//



// Converts a source file path (relative to the ydf directory) and search
// keyword into an url. When opening this url, the user expects to see the
// source file content.
    // Local github path.
    // Use absolute github paths.

int main(int argc, char** argv) {
  auto content_or =

// ===== FILE: yggdrasil_decision_forests/learner/export_doc_test.cc =====






class FakeLearner1 : public AbstractLearner {




class FakeLearner2 : public AbstractLearner {



class FakeLearner3 : public AbstractLearner {



  auto content =


// ===== FILE: yggdrasil_decision_forests/learner/learner_library.cc =====








absl::Status GetLearner(const proto::TrainingConfig& train_config,

  auto effective_train_config = train_config;



// ===== FILE: yggdrasil_decision_forests/learner/learner_library.h =====


// Abstract classes for model and model builder (called learner).




// Get a learner.
absl::Status GetLearner(const proto::TrainingConfig& train_config,

// Equivalent to "GetLearner" above.

// Returns the list of all registered learners.



// ===== FILE: yggdrasil_decision_forests/learner/cart/cart.cc =====








// Generates the indices for the training and validation datasets.
void GenTrainAndValidIndices(const float validation_ratio,

absl::Status SetDefaultHyperParameters(
  // The basic definition of CART does not have any attribute sampling.

  // There is no need for pre-sorting.

  // Set the default generic decision tree hyper-parameter which might have not
  // be set by this CART specific function.




absl::Status CartLearner::SetHyperParametersImpl(

  // Decision tree specific hyper-parameters.












  // Assemble and check the training configuration.
  auto config = training_config();



  // Initialize the model.
  auto mdl = std::make_unique<model::random_forest::RandomForestModel>();

  // Outputs probabilities.




  // Select the example for training and for validation.
  // The validation dataset is used for tree pruning and for self reported
  // validation.

  // Note: "effective_valid_dataset" will be either null, point to
  // "valid_dataset" or point to "buffer_valid_dataset".
    // The user provided a validation dataset.
    // Uses all the training examples for growing the tree.
    // Extract a validation dataset from the training dataset.
      // No validation examples.

  // Timeout in the tree training.

  // Trains the tree.


    auto valid_examples_in_valid_ds_rb =

    // "valid_examples_in_valid_ds" always contains all the examples in
    // "effective_valid_dataset".
    // Prune the tree.

    // Evaluate model on the validation dataset.

    // Store the validation evaluation in the RF OOB evaluation field.

  // Cache the structural variable importance in the model data.



// Prunes the node of a tree that would lead to an improvement of the "Score"
// computed by the "ScoreAccumulator".
//
// Template:
//   ScoreAccumulator: Class accumulating label+predictions and outputting a
//     score. Must implement a `.Add()` method to accumulate individual
//     examples, and a `.Score()` that returns the aggregated score.
//   Label: Representation of the label.
//   Prediction: Representation of the predictions.
//   Secondary: Representation of an optional secondary label. Must have
//     a default constructor, and is only passed to the ScoreAccumulator.Add
//     method.
//
// Args:
//   dataset: Validation dataset.
//   weights: Example weights.
//   labels: Example labels.
//   secondary_labels: Examples secondary labels. Empty if secondary labels are
//     not used. Secondary labels are an optional second column that store label
//     values. It is used in task where the label is stored on two columns (e.g.
//     ranking, uplifting). It is only up to "ScoreAccumulator" to use (or not)
//     secondary labels.
//   example_idxs: Indices of the examples to evaluate.
//   predictions: Example predictions.
//   node: Current node.
//
template <typename ScoreAccumulator, typename Label, typename Prediction,
absl::Status PruneNode(
    // Compute the predictions and return.
    // Leaf cannot be pruned "more".

  // Note: This copy ensures the values are tested in the same order.
  // This is a bit less efficiently memory-wise, but make the computation of the
  // evaluation more local (i.e., faster).

  // Maybe prune the children.



  // Compare the quality of the current node as a leaf or as a non-leaf.

    // The node is better as a non-leaf than as a leaf: Don't prune the node.

  // Turn the node into a leaf and prune the children.

  // Update the predictions with this node as a leaf.

absl::Status PruneTreeClassification(

  class AccuracyAccumulator {

    void Add(const int32_t label, const bool ignored, const int32_t prediction,

    // Accuracy.
    float Score() { return good_predictions_ / predictions_; }



absl::Status PruneTreeRegression(

  class NegMSEAccumulator {

    void Add(const float label, const bool ignored, const float prediction,

    // -MSE.
    float Score() { return -sum_squared_error_ / sum_weights_; }



absl::Status PruneTreeUpliftCategorical(


  class UpliftAccumulator {



    void Add(const int32_t outcome, const int32_t treatment,

    float Score() {
        // The leaf does not contain at least two treatments.





absl::Status PruneTree(







// ===== FILE: yggdrasil_decision_forests/learner/cart/cart.h =====


// Classification And Regression Tree (CART) is a family of learning algorithm
// for single decision trees.
//
// CART models are less accurate but more easily interpretable than Random
// Forests or Gradient Boosted Decision Trees.
//




class CartLearner : public AbstractLearner {

  // Unique identifier of the learning algorithm.

  // Generic hyper parameter names.


  // Sets the hyper-parameters of the learning algorithm from "generic hparams".
  absl::Status SetHyperParametersImpl(

  // Gets the available generic hparams.

  // Pre-defined hyper-parameter space for hyper-parameter optimization.

  // Gets a description of what the learning algorithm can do.



// Prune a decision tree using a validation dataset.
//
// For each non-leaf node, test if the validation metric (accuracy or rmse)
// would be better if the node was a leaf. If true, the node is turned into a
// leaf and the children nodes are pruned.
absl::Status PruneTree(




// ===== FILE: yggdrasil_decision_forests/learner/cart/cart_test.cc =====







void SetExpectedSortingStrategy(Internal::SortingStrategy expected,

class CartOnAdult : public utils::TrainAndTestTester {
  void SetUp() override {

    // CART only uses IN_NODE sorting.

  // Random Forest has an accuracy of ~0.860.

  // Show the tree structure.


// Similar as "CartOnAdult", but use the pre-split train and test dataset (
// instead of generating the splits).
class CartOnAdultPreSplit : public utils::TrainAndTestTester {
  void SetUp() override {

    // CART only use IN_NODE sorting.

  // Random Forest has an accuracy of ~0.860.

class CartOnAbalone : public utils::TrainAndTestTester {
  void SetUp() override {

    // CART only use IN_NODE sorting.

  // Random Forest has an rmse of ~2.10.
  // The default RMSE (always retuning the label mean) is ~3.204.



class CartOnIris : public utils::TrainAndTestTester {
  void SetUp() override {

    // CART only uses IN_NODE sorting.

  // Random Forest has an accuracy of ~0.947.



  // This test should have no variance.


class CartPruningTest : public ::testing::Test {
  void SetUp() override {








  // The classification tree is as follow:
  // (a>0.5; pred=1)
  //   â”‚
  //   â”œâ”€[pred=1]
  //   â””â”€(a>1.5; pred=2)
  //         â”‚
  //         â”œâ”€[pred=1]
  //         â””â”€[pred=2]
  //
  // The dataset is:
  //   a  l
  //   0  1
  //   1  2
  //   2  2
  //
  // The node "a>1.5" should be pruned.


  auto example_idxs_rb = decision_tree::SelectedExamplesRollingBuffer::Create(

  // Note: There is only one way to prune the tree and make it having 3 nodes.

class CartOnSimPTE : public utils::TrainAndTestTester {
  void SetUp() override {



    // CART only use IN_NODE sorting.

  // Note: A Qini of ~0.1 is expected with a simple Random Forest model.






// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/decision_tree_test.cc =====








// Margin of error for numerical tests.


struct FakeLabelStats : LabelStats {};

// A fake consumer that persistently fails to find a valid attribute.

// A fake consumer that sets the split score to 10 times the request index.

// A fake consumer that fails if the attribute_idx is even, and set the score to
// 10 times the attribute_idx otherwise.
  auto response = SplitterWorkResponse(






















// Tests the case of two distinct attribute values with the mean of the two
// values being equal to the first value because of float limited precision:
// i.e. a == (a+b)/2 with a<b.

  // Small basic dataset.

  // Configuration.

  // Compute the label distribution.


  // The expected element map is "0011".
  // R> entropy(c(3,3)) - 4/6*entropy(c(3,1)) - 2/6*entropy(c(2)) = 0.3182571

  // Since all the attributes have the same value, there are no valid splits.


  // Small basic dataset.

  // Configuration.

  // Compute the label distribution.


  // The expected element map is "0011".
  // R> entropy(c(4,5)) - 3/9*entropy(c(3)) - 6/9*entropy(c(1,5)) = 0.3865874


  // Since all the attributes have the same value, there are no valid splits.


  // Configuration.

      // Generate random attribute and label values.


      // Look for the best condition.

// The random categorical splitter is non deterministic (unless the see is
// fixed). This test (1) ensure that the random categorical splitter runs, (2)
// that the categorical splitter return splits, (3) and that the categorical
// splitter splits are always worst that the CART splitter (in the case of
// binary classification).



  // Generate random attribute and label values.


    // Look for the best condition.



    // The smaller the element index, the larger the ratio of positive label.




  int num_attributes_to_test;










  // We test that a condition was created on attribute 0 or 1 (non
  // deterministic). If the attribute 0 is selected, the condition should be
  // x>=1. If the second attribute is selected, the condition should be "x \in
  // {1}" or (exclusive) "x \in {2}" (non deterministic).

  // Small basic dataset.


  // Configuration.

  // Compute the label distribution.


  // R> entropy(c(3,3)) - 4/6*entropy(c(3,1)) - 2/6*entropy(c(2)) = 0.3182571


template <typename TestParam>
class FindBestSplitTest;

template <bool weighted>
class FindBestSplitTest<std::integral_constant<bool, weighted>>


  // Small basic dataset.


  // Configuration.

  // Compute the label distribution.


    // Same split as unweighted, score manually verified with numpy.
    // TODO: The "better split" is just an artifact of incorrect
    // rounding.
    // > varb = function (x) { mean(x*x) - mean(x)^2 }
    // > varb(c(1,1,0,0,1,0)) - 4/6*varb(c(1,1,1,0)) - 2/6*varb(c(0,0))


  // Compute the label distribution.







    // Same split as unweighted, score manually verified with numpy.
    // > varb = function (x) { mean(x*x) - mean(x)^2 }
    // > varb(c(1,1,0,0,1,0)) - 4/6*varb(c(1,1,1,0)) - 2/6*varb(c(0,0))



template <typename TestParam>
class FindBestSplitWithDuplicatesTest;



template <typename TestParam>
class FindBestSplitWithDuplicatesTest : public ::testing::Test {


  // Similar examples as for the
  // DecisionTree.FindBestNumericalSplitCartNumericalLabelBase test.


  // Computes the preprocessing.


    // Same split as unweighted, score manually verified with numpy.
    // > varb = function (x) { mean(x*x) - mean(x)^2 }
    // > varb(c(1,1,0,0,1,0)) - 4/6*varb(c(1,1,1,0)) - 2/6*varb(c(0,0))




  // Computes the preprocessing.




  // Small basic dataset.

  // Configuration.

  // Compute the label distribution.


  // The expected element map is "1100".
    // Verified with numpy, same split as unweighted
    // TODO: The better split is the same as before, but it's an
    // artifact of incorrect rounding.
    // > varb = function (x) { mean(x*x) - mean(x)^2 }
    // > varb(c(1,1,0,0,1,0)) - 4/6*varb(c(0,0,1,0)) - 2/6*varb(c(1,1))


  // Since all the attributes have the same value, there are no valid splits.


  // Small basic dataset.

  // Configuration.

  // Compute the label distribution.


  // R> entropy(c(2,4)) - 4/6*entropy(c(2,2)) - 2/6*entropy(c(2)) = 0.1744


  // Small basic dataset.

  // Configuration.

  // Compute the label distribution.


    // Verified with Numpy
    // > varb = function (x) { mean(x*x) - mean(x)^2 }
    // > varb(c(1,0,0,0,0,1)) - 4/6*varb(c(1,0,0,1)) - 2/6*varb(c(0,0))



  // This "na_replacement" value will be ignored and replaced with
  // "mean(attributes) = 1.5".



  // > mean(c(1, 1.5)) = 1.25



  // Small basic dataset.

  // This "na_replacement" value will be ignored and replaced with
  // the most frequent item = 2.

  // Configuration.

  // Compute the label distribution.


  // The expected element map is "0011".
  // R> entropy(c(3,3)) - 4/6*entropy(c(3,1)) - 2/6*entropy(c(2)) = 0.3182571


  // Since all the attributes have the same value, there are no valid splits.

  // All the attribute value are NA.


  // This "na_replacement" value will be ignored and replaced with
  // the most frequent attribute (0).






  // Test majority positive case.


  // The test defines 4 example: 0 and 2 are valid non-na examples, 1 is an
  // example filled with na-values, and 3 is a forbidden example.












  // A good attribute that perfectly separate the labels.
  // Will end up in the positive set.
  // Will end up in the negative set.

  // A bad attribute that does not separate the labels at all. All the item
  // values are present twice for each label value.

  // A pure/non-valid attribute.

  int num_attribute_classes = 6;
  int num_label_classes = 3;
  int min_num_obs = 1;

  // Compute the label distribution.





  // The expected element map is "000111".
  // R> entropy(c(4,4)) = 0.6931472


  // A attribute that does not perfectly separate the labels.
  // Label = 0 + put in negative split.
  // Label = 1 + put in negative split.
  // Label = 1 + put in positive split.

  int num_attribute_classes = 4;
  int num_label_classes = 3;
  int min_num_obs = 1;

  // Compute the label distribution.



  // The expected element map is "0010" + padding.
  // R> entropy(c(4,4)) - 6/8 * entropy(c(4,2)) = 0.2157616

  // List of selected examples.
  // Uniform weights.
  // Example labels. Ultimately, we want the first 4 examples to be in one
  // child, and the 4 other in the other.

  // A good attribute that perfectly separates the labels.
  // We would want the split to be "attribute \intersect {0,1,2} is empty".
  //
  // In the case of label_v1 because of the greedy selection, item 5 will be
  // selected first. Therefore the condition will be "attribute \intersect {5}
  // is empty".
  //
  // In the case of label_v2, the item 0 will be selected first, leading to the
  // best possible outcome.

  // Will end up in the positive set.
  // Will end up in the negative set.

  // A pure/non-valid attribute.

  int num_attribute_classes = 6;
  int min_num_obs = 1;

  // Compute the label distribution.





  // The expected element map is "100000" (32).
    // Same split as unweighted, verified with numpy
    // R>  var(c(1,2,3,4,11,12,13,14)) - (var(c(1,2,3,4,11,12)) * (6/8) +
    // var(c(13,14)) * (2/8)) = 12
    // With "var" the variance.


  // The expected element map is "000111".
    // Same split as unweighted, score verified with numpy.
    // R>   var(c(1,2,3,4,11,12,13,14)) - (var(c(1,2,3,4)) * (4/8) +
    // var(c(11,12,13,14)) * (4/8)) = 25
    // With "var" the variance (not the sampling variance).



    // All the candidate items are selected.

    // None of the items are selected.

    // The last candidate item is not selected.

  // examples.




  // Near the middle.
  // Near the middle.




  // Test case: Features are valid and scores are based on feature id, but
  // current best node has a high score.

  bool result = FindBestConditionConcurrentManager(





  // Test case: All features are invalid.
  bool result = FindBestConditionConcurrentManager(





  // Test case: Features are valid and scores are based on feature id, but
  // current best node has a high score.
  bool result = FindBestConditionConcurrentManager(


  // Test case: Features are valid and scores are based on feature id.






  // Test case: Features alternate between valid and invalid.
  bool result = FindBestConditionConcurrentManager(





  // Test case: Features alternate between valid and invalid, and processed in
  // reverse order.
  bool result = FindBestConditionConcurrentManager(



  // Ensure the parameter is defined.




  // Ensure the parameter is defined.

      bool is_default = field.second.mutual_exclusive().is_default();
        auto other_param_it =







  // The best split is on the right size of the bin[1].


  // The equivalent threshold values are in [2,4]. We take the center.



  // Empty dist.




  // Empty dist.


  // Empty dist.








  // EUCLIDEAN_DISTANCE is the only split score that handle natively the
  // distance between pure distributions.







  // The values {1,4} or {2,4} are discriminative.
  // 4 is always positive, and 3 is always negative.


  // EUCLIDEAN_DISTANCE is the only split score that handle natively the
  // distance between pure distributions.





  // Both the splits of attribute values [neg={3,2}, pos={1,4}] and [pos={3,1},
  // neg={2,4}] are equivalent and acceptable.

// Test the splitter with the replacement of missing outcome by the parent
// outcome.

  // The values  {1,4} or {2,4} are discriminative.
  // 4 is always positive, and 3 is always negative.


  // EUCLIDEAN_DISTANCE is the only split score that natively handles the
  // distance between pure distributions.





  // Both the splits of attribute values [neg={3,2}, pos={1,4}] and [pos={3,1},
  // neg={2,4}] are equivalent and acceptable.

// Tests that the minimum number of examples per treatment constraint works.

  // The best splits would be >=1.5 or >=3.5. However, those splits are not
  // valid as they don't satisfy a min number of examples per treatment = 2
  // constraint.






  // Without constraints, a split is found.


  // With constraints, no split is found.













struct GenericHyperParameterTestDef {
class GenericHyperParameterTest


  // The generic hyper-parameter is defined.




// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/generic_parameters.cc =====






absl::Status PruneInvalidHyperparameters(
  // Check if the definitions are consistent.


absl::Status GetGenericHyperParameterSpecification(








        auto param,
        auto param,




        auto param,

        auto param,

        auto param,

        auto param,


        auto param, get_params(kHParamSplitAxisSparseObliqueMaxNumProjections));














        auto param, get_params(kHParamNumericalVectorSequenceNumRandomAnchors));



absl::Status SetHyperParameters(
  int max_nodes = -1;
  bool max_nodes_is_set = false;














  // Oblique trees

  // Sparse oblique trees










  // Mhld oblique trees


  // Categorical












void PredefinedHyperParameterAxisSplitSpace(




void PredefinedHyperParameterCategoricalSpace(


// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/generic_parameters.h =====






// Generic hyper parameter names.


// Categorical-set splitter

// Categorical splitter

// Axis splits.












// Fill decision tree specific generic hyper parameter specifications.
// This function is designed to be called by the learners using decision tree
// learning.
//
// If only a subset of hyperparameters should be populated, both the set of
// valid hyperparameters and the set of invalid hyperparameters must be passed
// to this function. If any hyperparameter is neither valid nor invalid, this
// function returns an error. Note that if any hyperparameters are defined in
// hparam_def before being passed to this function, these hyperparameters must
// also be either valid or invalid.
absl::Status GetGenericHyperParameterSpecification(

// Set the fields of a decision tree training proto from the set of generic
// hyperparameters. "consumed_hparams" contains the list of already "consumed"
// hyper-parameters. An error is raised if `SetHyperParameters()` tries to
// consume an hyper-parameter initially `consumed_hparams`. All the consumed
// hyper-parameter keys are added to `consumed_hparams`.
absl::Status SetHyperParameters(

// Default predefined hyper-parameter space for axis splits.
void PredefinedHyperParameterAxisSplitSpace(

// Default predefined hyper-parameter space for categorical splits.
void PredefinedHyperParameterCategoricalSpace(



// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/generic_parameters_test.cc =====









      // kHParamMinExamples,
  absl::Status status = GetGenericHyperParameterSpecification(

      // kHParamNumCandidateAttributes,
  absl::Status status = GetGenericHyperParameterSpecification(

      // kHParamNumCandidateAttributes,
  absl::Status status = GetGenericHyperParameterSpecification(

      // kHParamNumCandidateAttributes,
  absl::Status status = GetGenericHyperParameterSpecification(

  absl::Status status = GetGenericHyperParameterSpecification(

  absl::Status status = GetGenericHyperParameterSpecification(

  absl::Status status = GetGenericHyperParameterSpecification(



// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/gpu.cc =====






// Computes the dot product between two vectors.
float DotProduct(int n, const float* __restrict a, const float* __restrict b) {
  float acc = 0;

// Computes the squared distance between two vectors. If the squared distance is
// greater than "max_value", return a value greater than "max_value" (but not
// necessary equal to the squared distance).
float SquaredDistance(int n, const float* __restrict a,
  float acc = 0;


absl::Status AddTwoVectorsCPU(absl::Span<const float> src_1,

    bool use_gpu) {
  auto c = absl::WrapUnique(new VectorSequenceComputer());






absl::Status VectorSequenceComputer::Release() {

absl::Status VectorSequenceComputer::ReleaseCPU() { return absl::OkStatus(); }

absl::Status VectorSequenceComputer::InitializeCPU() {

absl::Status VectorSequenceComputer::ComputeMaxDotProductCPU(
      float max_p = std::numeric_limits<float>::lowest();

absl::Status VectorSequenceComputer::ComputeNegMinSquareDistanceCPU(
      float min_dist2 = std::numeric_limits<float>::max();
      // Note: We negate the values so the vector condition is in the same
      // "direction" as the underlying threshold condition.

absl::Status VectorSequenceComputer::ComputeMaxDotProduct(

absl::Status VectorSequenceComputer::ComputeNegMinSquareDistance(

// GPU functions when not compiled with GPU support.

absl::Status CheckHasGPU(bool print_info) {

absl::Status VectorSequenceComputer::InitializeGPU() {

absl::Status VectorSequenceComputer::ComputeNegMinSquareDistanceGPU(

absl::Status VectorSequenceComputer::ReleaseGPU() {

absl::Status VectorSequenceComputer::ComputeMaxDotProductGPU(

absl::Status AddTwoVectorsGPU(absl::Span<const float> a,



// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/gpu.cu.cc =====








// Adds two vectors.
  int i = blockIdx.x * blockDim.x + threadIdx.x;

// Maximum of dot product kernel.
    float *__restrict__ values, float *__restrict__ anchors,
    int num_selected_examples, int num_anchors, int anchor_dim,
    int num_anchor_item_copy_per_thread) {
  int local_example_idx = blockIdx.x * blockDim.x + threadIdx.x;
  int anchor_idx = blockIdx.y;

  // Copy of anchor data in thread shared memory



  // Compute the dot product

  float max_p = std::numeric_limits<float>::lowest();
    float acc = 0;

// Minimum of distance kernel.
    float *__restrict__ values, float *__restrict__ anchors,
    int num_selected_examples, int num_anchors, int anchor_dim,
    int num_anchor_item_copy_per_thread) {
  int local_example_idx = blockIdx.x * blockDim.x + threadIdx.x;
  int anchor_idx = blockIdx.y;

  // Copy of anchor data in thread shared memory



  // Compute the dot product

  float min_d2 = std::numeric_limits<float>::max();
    float d2 = 0;
      float v = shared_anchor[dim_idx] - values[offset_vector + dim_idx];

// Converts a Cuda status into an absl status.
absl::Status CudaStatus(cudaError_t code) {

// RETURN_IF_ERROR on Cuda status.

// Copies a vector from host to device.
template <typename T>
absl::Status EasyCudaCopyH2D(absl::Span<const T> src, T *dst) {

// Copies a vector from device to host.
template <typename T>
absl::Status EasyCudaCopyD2H(T *src, size_t n, absl::Span<T> dst) {

// Allocates and copy data from host to device.
template <typename T>
absl::Status EasyCudaAllocAndCopy(absl::Span<const T> src, T **dst) {


absl::Status CheckHasGPU(bool print_info) {
    int driver_version = 0;

absl::Status AddTwoVectorsGPU(absl::Span<const float> a,

  // Allocate memory on the device
  float *d_a, *d_b, *d_c;

  // Copy data from host to device

  // Launch the kernel
  int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;

  // Copy data from device to host



absl::Status VectorSequenceComputer::InitializeGPU() {


    // Constants


  // Working data


absl::Status VectorSequenceComputer::ComputeMaxDotProductGPU(




  // TODO: Reduce kThreadPerBlocksExample if "num_anchors" *
  // kThreadPerBlocksExample is too large.





absl::Status VectorSequenceComputer::ComputeNegMinSquareDistanceGPU(




  // TODO: Reduce kThreadPerBlocksExample if "num_anchors" *
  // kThreadPerBlocksExample is too large.





absl::Status VectorSequenceComputer::ReleaseGPU() {


// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/gpu_test.cc =====






    int num_sequence = std::poisson_distribution<int>(50)(rng);


struct SyntheticQueryData {
















  auto query_data =










  auto query_data =







    int num_repeat = 10000 / selected_num_examples;





      auto query_data = BuildSyntheticQueryData(





// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/label.cc =====





// Set the label value for a classification label on a vertical dataset.
template <bool weighted>
absl::Status SetClassificationLabelDistribution(

absl::Status SetCategoricalUpliftLabelDistribution(
  // TODO: Update.





absl::Status SetRegressiveUpliftLabelDistribution(





// Set the label value for a regression label on a vertical dataset.
//
// Default policy to set the label value of a leaf in a regression tree i.e. set
// the value to the mean of the labels.
// `weights` may be empty, corresponding to unit weights.
template <bool weighted>
absl::Status SetRegressionLabelDistribution(


absl::Status SetLabelDistribution(




void UpliftLeafToLabelDist(const decision_tree::proto::NodeUpliftOutput& leaf,

void UpliftLabelDistToLeaf(const UpliftLabelDistribution& dist,


// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/label.h =====






// Label statistics.
struct LabelStats {

// Label statistics for Classification.
struct ClassificationLabelStats : LabelStats {


// Label statistics for Regression.
struct RegressionLabelStats : LabelStats {


// Label statistics for Regression with hessian.
struct RegressionHessianLabelStats : LabelStats {


// Label statistics for uplift with categorical treatment and categorical
// outcome.
struct CategoricalUpliftLabelStats : LabelStats {



// Label statistics for uplift with categorical treatment and numerical outcome.
struct NumericalUpliftLabelStats : LabelStats {



// Signature of a function that sets the value (i.e. the prediction) of a leaf
// from the gradient data.

// Similar to CreateSetLeafValueFunctor, but use pre-computed statistics instead
// of scanning the values.

// The default policy to set the value of a leaf.
// - Distribution of the labels for classification.
// - Mean of the labels for regression.
absl::Status SetLabelDistribution(

// Copies the content on uplift categorical label distribution to the leafs.
void UpliftLabelDistToLeaf(const UpliftLabelDistribution& dist,

// Copies the content on uplift categorical leaf output to a label distribution.
void UpliftLeafToLabelDist(const decision_tree::proto::NodeUpliftOutput& leaf,

// Training configuration for internal parameters not available to the user
// directly.
struct InternalTrainConfig {
  // How to set the leaf values. Used by all methods except layer-wise learning.

  // If true, the split score relies on a hessian: ~gradient^2/hessian (+
  // regularization). This is only possible for regression. Require
  // hessian_leaf=true.
  //
  // If false, the split score is a classical decision tree score. e.g.,
  // reduction of variance in the case of regression.
  bool hessian_score = false;

  // If true, the leaf relies on the hessian. This is only possible for
  // regression.
  bool hessian_leaf = false;

  // Index of the hessian column in the dataset. Only used if hessian_leaf=true.
  int hessian_col_idx = -1;

  // Index of the gradient column in the dataset.  Only used if
  // hessian_leaf=true.
  int gradient_col_idx = -1;

  // Regularization terms for hessian_score=true.
  float hessian_l1 = 0.f;
  float hessian_l2_numerical = 0.f;
  float hessian_l2_categorical = 0.f;

  // Number of attributes tested in parallel (using fiber threads).
  int num_threads = 1;

  // Non owning pointer to pre-processing information.
  // Depending on the decision tree configuration this field might be required.


  // If true, the list of selected example index ("selected_examples") can
  // contain duplicated values. If false, all selected examples are expected to
  // be unique.
  bool duplicated_selected_examples = true;

  // If set, the training of the tree will stop after this time, leading to an
  // under-grow but valid decision tree. The growing strategy defines how the
  // tree is "under-grown".

  // If set, overrides the sorting_strategy.



// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/oblique.cc =====









template <typename T>

template std::vector<int32_t> Extract<int32_t>(

template std::vector<float> Extract<float>(




int GetNumProjections(const proto::DecisionTreeTrainingConfig& dt_config,
    // Note: if there is only one feature, all the projections are the same.




template <typename LabelStats>


  // Effective number of projections to test.
  int num_projections;


  // Best and current projections.
  float best_threshold;


  // TODO: Cache.


    // Generate a current_projection.

    // Pre-compute the result of the current_projection.



  // Update with the actual current_projection definition.


absl::Status SolveLDA(const proto::DecisionTreeTrainingConfig& dt_config,
  // TODO: Cache.

  // Inverse the SW matrice.
    // The matrix is not invertible.

  // Get the eigenvalues / vectors.



  // Get the largest eigenvalue / vector.
  int arg_abs_max = -1;

  // Convert the top eigen vector into a projection.


struct ScoreAndThreshold {
  float score;
  float threhsold;

template <typename LabelStats, typename Labels>


  // Projection are never missing.

  // Find a good split in the current_projection.



template absl::StatusOr<SplitSearchResult>

template absl::StatusOr<SplitSearchResult>

template absl::StatusOr<SplitSearchResult>

template <typename LabelStats, typename Labels>
absl::Status EvaluateProjectionAndSetCondition(


template <typename LabelStats, typename Labels>
absl::Status EvaluateMHLDCandidates(
  // TODO: Multi-thread



      // Extract attribute value

      // Find best projection


      // Compute projection

      // Evaluate projection quality







template <typename LabelStats>


  // TODO: Cache.



  float global_best_score = best_condition->split_score();
  bool found_better_global = false;


      // No more features to try.

    // Compute the sets of set of features to evaluate.

    // Evaluate

    // Find the best local and global projection.
    float round_best_score = 0;
    int round_best_candidate_idx = -1;


      // No local improvement.







void SampleProjection(const absl::Span<const int>& features,

    float weight = unif1m1(*random);
        float sign = (weight >= 0) ? 1.f : -1.f;
        int exponent =
        // Return continuous weights.

      // As soon as one selected feature is monotonic, the oblique split
      // becomes monotonic.



  int max_num_features = dt_config.sparse_oblique_split().max_num_features();
  int cur_num_projections = projection->size();

    // For a small number of features, a boolean vector is more efficient.
    // Re-evaluate if this becomes a bottleneck.
    // Floyd's sampling algorithm.
        // t was already sampled, so insert j instead.
        // t was not yet sampled.

absl::Status SetCondition(const Projection& projection, const float threshold,

absl::Status LDACache::ComputeClassification(
  // Solve a LDA (Linear Discriminant Analysis) using the Eigenvalue
  // decomposition approach.
  //
  // Based on the section 2 of "Revisiting Classical Multiclass Linear
  // Discriminant Analysis with a Novel Prototype-based Interpretable
  // Solution".
  //
  // TODO: Experiment with other approaches. For instance, the singular
  // value decomposition approach.


  // Compute the mean of the features (globally and per class).
  // TODO: Cache.






  // Normalize the sums into means.


  // Compute Sb

  // Compute Sw


  // Help the matrix to be invertible.

    // Index features.


// Builds the feature mapping. "mapping[i]" is the index in "sw_" and "sb_" of
// feature "selected_features[i]".
absl::Status LDACache::BuildMapping(const std::vector<int>& selected_features,

absl::Status LDACache::Extract(const std::vector<int>& selected_features,
  // TODO: Cache.


absl::Status LDACache::GetSB(const std::vector<int>& selected_features,

absl::Status LDACache::GetSW(const std::vector<int>& selected_features,





absl::Status ProjectionEvaluator::Evaluate(
    float value = 0;
      // TODO: Move the indirection outside of the loop.
      float attribute_value = (*attribute_values)[example_idx];

absl::Status ProjectionEvaluator::ExtractAttribute(
    float value = src_values[example_idx];

void SubtractTransposeMultiplyAdd(double weight, absl::Span<double> a,


void SubtractTransposeMultiplyAdd(



// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/oblique.h =====


// Implementation and extension of the "Sparse Projection Oblique Random
// Forests" 2020 JMLR paper by Tomita et al
// (https://www.jmlr.org/papers/volume21/18-664/18-664.pdf) and the
// "Classification Based on Multivariate Contrast Patterns" 2019 paper by
// Canete-Sifuentes et al.
//
// Sparse Projection Oblique Random Forests algorithm:
//
// During training, at each node, the algorithm samples multiple random sparse
// linear projections of the numerical features, and evaluate then as classical
// numerical features (i.e. looking for a split projection >= threshold).
//
// Experiments in Tomita's paper indicates that this split algorithm used with
// Random Forest can leads to improvements over classical (RF) and other
// random-projection-oblique (RR-RF, CCF) random forest algorithms. These
// results have been confirmed experimentally using this implementation.
//
// Multi-class Hellinger Linear Discriminant algorithm:
//
// The MHLD algorithm works by greedily building projections by adding features
// one after the other. Given a set of features, the projection coefficient is
// determined with Linear Discriminant Analysis (LDA). Like for the SPO
// algorithm, the regular splitting algorithm is used to select the threshold.
//




// The following three "FindBestConditionOblique" functions are searching
// for the best sparse oblique split for different objectives / loss functions.
// These methods only differ by the type of the "label_stats" argument.

// Classification.

// Regression with hessian term.

// Regression.

// Computes the number of projections to test i.e.
// num_projections = min(max_num_projections,
// ceil(num_features ^ num_projections_exponent)).
int GetNumProjections(const proto::DecisionTreeTrainingConfig& dt_config,
                      int num_numerical_features);

// Extraction of label values. Different implementations for different types of
// labels.


struct GradientAndHessian {


// Extracts values using an index i.e. returns "values[selected]".
template <typename T>

// Runs a splitter to finds a "x >= t" condition on
// (projection_values,selected_labels,selected_weights).
template <typename LabelStats, typename Labels>


// A projection is defined as \sum features[projection[i].index] *
// projection[i].weight;
struct AttributeAndWeight {
  int attribute_idx;
  float weight;

// Utility to evaluate projections.
//
// This object references the data of the vertical dataset given as input.
class ProjectionEvaluator {
  // Initialize.
  //
  // Args:
  //   train_dataset: Dataset containing the data. This object should remain
  //     valid until the "ProjectionEvaluator" is destroyed.
  //   numerical_features: Features to index. Projections can only use indexed
  //     features.

  // Evaluates a projection of a set of selected examples.
  //
  // "values", the output variable, contains "selected_examples.size()" values.
  // If one of the input feature of the projection is missing, this input
  // feature is replaced by the mean value of feature as computed on the
  // training dataset. This is the same logic used during inference.
  absl::Status Evaluate(const Projection& projection,

  absl::Status ExtractAttribute(
      int attribute_idx, absl::Span<const UnsignedExampleIdx> selected_examples,


  float NaReplacementValue(int attribute_idx) const {

  // Non-owning pointer to numerical attributes.
  // Indexed by attribute idx.

  // Replacement for missing values.
  // Indexed by attribute idx.

  // Constructor status.
  absl::Status constructor_status_;

// Computes the SW and SB matricies needed to solve the LDA optimization.
class LDACache {
  absl::Status ComputeClassification(
      bool index_features = true);


  // Gets the SB matrice for a subset of features.
  absl::Status GetSB(const std::vector<int>& selected_features,

  // Gets the SW matrice for a subset of features.
  absl::Status GetSW(const std::vector<int>& selected_features,

  // Builds the feature mapping. "mapping[i]" is the index in "sw_" and "sb_" of
  // feature "selected_features[i]".
  absl::Status BuildMapping(const std::vector<int>& selected_features,

  absl::Status Extract(const std::vector<int>& selected_features,

  // The SB and SW square matrices for all the features.

  // Size of the sb and sw square matrices.
  int size_;

  // "feature_to_idx_[i]" is the index in sb_ and sw_ of the feature i.

// Computes: output += weight * (a - b) * transpose(a - b), where "a" and "b"
// are vectors. "output" should be of the size "a.size() * b.size()".
void SubtractTransposeMultiplyAdd(double weight, absl::Span<double> a,

// Computes: output += weight * (a - b) * transpose(a - b), where a=
// projection_evaluator[example_idx,:].
void SubtractTransposeMultiplyAdd(

// Randomly generates a projection from `features`. `features` should only be
// numerical. A projection cannot be empty. If the projection contains only one
// dimension, the weight is guaranteed to be 1. If the projection contains an
// input feature with monotonic constraint, monotonic_direction is set to 1
// (i.e. the projection should be monotonically increasing).
void SampleProjection(const absl::Span<const int>& features,
                      float projection_density,

// Converts a Projection object + float threshold into a proto condition of the
// same semantic. `projection` cannot be empty.
absl::Status SetCondition(const Projection& projection, float threshold,



// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/preprocessing.cc =====






bool StrategyRequireFeaturePresorting(




absl::Status PresortNumericalFeatures(
  // Check number of examples.



  // For all the input features in the model.
    // Skip non numerical features.


      // Global imputation replacement.

        auto value = values[example_idx];

      // Sort by feature value and example index.


      float last_value;


// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/preprocessing.h =====







// Item used to pass the sorted numerical attribute values to the
// "ScanSplitsPresortedSparseDuplicateExampleTemplate" function below.
//
// Index of the example in the training dataset.
// The highest bit (MaskDeltaBit) is 1 iif. the feature value of this item is
// strictly greater than the preceding one. The other bits (MaskExampleIdx) are
// used to encode the example index.
struct SparseItemMeta {





// Returns true if the strategy require for the input features to be pre-sorted.
bool StrategyRequireFeaturePresorting(

// Pre-computation on the training dataset used for the training of individual
// trees. The pre-processing is computed before any tree is trained.
class Preprocessing {
  struct PresortedNumericalFeature {
    // Example index sorted in increasing order of feature values.
    // Missing values are treated as replaced by the GLOBAL_IMPUTATION strategy.
    // The high bit of each example index is set iif. the feature value is
    // different (greater) than the previous one.




  void set_num_examples(const uint64_t value) { num_examples_ = value; }

  // List of presorted numerical features, indexed by feature index.
  // If feature "i" is not numerical or not presorted,
  // "presorted_numerical_features_[i]" will be an empty index.

  // Total number of examples.

// Preprocess the dataset before any tree training.

// Component of "PreprocessTrainingDataset". Computes pre-sorted numerical
// features.
absl::Status PresortNumericalFeatures(



// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/splitter_accumulator.h =====


// Template classes used by the splitter to accumulate statistics: Buckets and
// accumulators.
//
// See "decision_tree_splitter_scanner.h" for an explanation of the use of these
// objects.
//
// Feature buckets
// ===============
// Available: FeatureNumericalBucket, FeatureCategoricalBucket,
// FeatureIsMissingBucket.
//
// Label buckets & accumulator
// ===========================
// Available: LabelNumerical{Bucket,ScoreAccumulator}.
//





// Bucket data containers.
//
// Bucket definitions are templated to facilitate code reuse. Since buckets are
// constructed many times, it's worth saving memory aggressively and only
// construct fields that will actually be used. C++ does not support 0-byte
// objects, hence the unused fields cannot be set to void. Empty structs would
// occupy 1 byte for the unused field. Combining two fields into a struct is
// therefore the most space-efficient alternative.
struct BooleanValueAndWeight {
  bool value;
  float weight;

struct BooleanValueOnly {
  bool value;

struct IntegerValueAndWeight {
  int value;
  float weight;

struct IntegerValueOnly {
  int value;

struct FloatValueAndWeight {
  float value;
  float weight;

struct FloatValueOnly {
  float value;

struct FloatGradientHessianAndWeight {
  float gradient;
  float hessian;
  float weight;

struct FloatGradientHessianOnly {
  float gradient;
  float hessian;

struct FloatSumGradientHessianAndWeight {
  float sum_gradient;
  float sum_hessian;
  float sum_weight;

struct FloatSumGradientHessianOnly {
  float sum_gradient;
  float sum_hessian;

struct SumTruesAndWeights {

struct SumTruesOnly {


// ===============
// Feature Buckets
// ===============
//
// FeatureBucket Accumulates statistics about the features. They should
// implement the following methods:
//
// Ordering of the buckets. If "kRequireSorting=true", the bucket will be
// scanned in increasing order.
// bool operator<(const FeatureBucket& other) const;
//
// If true, the buckets will be sorted before being scanned.
// static constexpr bool kRequireSorting;
//
// Given the first and last filled buckets for a particular attribute, test if
// this attribute is valid. If invalid, the bucket is not scanned. Note:
// Different algorithms can invalid attributes differently.
// static bool IsValidAttribute(const FeatureBucket& first,
//                             const FeatureBucket& last);
//
// Are two consecutive buckets defining a valid split i.e. can
// "SetConditionFinal" be called on "left".
// static bool IsValidSplit(const FeatureBucket& left, const FeatureBucket&
// right)
//
// FeatureFiller controls the accumulation in a feature bucket. They should
// implement the following methods:
//
// Number of buckets to allocate.
// size_t NumBuckets() const;
//
// // Initialize the content of a bucket.
// void InitializeAndZero(const int bucket_idx, FeatureBucket* acc) const;
//
// // In which bucket a given example is falling.
// size_t GetBucketIndex(const size_t local_example_idx, const
// UnsignedExampleIdx example_idx) const;
//
// // Consume a training example.
// void ConsumeExample(const UnsignedExampleIdx example_idx, FeatureBucket* acc)
// const;
//
// Set the split function if this bucket is selected as
// the best greater bucket in the negative side of the split. All the
// following buckets will end in the positive side of the split.
// template<typename ExampleBucketSet>
// virtual void SetConditionFinal(const ExampleBucketSet& example_bucket_set,
//                                const size_t best_bucket_idx,
//                                proto::NodeCondition* condition) const = 0;
//
// Numerical feature.
struct FeatureNumericalBucket {
  // Numerical feature value.
  float value;

  // The buckets will be sorted according to the numerical values.

  bool operator<(const FeatureNumericalBucket& other) const {



  class Filler {


    void InitializeAndZero(const int bucket_idx,


    void ConsumeExample(const UnsignedExampleIdx example_idx,

    float GetValue(const UnsignedExampleIdx example_idx) const {

    void SetConditionFinalFromThresholds(

    template <typename ExampleBucketSet>
    void SetConditionFinal(const ExampleBucketSet& example_bucket_set,

    float MissingValueReplacement() const { return na_replacement_; }




// Discretized Numerical feature.
struct FeatureDiscretizedNumericalBucket {

  bool operator<(const FeatureDiscretizedNumericalBucket& other) const {



  class Filler {


    void InitializeAndZero(const int bucket_idx,


    void ConsumeExample(const UnsignedExampleIdx example_idx,

    template <typename ExampleBucketSet>
    void SetConditionFinal(const ExampleBucketSet& example_bucket_set,

    template <typename ExampleBucketSet>
    void SetConditionInterpolatedFinal(
      // The "discretized_higher_condition" does not allow to express
      // condition with threshold other than the threshold of the discretized
      // value. Therefore, we do an interpolation in the bucket index domain and
      // round up to the smallest one.

    int num_bins_;


  // The feature bucket contains no information.

// Categorical feature.
struct FeatureCategoricalBucket {

  bool operator<(const FeatureCategoricalBucket& other) const {



  class Filler {


    void InitializeAndZero(const int bucket_idx,


    void ConsumeExample(const UnsignedExampleIdx example_idx,

    template <typename ExampleBucketSet>
    void SetConditionFinal(const ExampleBucketSet& example_bucket_set,
      bool na_replacement_in_pos = false;



    // Set the condition as "value \in mask" with
    // mask = [ example_bucket_set.items[bucket_idx].feature.value for
    //   bucket_idx in range(best_order_idx+1,example_bucket_set.items.size())]
    template <typename ExampleBucketSet>
    void SetConditionFinalWithOrder(
      bool na_replacement_in_pos = false;



    // Set the condition as "value \in mask".
    void SetConditionFinalWithBuckets(const std::vector<int32_t>& mask,
      bool na_replacement_in_pos =

    int num_categorical_values_;
    int na_replacement_;



// Boolean feature.
struct FeatureBooleanBucket {

  bool operator<(const FeatureBooleanBucket& other) const {



  class Filler {


    void InitializeAndZero(const int bucket_idx,


    void ConsumeExample(const UnsignedExampleIdx example_idx,

    template <typename ExampleBucketSet>
    void SetConditionFinal(const ExampleBucketSet& example_bucket_set,




// Binary feature about the presence / absence of value.
struct FeatureIsMissingBucket {

  bool operator<(const FeatureIsMissingBucket& other) const {



  class Filler {


    void InitializeAndZero(const int bucket_idx,


    void ConsumeExample(const UnsignedExampleIdx example_idx,

    template <typename ExampleBucketSet>
    void SetConditionFinal(const ExampleBucketSet& example_bucket_set,




// ============
// Accumulators
// ============
//
// ScoreAccumulators accumulate the label statistics for a set of buckets. They
// should implement the following methods.
//
// Score obtained for all the scanned bucket. The score of the split will be 1)
// the example weighted sum of the scores of the accumulators (if
// "kNormalizeByWeight=true"), or 2) the sum of scores of the accumulators (if
// "kNormalizeByWeight=false").
// double Score() const;
//
// Weighted number of scanned examples.
// double WeightedNumExamples() const;
//
struct LabelNumericalScoreAccumulator {



  void ImportLabelStats(const proto::LabelStatistics& src) {

  void ExportLabelStats(proto::LabelStatistics* dst) const {

  void Sub(LabelNumericalScoreAccumulator* dst) const { dst->label.Sub(label); }

  void Add(LabelNumericalScoreAccumulator* dst) const { dst->label.Add(label); }


struct LabelNumericalWithHessianScoreAccumulator {



  void ImportLabelStats(const proto::LabelStatistics& src) {

  void ExportLabelStats(proto::LabelStatistics* dst) const {

  void Sub(LabelNumericalWithHessianScoreAccumulator* dst) const {

  void Add(LabelNumericalWithHessianScoreAccumulator* dst) const {


struct LabelCategoricalScoreAccumulator {



  void SetZero() { label.Clear(); }

  void ImportLabelStats(const proto::LabelStatistics& src) {

  void ExportLabelStats(proto::LabelStatistics* dst) const {

  void Sub(LabelCategoricalScoreAccumulator* dst) const {

  void Add(LabelCategoricalScoreAccumulator* dst) const {


struct LabelBinaryCategoricalScoreAccumulator {



  void Clear() {

  void Set(const double trues, const double weights) {

  void AddOne(const bool value, const float weights) {

  void AddOne(const bool value) {

  void SubOne(const bool value, const float weights) {

  void SubOne(const bool value) {

  void AddMany(const double trues, const double weights) {

  void SubMany(const double trues, const double weights) {


struct LabelHessianNumericalScoreAccumulator {

  // Minimum hessian value when computing hessian scores and leaf values.



    // grad^2 / hessian

  // Leaf value without any constraint applied.

    // grad / hessian


  void SetRegularization(double l1, double l2) {

  void Clear() {

  template <typename T>
  void Set(const T gradient, const T hessian, const T weights) {

  template <typename T>
  void Add(const T gradient, const T hessian, const T weights) {

  template <typename T>
  void Sub(const T gradient, const T hessian, const T weights) {


  // Regularization parameters.

  // Optional constraint on the leaf values.
  // If set, constraints.min_max.has_value() is true.

// ===============
// Label Buckets
// ===============
//
// LabelBucket accumulate statistics about labels in a bucket.
//
// LabelBuckets may be templated to improve performance for special cases such
// as unweighted datasets.
//
// LabelBuckets should implement the following methods:
//
// Add a bucket to an accumulator.
// void AddToScoreAcc(ScoreAccumulator* acc) const;
//
// Remove a bucket from an accumulator.
// void SubToScoreAcc(ScoreAccumulator* acc) const;
//
// Ordering between bucket. Used to sort the buckets if
// "require_label_sorting=true".
// bool operator<(const LabelBucket& other) const;
//
// LabelFillers control the accumulation of statistics in a label bucket. They
// need to implement the following methods:
//
// Initialize and zero a bucket before scanning. "acc" might be a previously
// used bucket.
// void InitializeAndZero(LabelBucket* acc) const;
//
// Add the statistics about one examples to the bucket.
// void ConsumeExample(const UnsignedExampleIdx example_idx, LabelBucket* acc)
// const;
//
// Initialize and empty an accumulator.
// void InitEmpty(ScoreAccumulator* acc) const;
//
// Initialize an accumulator and set it to contain all the training examples.
// void InitFull(ScoreAccumulator* acc) const;
//
// Normalize the score of the bucket. The final split score is:
// NormalizeScore(weighted sum of the positive and negative accumulators).
// double NormalizeScore(const double score) const;
//

template <bool weighted>
struct LabelNumericalOneValueBucket {

  void AddToScoreAcc(LabelNumericalScoreAccumulator* acc) const {

  void SubToScoreAcc(LabelNumericalScoreAccumulator* acc) const {

  class Initializer {

    void InitEmpty(LabelNumericalScoreAccumulator* acc) const {

    void InitFull(LabelNumericalScoreAccumulator* acc) const {


    bool IsValidSplit(const LabelNumericalScoreAccumulator& neg,



  class Filler {

    void InitializeAndZero(LabelNumericalOneValueBucket* acc) const {}

    void Finalize(LabelNumericalOneValueBucket* acc) const {}

    void ConsumeExample(const UnsignedExampleIdx example_idx,

    template <typename ExampleIdx>
    void AddDirectToScoreAcc(const ExampleIdx example_idx,

    template <typename ExampleIdx>
    void SubDirectToScoreAcc(const ExampleIdx example_idx,

    template <typename ExampleIdx>
    void AddDirectToScoreAccWithDuplicates(

    template <typename ExampleIdx>
    void SubDirectToScoreAccWithDuplicates(

    template <typename ExampleIdx>
    void Prefetch(const ExampleIdx example_idx) const {




template <bool weighted>
struct LabelHessianNumericalOneValueBucket {

  void AddToScoreAcc(LabelHessianNumericalScoreAccumulator* acc) const {

  void SubToScoreAcc(LabelHessianNumericalScoreAccumulator* acc) const {

  class Initializer {

    void InitEmpty(LabelHessianNumericalScoreAccumulator* acc) const {

    void InitFull(LabelHessianNumericalScoreAccumulator* acc) const {


    bool IsValidSplit(const LabelHessianNumericalScoreAccumulator& neg,



    // +1/-1 if the feature is monotonic increasing / decreasing. 0 if the
    // feature is not constrained.

    // Constraints on the leaf.

  class Filler {

    void InitializeAndZero(LabelHessianNumericalOneValueBucket* acc) const {}

    void Finalize(LabelHessianNumericalOneValueBucket* acc) const {}

    void ConsumeExample(const UnsignedExampleIdx example_idx,

    template <typename ExampleIdx>
    void AddDirectToScoreAcc(const ExampleIdx example_idx,

    template <typename ExampleIdx>
    void SubDirectToScoreAcc(const ExampleIdx example_idx,

    template <typename ExampleIdx>
    void AddDirectToScoreAccWithDuplicates(

    template <typename ExampleIdx>
    void SubDirectToScoreAccWithDuplicates(

    template <typename ExampleIdx>
    void Prefetch(const ExampleIdx example_idx) const {




template <bool weighted>
struct LabelCategoricalOneValueBucket {

  // Not called "kCount" because this is used as a template parameter and
  // expects the name to be `count` (in other such structs it is not a
  // constant).

  void AddToScoreAcc(LabelCategoricalScoreAccumulator* acc) const {

  void SubToScoreAcc(LabelCategoricalScoreAccumulator* acc) const {

  class Initializer {

    void InitEmpty(LabelCategoricalScoreAccumulator* acc) const {

    void InitFull(LabelCategoricalScoreAccumulator* acc) const {


    bool IsValidSplit(const LabelCategoricalScoreAccumulator& neg,



  class Filler {

    void InitializeAndZero(

    void Finalize(LabelCategoricalOneValueBucket<weighted>* bucket) const {}

    void ConsumeExample(

    template <typename ExampleIdx>
    void AddDirectToScoreAcc(const ExampleIdx example_idx,

    template <typename ExampleIdx>
    void SubDirectToScoreAcc(const ExampleIdx example_idx,

    template <typename ExampleIdx>
    void AddDirectToScoreAccWithDuplicates(

    template <typename ExampleIdx>
    void SubDirectToScoreAccWithDuplicates(

    template <typename ExampleIdx>
    void Prefetch(const ExampleIdx example_idx) const {





template <bool weighted>
struct LabelBinaryCategoricalOneValueBucket {

  // Not called "kCount" because this is used as a template parameter and
  // expects the name to be `count` (in other such structs it is not a
  // constant).

  void AddToScoreAcc(LabelBinaryCategoricalScoreAccumulator* acc) const {

  void SubToScoreAcc(LabelBinaryCategoricalScoreAccumulator* acc) const {

  class Initializer {

    void InitEmpty(LabelBinaryCategoricalScoreAccumulator* acc) const {

    void InitFull(LabelBinaryCategoricalScoreAccumulator* acc) const {


    bool IsValidSplit(const LabelBinaryCategoricalScoreAccumulator& neg,



  class Filler {

    void InitializeAndZero(

    void Finalize(

    void ConsumeExample(

    template <typename ExampleIdx>
    void AddDirectToScoreAcc(

    template <typename ExampleIdx>
    void SubDirectToScoreAcc(

    template <typename ExampleIdx>
    void AddDirectToScoreAccWithDuplicates(

    template <typename ExampleIdx>
    void SubDirectToScoreAccWithDuplicates(

    template <typename ExampleIdx>
    void Prefetch(const ExampleIdx example_idx) const {





template <bool weighted>
struct LabelNumericalBucket {

  void AddToScoreAcc(LabelNumericalScoreAccumulator* acc) const {

  void SubToScoreAcc(LabelNumericalScoreAccumulator* acc) const {

  void AddToBucket(LabelNumericalBucket* dst) const {

  bool operator<(const LabelNumericalBucket& other) const {

  class Initializer {


    void InitEmpty(LabelNumericalScoreAccumulator* acc) const {

    void InitFull(LabelNumericalScoreAccumulator* acc) const {


    bool IsValidSplit(const LabelNumericalScoreAccumulator& neg,



  class Filler {

    void InitializeAndZero(LabelNumericalBucket* acc) const {

    void Finalize(LabelNumericalBucket* acc) const {}

    void ConsumeExample(const UnsignedExampleIdx example_idx,



template <bool weighted>
struct LabelNumericalWithHessianBucket {

  void AddToScoreAcc(LabelNumericalWithHessianScoreAccumulator* acc) const {

  void SubToScoreAcc(LabelNumericalWithHessianScoreAccumulator* acc) const {

  void AddToBucket(LabelNumericalWithHessianBucket* dst) const {

  bool operator<(const LabelNumericalWithHessianBucket& other) const {

  class Initializer {

    void InitEmpty(LabelNumericalWithHessianScoreAccumulator* acc) const {

    void InitFull(LabelNumericalWithHessianScoreAccumulator* acc) const {


    bool IsValidSplit(





template <bool weighted>
struct LabelHessianNumericalBucket {
  // The priority is defined as ~ "sum_gradient / sum_hessian" (with extra
  // regularization and check of sum_hessian = 0.
  // This value is computed from doubles, and then simply compared (i.e. this
  // is not an accumulator). In memory, it will get aligned to the feature
  // bucket of size 4 bytes.
  float priority;


  void AddToScoreAcc(LabelHessianNumericalScoreAccumulator* acc) const {

  void SubToScoreAcc(LabelHessianNumericalScoreAccumulator* acc) const {

  bool operator<(const LabelHessianNumericalBucket& other) const {

  class Initializer {

    void InitEmpty(LabelHessianNumericalScoreAccumulator* acc) const {

    void InitFull(LabelHessianNumericalScoreAccumulator* acc) const {


    bool IsValidSplit(const LabelHessianNumericalScoreAccumulator& neg,



    // +1/-1 if the feature is monotonic increasing / decreasing. 0 if the
    // feature is not constrained.

    // Constraints on the leaf.

  class Filler {

    void InitializeAndZero(LabelHessianNumericalBucket* acc) const {

    void Finalize(LabelHessianNumericalBucket* acc) const {

    void ConsumeExample(const UnsignedExampleIdx example_idx,




template <bool weighted>
struct LabelCategoricalBucket {

  void AddToScoreAcc(LabelCategoricalScoreAccumulator* acc) const {

  void SubToScoreAcc(LabelCategoricalScoreAccumulator* acc) const {

  void AddToBucket(LabelCategoricalBucket* dst) const {

  float SafeProportionOrMinusInfinity(int idx) const {

  class Initializer {



    void InitEmpty(LabelCategoricalScoreAccumulator* acc) const {

    void InitFull(LabelCategoricalScoreAccumulator* acc) const {


    bool IsEmpty(const int32_t idx) const {

    bool IsValidSplit(const LabelCategoricalScoreAccumulator& neg,



  class Filler {

    void InitializeAndZero(LabelCategoricalBucket* acc) const {

    void Finalize(LabelCategoricalBucket* acc) const {}

    void ConsumeExample(const UnsignedExampleIdx example_idx,





template <bool weighted>
struct LabelBinaryCategoricalBucket {

  void AddToScoreAcc(LabelBinaryCategoricalScoreAccumulator* acc) const {

  void SubToScoreAcc(LabelBinaryCategoricalScoreAccumulator* acc) const {

  float SafeProportionOrMinusInfinity(int idx) const {

  class Initializer {

    void InitEmpty(LabelBinaryCategoricalScoreAccumulator* acc) const {

    void InitFull(LabelBinaryCategoricalScoreAccumulator* acc) const {


    bool IsValidSplit(const LabelBinaryCategoricalScoreAccumulator& neg,



  class Filler {

    void InitializeAndZero(

    void Finalize(LabelBinaryCategoricalBucket<weighted>* bucket) const {}

    void ConsumeExample(const UnsignedExampleIdx example_idx,







// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/splitter_scanner.h =====


// Templated methods for the splitter i.e. to find the best split.
//
// Naming convention:
//   selected_examples: Indices of the training examples.
//   ExampleBucket (or bucket) : Accumulation of information for a set of
//     examples. Composed of a FeatureBucket (accumulate the information about
//     the feature) and the LabelBucket (accumulate the information about the
//     label). In some algorithms, the bucket may only contain one example. The
//     examples in a bucket are not separable according to the split being
//     searched i.e. for a categorical feature, all the examples of the same
//     feature value will end in the same bucket.
//   ExampleBucketSet: A set of ExampleBuckets. A ExampleBucketSet is
//     constructed by scanning the training examples, assigning each example to
//     one of the buckets, and accumulating the information about this example
//     (feature and label) to its ExampleBucket. The split is made to optimize
//     the separation of buckets in a bucket-set.
//   score accumulator: Accumulates the statistics from multiple buckets, and
//     propose a split and a split score.
//
// Finding a split is done by calling "FindBestSplit" with the label and feature
// bucket corresponding to the label and feature.
//
// The FindBestSplit algorithm works as follows:
//   - Allocate the buckets.
//   - Iterate over the training examples and fill the buckets.
//   - Optionally, reorder the buckets.
//   - Iterate over the buckets and fill update the score accumulator. At each
//     step, evaluate the score of the split.
//
// If the preprocessor "YDF_DEBUG_PRINT_SPLIT" is set, detailed logs of the
// splitting algorithm are printed with LOG(INFO).
//





// TODO: Explain the expected signature of FeatureBucket and LabelBucket.
template <typename FeatureBucket, typename LabelBucket>
struct ExampleBucket {


  struct SortFeature {
    bool operator()(const ExampleBucket& a, const ExampleBucket& b) {

  struct SortLabel {
    bool operator()(const ExampleBucket& a, const ExampleBucket& b) {

template <typename ExampleBucket>
struct ExampleBucketSet {


// Used bucket sets.

// Label: Numerical.
// TODO Add unweighted versions for other LabelNumericalBuckets.
template <bool weighted>

template <bool weighted>

template <bool weighted>

template <bool weighted>

template <bool weighted>

// Label: Hessian Numerical.

template <bool weighted>

template <bool weighted>

template <bool weighted>

template <bool weighted>

template <bool weighted>

// Label: Weighted Categorical.








// Label: Unweighted Categorical.








// Label: Weighted Binary Categorical.








// Label: Unweighted Binary Categorical.








// Label: Uplift categorical.



// Label: Uplift numerical.



// Memory cache for the splitter.
//
// Used to avoid re-allocating memory each time the splitter is called.
struct PerThreadCacheV2 {
  // Cache for example bucket sets.
  // The postfix digit is only used to differentiate between the objects. There
  // is not special semantic to it.











  // Cache for the label score accumulator;


  // Mask of selected examples.

  // Selected bucket indices;

  // Selected categorical attribute values;

// Get the example bucket set from the thread cache.
template <typename ExampleBucketSet>
  // Numerical.
    // Unweighted Numerical.
    // Hessian Numerical.
    // Unweighted Hessian Numerical.
    // Categorical.
    // Unweighted Categorical.
    // Binary Categorical.
    // Unweighted Binary Categorical.
    // Uplift categorical
    // Uplift numerical

// Get the label score accumulator from the cache.
template <typename LabelScoreAccumulator>

template <typename ExampleBucketSet, bool require_label_sorting>
void FillExampleBucketSet(
  // Allocate the buckets.

  // Initialize the buckets.
  int bucket_idx = 0;

  // Fill the buckets.

  // Finalize the buckets.

  //  Sort the buckets.



template <typename LabelScoreAccumulator, typename Initializer>


// Scans the buckets iteratively. At each iteration evaluate the split that
// could put all the already visited buckets in the negative branch, and the non
// visited buckets in the positive branch.
template <typename ExampleBucketSet, typename LabelScoreAccumulator,
          bool bucket_interpolation = false>



  // Initialize the accumulators.
  // Initially, all the buckets are in the positive accumulators.


  // Running statistics.
  bool tried_one_split = false;


  int best_bucket_idx = -1;
  int best_bucket_interpolation_idx = -1;

  // If true, a new best split was found ("best_bucket_idx" was set accordingly)
  // but no new examples were observed (i.e. all the bucket visited since the
  // last new best split were empty).
  bool no_new_examples_since_last_new_best_split = false;





    // Remove the bucket from the positive accumulator and add it to the
    // negative accumulator.



    // Enough examples?






      // Memorize the split.


    // Finalize the best found split.

        // Bucket interpolation.


// Scans the buckets (similarly to "ScanSplits"), but in the order specified by
// "bucket_order[i].second" (instead of the bucket order).
template <typename ExampleBucketSet, typename LabelScoreAccumulator,



  // Initialize the accumulators.
  // Initially, all the buckets are in the positive accumulators.


  // Running statistics.
  bool tried_one_split = false;



  int best_bucket_idx = -1;
  int best_order_idx = -1;


    // Remove the bucket from the positive accumulator and add it to the
    // negative accumulator.



    // Enough examples?


    // Compute the split's score.

      // Memorize the split.

    // Finalize the best found split.


// Scans the buckets (similarly to "ScanSplits") from a pre-sorted sparse
// collection of buckets. This method is used for features with ordering (e.g.
// numerical features) that are pre-sorted. When applicable, this method is
// equivalent (but more efficient) than the classical "ScanSplits" function
// (that would require extraction + sorting the buckets).
//
// Args:
//   - total_num_examples: Total number of examples in the training dataset.
//   - selected_examples: Index of the active training examples. All values
//     should be in [0,total_num_examples).
//   - sorted_attributes: List of sorted attribute values.
//   - feature_filler: Access to the raw feature data.
//   - label_filler: Access to the raw label data.
//   - min_num_obs: Minimum number of examples (non-weights) in each side of the
//     split.
//   - attribute_idx: Attribute being tested.
//   - condition: Input and output condition.
//   - cache: Utility cache data.
//   - duplicate_examples: If true, "selected_examples" can contain multiple
//     times the same example.
template <typename ExampleBucketSet, typename LabelScoreAccumulator,
          bool duplicate_examples = true>

  // Compute a mask (duplicate_examples=false) or count
  // (duplicate_examples=true) of the selected examples.
  auto get_mask = [&]() -> const auto& {

  // Initialize the accumulators. Initially, all the buckets are in the positive
  // accumulators.


  // Running statistics.

  // At least one split was tested.
  bool tried_one_split = false;
  // At least one better split was found.
  bool found_split = false;


  // Statistics of the best split found so far.

  // A new (i.e. different) attribute value was observed in the scan since the
  // last score test.
  bool new_attribute_value = false;

  // Index of the nearest previous example with a  value different from the
  // current example (i.e. the "sorted_example_idx" example).

  // Iterate over the attribute values in increasing order.
  // Note: For some reasons, the iterator for-loop is faster than the
  // for(auto:sorted_attributes) for loop (test on 10 different compiled
  // binaries).

    auto example_idx = sorted_attribute & SparseItemMeta::kMaskExampleIdx;

    // Skip non selected examples.

    // Prefetch the label information needed at the end of the loop body.

    // Test Split
        // Compute the split's score.

          // A better split was found. Memorize the split.

    // Update positive and negative accumulators.
    // Remove the bucket from the positive accumulator and add it to the
    // negative accumulator.

    // Finalize the best found split.
    // TODO: Experiment with random splits in ]best_previous_feature_value,
    // best_feature_value[.




// Call "ScanSplitsPresortedSparseDuplicateExampleTemplate" after unfolding the
// "duplicate_examples" argument. See
// "ScanSplitsPresortedSparseDuplicateExampleTemplate" for the function
// documentation.
template <typename ExampleBucketSet, typename LabelScoreAccumulator>

// Generates and evaluates random assignments of buckets to the positive or
// negative branches. Used to learn categorical splits of the form "value \in
// mask", where "mask" was selected from a randomly generates set of masks.
template <typename ExampleBucketSet, typename LabelScoreAccumulator>

    // Not enough examples.

    // Invalid bucket set.

  // Initialize the accumulators.
  // Initially, all the buckets are in the positive accumulators.


  // Running statistics.
  bool tried_one_split = false;



  // List the non empty buckets.
    // All the examples have the same attribute value.



      // Not enough examples in the positive branch.

      // Not enough examples in the negative branch.




      // Better split found. Memorize it.

    // Finalize the best found split.
    // Note: The bucket are sorted by index i.e. best_pos_buckets[i] ==
    // example_bucket_set.items[i].feature.value.

// Find the best possible split (and update the condition accordingly) using
// a simple "scan" of the buckets.  See "ScanSplits".
template <typename ExampleBucketSet, typename LabelBucketSet,
          bool require_label_sorting, bool bucket_interpolation = false>

  // Create buckets.

  // Scan buckets.

// Find the best possible split (and update the condition accordingly) using
// a random scan of the buckets.  See "ScanSplitsRandomBuckets".
template <typename ExampleBucketSet, typename LabelBucketSet>

  // Create buckets.

  // Scan buckets.

// Adds the content's of "src" label bucket to "dst"'s label bucket.
template <typename ExampleBucketSet>
void AddLabelBucket(const ExampleBucketSet& src, ExampleBucketSet* dst) {

// Pre-defined ExampleBucketSets

// Label: Regression.

template <bool weighted>

template <bool weighted>

template <bool weighted>

template <bool weighted>

template <bool weighted>

template <bool weighted>

// Label: Classification.






// Label: Unweighted Classification.






// Label: Binary Classification.






// Label: Unweighted Binary Classification.






// Label: Hessian Regression.

template <bool weighted>

template <bool weighted>

template <bool weighted>

template <bool weighted>

template <bool weighted>

template <bool weighted>

// Label : Uplift categorical




// Label : Uplift numerical






// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/training.cc =====








// Generates a failure absl status if the configuration contains monotonic
// constraints.
absl::Status FailIfMonotonic(

// Number of trials to run when learning a categorical split with randomly
// generated masks.
//
// Args:
//   config: A random categorical split learning configuration.
//
// Returns:
//   A function active_dictionary_size -> number_of_trials.
//
// The "active_dictionary_size" is the number of unique categorical values that
// are present at least once in the training examples of the node.

// Helper function to set a condition statistics. Do not set the following
// fields: "mutable_condition" and "na_value".
void SetConditionHelper(

// Helper function to set a condition statistics.
// Similar as "SetConditionHelper" above, but for a regression problem.
void SetConditionHelper(

// Computes and set in "na_replacement" the value to use as a replacement of
// missing values when the "local imputation" strategy is used.
//
// Explanation: The "local imputation" strategy to handle missing values
// consists in replacing these missing values by the mean of the feature in the
// training dataset.
//
// If the feature only contains missing values, the "na_replacement" argument is
// left unchanged.
//
// `weights` may be empty which is equivalent to unit weights.
void LocalImputationForNumericalAttribute(

// Similar as "LocalImputationForNumericalAttribute", but for a categorical
// attribute. Return the most frequent attribute value.
void LocalImputationForCategoricalAttribute(

// Similar to "LocalImputationForCategoricalAttribute", but for a boolean
// attribute. Returns the most frequent attribute value.
void LocalImputationForBooleanAttribute(

// Return the minimum and maximum values of a numerical attribute.
// Return false if there is no min-max e.g. selected_examples is empty or all
// the values are NAs.
bool MinMaxNumericalAttribute(
  float local_min_value = 0;
  float local_max_value = 0;
  bool first = true;

// Search for the attribute item that maximize the immediate variance reduction.
template <bool weighted>
  // Search for the attribute item that maximize the variance reduction. Note:
  // We ignore attribute that reduce the current variance reduction.
  int best_attr_value = -1;
      // The attribute value was already selected or was excluded from the
      // search space.
    // Compute the variance reduction of the already selected values
    // "positive_attributes_vector" and the candidate value
    // "candidate_attr_value".
        // The example is already in the positive set.

      // Search if X = attribute_bank[ attribute_values[example_idx].first,
      // attribute_values[example_idx].second ] contains the current candidate
      // attribute value "candidate_attr_value".
      //
      // We use "running_attr_bank_idx[select_idx]" that contains the index in
      // "X" of the last tested candidate attribute.
      //
      // Note: "X" does not contains duplicates and is sorted in increasing
      // order. The candidate attributes "candidate_attr_value" are testing in
      // increasing order.
      bool match = false;

        // Add the example to the positive set and remove it from the
        // negative.
    // Remove the attribute from the candidate set if the attribute is pure
    // for the current negative set.

      // Best score so far.

// Select which sorting strategy to use effectively.
//
// If the strategy is AUTO or PRESORTED, the fastest / selected strategy depends
// on the data.

    // The internal configuration configured by the learning algorithm takes
    // precedence on the sorting strategy.
    // Otherwise, the training configuration (controlled by the user or
    // unit-test controller) selects the strategy.
    // User specified strategy.



// Specialization in the case of classification.














      // Condition of the type "Attr is True".




  // Condition of the type "Attr is NA".








      // Condition of the type "Attr >= threshold".



      // Condition of the type "Attr >= threshold".


      // Condition of the type "Attr \in X".


      // Condition of the type "Attr is True".




  // Condition of the type "Attr is NA".


// Specialization in the case of regression.






      // Condition of the type "Attr >= threshold".


      // Condition of the type "Attr >= threshold".

      // Condition of the type "Attr \in X".


      // Condition of the type "Attr is True".



  // Condition of the type "Attr is NA".


// Specialization in the case of uplift with categorical outcome.








  // Condition of the type "Attr is NA".


// Specialization in the case of uplift with numerical outcome.








  // Condition of the type "Attr is NA".





    // An oblique split cannot be invalid.









  // Single Thread Setup.

  // Was a least one good split found?
  bool found_good_condition = false;

      // Nothing to do.

  // Get the indices of the attributes to test.
  int remaining_attributes_to_test;

  // Index of the next attribute to be tested in "candidate_attributes".
  int candidate_attribute_idx_in_candidate_list = 0;

    // Get the attribute data.










  // This method looks for the best split using worker threads.
  //
  // Background:
  // The best split is the split with the best score among the first
  // "min_num_jobs_to_test" valid split searches. The order of the split is
  // defined by the list of candidate attributes generated by
  // GetCandidateAttributes. A split search is valid if it tested at least one
  // split.
  //
  // Since the execution is multi-threaded, splits are computed/evaluated in
  // unpredictable order. However, this method's result is as if the splits were
  // evaluated sequentially according to the order defined by
  // GetCandidateAttributes. Oblique splits are always evaluated (if requested
  // by the user).
  //
  // A work unit (called a "job") is the evaluation of a single attribute or, in
  // the case of oblique splits, the evaluation of a given number of random
  // projections. A job with a given idx can be in one of multiple states (in
  // chronological order):
  //
  // 1. Before being scheduled (idx >= next_job_to_schedule).
  // 2. Scheduled and being computed by a worker.
  // 3. The worker is done with the computation, and the result was recorded by
  // the manager (cache->durable_response_list[idx].set).
  // 4. The result was processed by the manager (idx < next_job_to_process).
  //
  // This method guarantees that the jobs/splits are processed in order.
  //
  // Note that next_job_to_process < next_job_to_schedule always holds.



  // Constant and static part of the requests.

  // Computes the number of oblique projections to evaluate and how to group
  // them into requests.
  int num_oblique_jobs = 0;
  int num_oblique_projections;
  int num_oblique_projections_per_oblique_job;


        // Arbitrary minimum number of oblique projections to test in each job.
        // Because oblique jobs are expensive (more than non oblique jobs), it
        // is not efficient to create a request with too little work to do.
        //
        // In most real cases, this parameter does not matter as the limit is
        // effectively constraint by the number of threads.


  // Prepare caches.

  // Get the ordered indices of the attributes to test.
  int min_num_jobs_to_test;

  // All the oblique jobs need to be done.
  // Note: When do look for oblique splits, we also run the classical numerical
  // splitter.


  // Marks all the caches "available".

  // Marks all the duration responses as "non set".

  // Score and value of the best found condition.

  // Get Channel readers and writers.

  // Number of jobs currently scheduled.
  int num_in_flight = 0;

  // Helper function to create a WorkRequest.
  //
  // If attribute_idx is != -1 create a request for an axis-aligned split.
  //
  // If attribute_idx is == -1 and num_oblique_projections_to_run != -1, create
  // a request for an oblique split.
  //
  auto build_request =

  // Schedule all the oblique jobs.
  int next_job_to_schedule = 0;
    int num_projections_in_request;


  // Schedule some non-oblique jobs if threads are still available.


  int num_valid_job_tested = 0;
  int next_job_to_process = 0;

  absl::Status status;

    // Get a new result from a worker splitter.
    auto maybe_response = processor.GetResult();


      // Record, but do not process, the worker response.
      auto response_or = std::move(maybe_response).value();
      auto response = std::move(response_or).value();
      // Release the cache immediately to be reused by other workers.

      // Record response for further processing.
        // The worker found a potentially better solution.

    // Process new responses that can be processed.
      // Something to process.
      auto durable_response =


      // Enough jobs have been tested to take a decision.

      // We have processed all the jobs.

    // Schedule the testing of more conditions.

  // Drain the response channel.
    auto maybe_response = processor.GetResult();
      // The channel was closed.
    auto response_or = std::move(maybe_response).value();

  // Move the random generator state to make the behavior deterministic.



    // Multi-thread.

  // Single thread.


























    float na_replacement, const UnsignedExampleIdx min_num_obs,

  // Determine the minimum and maximum values of the attribute.
  float min_value, max_value;
  // There should be at least two different unique values.
  // Randomly select some threshold values.
  struct CandidateSplit {
    float threshold;
    bool operator<(const CandidateSplit& other) const {



  // Compute the split score of each threshold.
    float attribute = attributes[example_idx];
    auto it_split = std::upper_bound(



  // Select the best threshold.
  bool found_split = false;



    float na_replacement, const UnsignedExampleIdx min_num_obs,



  // "Why ==3" ?
  // Categorical attributes always have one class reserved for
  // "out-of-vocabulary" items. The "num_label_classes" takes into account this
  // class. In case of binary classification, "num_label_classes" is 3 (OOB,
  // False, True).
    // Binary classification.

    // Multi-class classification.



    // Binary classification.


    // Multi-class classification.



template <bool weighted>

  // Determine the minimum and maximum values of the attribute.
  float min_value, max_value;

  // There should be at least two different unique values.
  // Randomly select some threshold values.
  struct CandidateSplit {
    float threshold;
    bool operator<(const CandidateSplit& other) const {


  // Compute the split score of each threshold.
    float attribute = attributes[example_idx];

    auto it_split = std::upper_bound(


  // Select the best threshold.
  int best_candidate_split_idx = -1;


template <bool weighted>
    float na_replacement, UnsignedExampleIdx min_num_obs,






template absl::StatusOr<SplitSearchResult>
    float na_replacement, UnsignedExampleIdx min_num_obs,

template absl::StatusOr<SplitSearchResult>
    float na_replacement, UnsignedExampleIdx min_num_obs,

template <bool weighted>
    int num_bins, const std::vector<float>& gradients,




template <bool weighted>






template absl::StatusOr<SplitSearchResult>

template absl::StatusOr<SplitSearchResult>

template <bool weighted>




    // Binary classification.




    // Multi-class classification.



template <bool weighted>




    bool na_replacement, UnsignedExampleIdx min_num_obs,


    // Binary classification.




    // Multi-class classification.





template <bool weighted>




template absl::StatusOr<SplitSearchResult>
template absl::StatusOr<SplitSearchResult>

template <bool weighted>
    bool na_replacement, const UnsignedExampleIdx min_num_obs,




template <bool weighted>







template <bool weighted>








  // Bitmap of available attribute values. During the course of the algorithm,
  // an attribute value is available if:
  //  - It is selected by the initial random sampling of candidate attribute
  //  values.
  //  - It is not (yet) selected in the positive set.
  //  - It is not pure in the negative examples i.e. it is not present in all
  //  or in none of the non-selected examples (ps: Initially, all the examples
  //  are non-selected).
  // The "positive attribute set" are the attribute values that, if present
  // in the example, evaluates the node condition as true.
  // Bitmap of the example that are already in the positive set i.e. for which
  // the condition defined by "positive_attributes_vector" is positive.
  // Instead of being indexed by the example_idx, this bitmap is indexed by
  // "selected_examples" i.e. "positive_selected_example_bitmap[i]==true"
  // means that "selected_examples[i]" is selected.
  // Weighted and non weighted distribution of the labels in the positive and
  // negative sets.
  // All the examples are initially in the negative set.
  // Number of example (with weights) where the attribute value (an attribute
  // value is a set of categorical items) that contains the i-th  categorical
  // items.
  // Count per categorical item value.
  // Sample-out items.

  // Contains, for each example, the index in the "attribute_bank"
  // corresponding to the next candidate attribute value
  // "candidate_attr_value".
  //
  // When initialized, this corresponds to the index of the first values for
  // this particular example: i.e. running_attr_bank_idx[select_idx] ==
  // attribute_values[selected_examples[select_idx]].first.

  int num_iterations = 0;

  // Information gain of the current condition i.e.
  // "positive_attributes_vector".

    // Initialize the running attribute bank index.
    // Search for the attribute item that maximize the information gain. Note:
    // We ignore attribute that reduce the current information gain.
    int best_attr_value = -1;
        // The attribute value was already selected or was excluded from the
        // search space.
      // Compute the information gains of the already selected values
      // "positive_attributes_vector" and the candidate value
      // "candidate_attr_value".
          // The example is already in the positive set.

        // Search if X = attribute_bank[ attribute_values[example_idx].first,
        // attribute_values[example_idx].second ] contains the current
        // candidate attribute value "candidate_attr_value".
        //
        // We use "running_attr_bank_idx[select_idx]" that contains the index
        // in "X" of the last tested candidate attribute.
        //
        // Note: "X" does not contains duplicates and is sorted in increasing
        // order. The candidate attributes "candidate_attr_value" are testing
        // in increasing order.
        bool match = false;

          // Add the example to the positive set and remove it from the
          // negative.
          float weight = weights.empty() ? 1.f : weights[example_idx];
      // Remove the attribute from the candidate set if the attribute is pure
      // for the current negative set.

        // Best score so far.
    // Check if a satisfying attribute item was found.
    // Add the attribute item to the positive set.
    // Update the label distributions in the positive and negative sets.
        // The example is already in the positive set.
        float weight = weights.empty() ? 1.f : weights[example_idx];


    // Assign the positive set to the condition.

template <bool weighted>
  // Bitmap of available attribute values. During the course of the algorithm,
  // an attribute value is available if:
  //  - It is selected by the initial random sampling of candidate attribute
  //  values.
  //  - It is not (yet) selected in the positive set.
  //  - It is not pure in the negative examples i.e. it is not present in all
  //  or in none of the non-selected examples (ps: Initially, all the examples
  //  are non-selected).
  // The "positive attribute set" are the attribute values that, if present
  // in the example, evaluates the node condition as true.
  // Bitmap of the example that are already in the positive set i.e. for which
  // the condition defined by "positive_attributes_vector" is positive.
  // Instead of being indexed by the example_idx, this bitmap is indexed by
  // "selected_examples" i.e. "positive_selected_example_bitmap[i]==true"
  // means that "selected_examples[i]" is selected.
  // Weighted and non weighted distribution of the labels in the positive and
  // negative sets.
  // All the examples are initially in the negative set.
  // Number of example (with weights) where the attribute value (an attribute
  // value is a set of categorical items) that contains the i-th  categorical
  // items.
  // Count per categorical item value.

  // Sample-out items.

  // Contains, for each example, the index in the "attribute_bank"
  // corresponding to the next candidate attribute value
  // "candidate_attr_value".
  //
  // When initialized, this corresponds to the index of the first values for
  // this particular example: i.e. running_attr_bank_idx[select_idx] ==
  // attribute_values[selected_examples[select_idx]].first.

  // Variance reduction of the current condition i.e.
  // "positive_attributes_vector".
    // Initialize the running attribute bank index.
    // Find the attribute with best immediate variance reduction.
    int best_attr_value;
    // Check if a satisfying attribute item was found.
    // Add the attribute item to the positive set.
    // Update the label distributions in the positive and negative sets.
        // The example is already in the positive set.

    // Assign the positive set to the condition.

template absl::StatusOr<SplitSearchResult>

template absl::StatusOr<SplitSearchResult>

template <typename LabelBucket, typename ExampleBucketSet,


  // Create buckets.

  // Scanner for the "one label value vs others".
    // Value and index of the buckets.

        // Never observed label value.
        // "True vs others" or "False vs others" are equivalent for binary
        // classification.

      // Order value of the buckets.

      // Sort the bucket indices.

      // Scan the buckets in order.

  // Scanner for the "one hot" type condition i.e. conditions of the type:
  // "attribute == value".
  //
  // Note: In the majority of cases, one-hot (on attribute value) is worst that
  // "one class vs others". This is however a common solution, and this code is
  // present for comparison purpose.




    bool tried_one_split = false;
    int best_bucket_idx = -1;



      // Enough examples?




        // Memorize the split.

      // Finalize the best found split.







    // Binary classification.
    // Multi-class classification.




  // TODO: Add support for-presorted splitting.





  // TODO: Add support for pre-sorted splitting.





  // TODO: Add support for pre-sorted splitting.








  // TODO: Add support for pre-sorted splitting.





int NumAttributesToTest(const proto::DecisionTreeTrainingConfig& dt_config,
  int num_attributes_to_test;
  // User specified number of candidate attributes.

  // Automatic number of attribute selection logic.

  // Special value to use all the available attributes.

  // Make sure we don't select more than the available attributes.


void GetCandidateAttributes(


absl::Status GenerateRandomImputation(

absl::Status GenerateRandomImputationOnColumn(
  // Extract the indices of the example with non-na values i.e. the candidate
  // for sampling.




      // Sample a random non-na value.

void SetInternalDefaultHyperParameters(

void SetDefaultHyperParameters(proto::DecisionTreeTrainingConfig* config) {
  // Emulation of histogram splits.

  // By default, use axis aligned splits.

  // By default, use the cart categorical algorithm for categorical features.

  // By default, use the local growing strategy i.e. divide and conquer.

  // Change the pre-sorting strategy if not supported by the splitter.
  auto sorting_strategy = config->internal().sorting_strategy();

  // If possible, use presorting by default.



  // The binary weight hyperparameter is deprecated for the more general weights
  // hyperparameter.

  // By default, we use binary weights.

absl::Status GrowTreeBestFirstGlobal(




  struct CandidateSplit {
    // Split.
    // Indices of examples in the node.
    // Global score of the split.
    float score;
    // The currently leaf node.
    // Depth of the node.
    int depth;

    bool operator<(const CandidateSplit& other) const {

  // List of candidate splits.

  // Initialize a node and update the list of candidate splits with a given
  // node.

      // Stop the grow of the branch.
      // No good condition found. Close the branch.



  // Total number of nodes in the tree.
  int num_nodes = 1;


    // Ensure the candidate set is not larger than  "max_num_nodes". Note:
    // There is not need for mode than "max_num_nodes" candidate splits.

    // Split the node.
    auto split = candidate_splits.top();



    // Add new candidate splits for children.

        auto exemple_split,


  // Finalize the remaining candidates.

absl::Status DecisionTreeTrain(
  // Note: This function is the entry point of all decision tree learning.

  // Check the sorting strategy.

  // Decide if execution should happen in single-thread or concurrent mode.


  // Fail if the data spec has invalid columns.

  // Check monotonic constraints

  // Check if oblique splits are correctly specified

    // Split the examples in two parts. One ("selected_examples_buffer") will be
    // used to infer the structure of the trees while the second
    // ("leaf_examples_buffer") will be used to determine the leaf values (i.e.
    // the predictions).


    // Reduce the risk of std::vector re-allocations.



  auto leaf_example_span = leaf_examples.has_value()




absl::Status FindBestConditionStartWorkers(
  auto find_condition =

absl::Status DecisionTreeCoreTrain(

  auto selected_examples_rb = SelectedExamplesRollingBuffer::Create(


absl::Status NodeTrain(

    // Set the node value (i.e. the label distribution).

      // Override the leaf values.

    // Stop the growth of the branch.

  // Dataset used to train this node.
  // If true, the entire dataset "local_train_dataset" is composed of training
  // examples for this node. If false, only the subset of
  // "local_train_dataset" indexed by "selected_examples" are to be considered
  // for this node i.e. local_train_dataset[selected_examples[i]].
  bool splitter_dataset_is_compact;

  // Extract the random local imputation.


  // Determine the best split.

    // No good condition found. Close the branch.

  // Separate the positive and negative examples.
      auto example_split,

    // The splitter statistics don't match exactly the condition evaluation and
    // one of the children is pure.

  // Separate the positive and negative examples used only to determine the node
  // value.

  // Set leaf outputs

  // Children constraints
  auto pos_constraints = constraints;
  auto neg_constraints = constraints;

  // Positive child.

  // Negative child.

absl::Status ApplyConstraintOnNode(const NodeConstraints& constraint,

absl::Status DivideMonotonicConstraintToChildren(

  // TODO: Experiment with other ways to select limit.
  float limit = parent_node->node().regressor().top_value();

    // A failure is indicative of an issue with the splitter i.e. the
    // "FindCondition" function call just before.








bool MaskPureSampledOrPrunedItemsForCategoricalSetGreedySelection(
      // Too much candidate items.
      // Randomly masked item.
      // Pure item.





  // The following test ensure that the effective number of positive examples is
  // equal to the expected number of positive examples. A miss alignment
  // generally indicates that the splitter does not work correctly.
  //
  // Incorrectly working splitters can make the model worst than expected if
  // the error happens often. If such error happen rarely, the impact is likely
  // insignificant.
  //
  // This generates an error in unit testing and a warning otherwise.
      // Logging this message too often will crash.



// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/training.h =====






// A collection of objects used by split-finding methods.
//
// The purpose of this cache structure is to avoid repeated allocation of the
// contained objects over the course of training.
//
// A splitter (e.g. "FindBest...") is guarantied exclusive access at any time
// (thread safe). This structure is passed as place holder. Assume it is filled
// with garbage, it's not cleaned. The only thing preserved between calls to
// splitters is the size of the vectors, and the fact that they are
// pre-allocated from previous caller to specific purpose (e.g.
// "numerical_features" is allocated to contains one item for each numerical
// features).
struct SplitterPerThreadCache {
  // Objects used by splitters.




// Applies a constraint over a leaf.
absl::Status ApplyConstraintOnNode(const NodeConstraints& constraint,

// Divides a monotonic constraint over node's children.
absl::Status DivideMonotonicConstraintToChildren(
    bool check_monotonic, NodeWithChildren* parent_node,

// Set of immutable arguments in a splitter work request.
struct SplitterWorkRequestCommon {

// Data packed with the work request that can be used by the manager to pass
// information to itself.
struct SplitterWorkManagerData {
  //  Index of the condition in the cache pool.
  int cache_idx;
  // Index of the job.
  int job_idx;

// Work request for a splitter i.e. finding the best possible condition for a
// given attribute on a given dataset.
struct SplitterWorkRequest {


  // The attribute index to pass onto splitters.
  int attribute_idx;

  // Non-owning pointer to an entry in PerThreadCache.splitter_cache_list.

  // Set of immutable arguments passed onto splitters.
  // Seed used to initialize the random generator.
  // If not -1, search for oblique split. In this case "attribute_idx" should be
  // -1.
  int num_oblique_projections_to_run;

  // Copy is not allowed.
                      int num_oblique_projections_to_run)

// Contains the result of a splitter.
struct SplitterWorkResponse {

  // The status returned by a splitter.


  // Copy is not allowed.


// Records the status of workers in a concurrent setup.
// Part of the worker response (SplitterWorkResponse) that need to be kept in
// order to simulate sequential feature splitting.
struct SplitterWorkDurableResponse {

  // The status returned by a splitter.

  // If not set, the other fields are not meaningful.
  bool set;

// Memory cache used to reduce the number of allocation / de-allocation of
// memory during training. One mutable "PerThreadCache" object is required by
// the "train" method.
//
// A "PerThreadCache" object can be reused in successive calls to "train", but
// NOT for concurrent calls.
//
// Details: This cache allows to reduce the multithread locking of tcmalloc.
// Without the cache, most of the training time is spent in memory management.
struct PerThreadCache {
  // Object used by the splitter manager.

  // A set of objects that are used by FindBestCondition.

  // List of available indices into splitter_cache_list.

  // Used to handle selected example indices.

  // Used to handle selected leaf example indices.

// In a concurrent setup, this structure encapsulates all the objects that are
// needed to communicate with splitter workers.
struct SplitterConcurrencySetup {
  // Whether concurrent execution has been enabled.
  bool concurrent_execution;
  // The number of threads available in the worker pool.
  int num_threads;

  // Distributed split finder.

// Signature of a function that sets the value (i.e. the prediction) of a leaf
// from the gradient label statistics.

// Find the best condition for a leaf node. Return true if a condition better
// than the one initially in `best_condition` was found. If `best_condition` is
// a newly created object, return true if a condition was found (since
// `best_condition` does not yet define a condition).
//
// This is the entry point / main function to call to find a condition.

// Following are the method to handle multithreading in FindBestCondition.
// =============================================================================

// Dispatches the condition search to either single thread or multithread
// computation.

// Single thread search for conditions.

// Multi-thread search for conditions.

// Starts the worker threads needed for "FindBestConditionConcurrentManager".
absl::Status FindBestConditionStartWorkers(

// A worker that receives splitter work requests and dispatches those to the
// right specialized splitter function.
//
// Important: This function closes the "out" channel upon termination.

// Following are the "FindBestCondition" specialized for specific tasks.
// =============================================================================






// Following are the "FindBestCondition" specialized for both a task (i.e. label
// semantic) and feature semantic. The function names follow the pattern:
// FindSplitLabel{label_type}Feature{feature_type}{algorithm_name}.
//
// Some splitters are only specialized on the feature, but not one the label
// typee (e.g. "FindBestConditionOblique";
// =============================================================================

// Search for the best split of the type "Attribute is NA" (i.e. "Attribute is
// missing") for classification.

// Search for the best split of the type "Attribute is NA" (i.e. "Attribute is
// missing") for regression.
template <bool weighted>




// Search for the best split of the type "Attribute is NA" (i.e. "Attribute is
// missing") for hessian regression.
template <bool weighted>

// Search for the best split of the type Boolean for classification.
    bool na_replacement, UnsignedExampleIdx min_num_obs,

// Search for the best split of the type Boolean for regression.
template <bool weighted>

template <bool weighted>
    bool na_replacement, UnsignedExampleIdx min_num_obs,

// Search for the best split for a numerical attribute and a categorical label
// using the CART algorithm for a dataset loaded in memory. All the threshold
// are evaluated. All Na values are replaced by "na_replacement". Does not
// support weights.
//
// Currently the code only support conditions of the form: attribute>=threshold.
//
// "labels" and "attributes" contain respectively the label and attribute value
// of all the example in the dataset. Both vectors have the same size. From this
// dataset, only the example indexed in "selected_examples" should be considered
// to evaluation the quality of a split i.e. only consider the examples defined
// by attributes[selected_examples[i]] and labels[selected_examples[i]]. Note:
// As an example, non selected examples could be in other part of the tree, OOB
// or excluded for cross-validation purpose.
//
// "num_label_classes" represents the number of label classes i.e. all the
// number in "label" should be in [0, num_label_classes[.
//
// "na_replacement" replaces the NA (non-available) values in "attributes[i]"
// when evaluating the splits i.e. given a split threshold "t", an example "i"
// with attributes[i]==NA will be considered positive iif  "na_replacement" >=t.
//
// "min_num_obs" is the minimum number of (training) observations in either side
// of the split. Splits that invalidate this constraint are ignored.
//
// "label_distribution" is the label distribution of the selected examples i.e.
// the distribution of "labels" for all elements in "selected_examples".
//
// "attribute_idx" is the column index of the attribute. This argument is not
// used for computation. Instead, it is used to update the "condition" is a good
// split is found.
//
// "condition" will contain the best valid found split (if any). If "condition"
// already contains a valid condition (i.e. "condition.split_score()" is set),
// "condition" will be updated iff a new found split has a better score than the
// split initially contained in "condition".
//
// The function returns kBetterSplitFound is a better split was found,
// kNoBetterSplitFound if there are some valid splits but none of them were
// better than the split initially in "condition", and kInvalidAttribute is not
// valid split was found.
//
// `weights` may be empty and this is equivalent to unit weights.
    float na_replacement, UnsignedExampleIdx min_num_obs,

// Similarly to "FindSplitLabelClassificationFeatureNumericalCart", but uses an
// histogram approach to find the best split.
//
// `weights` may be empty and this is equivalent to unit weights.
    float na_replacement, UnsignedExampleIdx min_num_obs,

// Similar to "FindSplitLabelClassificationFeatureNumericalCart", but work on
// pre-discretized numerical values.
//
// `weights` may be empty and this is equivalent to unit weights.
    int num_bins, const std::vector<int32_t>& labels, int32_t num_label_classes,

// Search for the best split for a numerical attribute and a numerical label
// using the CART algorithm for a dataset loaded in memory.
//
// This functions works similarly as
// "FindSplitLabelClassificationFeatureNumericalCart" for categorical labels.
template <bool weighted>

template <bool weighted>
    float na_replacement, UnsignedExampleIdx min_num_obs,

template <bool weighted>
    int num_bins, const std::vector<float>& gradients,

// Similarly to "FindSplitLabelClassificationFeatureNumericalCart", but uses an
// histogram approach to find the best split.
template <bool weighted>

// Similar to "FindSplitLabelClassificationFeatureNumericalCart", but work on
// pre-discretized numerical values.
template <bool weighted>
    int num_bins, const std::vector<float>& labels,

// Looks for the best split for a categorical attribute and a categorical label
// using the algorithm configured in "dt_config" for a dataset loaded in memory.
// Such split is defined as a subset of the possible values of the attribute.
// During inference, the node test will succeed iif the attribute value is
// contained in this subset.
//
// Arguments are similar to "FindSplitLabelClassificationFeatureNumericalCart".
// "num_attribute_classes" specifies the number of classes of the attribute
// (i.e. the maximum value for the elements in "attributes").
//
// `weights` may be empty and this is equivalent to unit weights.

// Looks for the best split for a categorical attribute and a numerical
// label using the algorithm set in "dt_config" for a dataset loaded in memory.
//
// This function works similarly as
// "FindSplitLabelClassificationFeatureCategorical" for categorical labels.
template <bool weighted>

template <bool weighted>

// Looks for the best split for a categorical set attribute and a categorical
// label for a dataset loaded in memory.
//
// The categorical set split is defined by examples where the intersection of
// "positive_set" and the examples attribute (a categorical set) is empty /
// non-empty.
//
// The best split algorithm works by greedily selecting categorical values into
// the "positive_set". It can be described as follow:
//
//    positive_set = {}
//    Sample uniformly a subset of candidate attribute item "candidate_items".
//    score = 0
//    while true
//      Find "a" in "candidate_items" such that the score of
//      "positive_set + a" (intersection) is maximized. This score is the
//      "new_score".
//      if new_score < score
//        break
//      add "s" to "positive_set".
//      score = new_score.
//    return positive_set
//
// `weights` may be empty and this is equivalent to unit weights.

// Similar as the previous
// "FindSplitLabelClassificationFeatureCategoricalSetGreedyForward", but for
// regression.
// The "information gain" is replaced by the "variance reduction".
template <bool weighted>

// Find the best possible condition for a uplift with categorical treatment,
// a numerical feature and categorical outcome.

// Find the best possible condition for a uplift with categorical treatment,
// a numerical feature and numerical outcome.

// Find the best possible condition for a uplift with categorical treatment,
// a categorical feature, and categorical outcome.

// Find the best possible condition for a uplift with categorical treatment,
// a categorical feature and numerical outcome.

// Find the best possible oblique condition.

// End of the FindBestCondition specialization.
// =============================================================================

// Returns the number of attributes to test ("num_attributes_to_test") and a
// list of candidate attributes to test in order ("candidate_attributes").
// "candidate_attributes" is guaranteed to have at least
// "num_attributes_to_test" elements.
void GetCandidateAttributes(

// Generate a random imputation of NA (i.e. missing) values i.e.
// Copy the attributes in "attributes" from the examples indexed by "examples"
// from the source dataset "src" into the "dst" dataset while replacing the
// missing values from samples from the "examples" in "src". Columns not
// specified by "attributes" are not copied. If, for a given attribute, all the
// values are NA, the data is simply copied i.e. "dst" will contain na values.
absl::Status GenerateRandomImputation(

// Random imputation on a given column. See documentation of
// "GenerateRandomImputation".
absl::Status GenerateRandomImputationOnColumn(

// Grows a decision tree with a "best-first" (or "leaf-wise") grow i.e. the leaf
// that best improve the overall tree is split.
absl::Status GrowTreeBestFirstGlobal(

// The core training logic that is the same between single-threaded execution
// and concurrent execution.
//
// If "leaf_examples" is non null, it contains the examples to use to determine
// the value of the leaves while "selected_examples" contains the examples to
// use to determine the structure of the tree. If "leaf_examples" is null, the
// examples "selected_examples" are used for both.
//
// The "selected_examples" buffer will be modified during training.
absl::Status DecisionTreeCoreTrain(

// Train the tree. Fails if the tree is not empty.
absl::Status DecisionTreeTrain(

// Train a node and its children.
absl::Status NodeTrain(

// Set the default values of the hyper-parameters.
void SetDefaultHyperParameters(proto::DecisionTreeTrainingConfig* config);

// Set the default values of the internal hyper-parameters. Should be called
// after "SetDefaultHyperParameters". Does not change user visible
// hyper-parameters.
void SetInternalDefaultHyperParameters(

// Number of attributes to test when looking for an optimal split.
int NumAttributesToTest(const proto::DecisionTreeTrainingConfig& dt_config,
                        int num_attributes, model::proto::Task task);

// Returns -1 if a feature is decreasing monotonic, +1 if a feature is a
// increasing monotonic, and 0 if a feature is not constrained.


// Initializes the item mask i.e. the bitmap of the items to consider or to
// ignore in the greedy selection for categorical-set attributes. An item is
// masked if:
//   1. It is "pure" i.e. the item is present in all or in none of the examples.
//   2. The item has been sampled-out (see
//   "categorical_set_split_greedy_sampling" in "dt_config").
//   3. The item is pruned by the maximum number of items (see
//   "categorical_set_split_max_num_items" in "dt_config").
// Return true iif at least one item is non masked.
bool MaskPureSampledOrPrunedItemsForCategoricalSetGreedySelection(

// Create the histogram bins (i.e. candidate threshold values) for an histogram
// based split finding on a numerical attribute.

// Sets in "positive_examples" and "negative_examples" the examples from
// "examples" that evaluate respectively positively and negatively to the
// condition "condition". The items in "examples" are expected to be sorted.
// When the function returns, "examples" might not be sorted anymore.
// "positive_examples" and "negative_examples" will be pointing to subsets of
// "examples".
//
// If "examples_are_training_examples=true", optimizes the allocation by
// assuming "examples" are the examples used to train the tree.
    bool error_on_wrong_splitter_statistics,
    bool examples_are_training_examples = true);




// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/training_test.cc =====







// Margin of error for numerical tests.

// Returns a simple datasets with gradients in the second column.
  // TODO Replace PARSE_TEST_PROTO by a modern function when
  // possible.


  // Those values are similar to the section 3.2 of the "Linear Discriminant
  // Analysis: A Detailed Tutorial" by Tharwat et al.


struct TrainTreeParam {


  // Dataset

  // We want:
  // "f1">=2.5 [s:0.347222 n:6 np:4 miss:0] ; pred:0.833333
  //     â”œâ”€(pos)â”€ "f2">=1.5 [s:0.0625 n:4 np:2 miss:0] ; pred:1.25
  //     |        â”œâ”€(pos)â”€ pred:1.5
  //     |        â””â”€(neg)â”€ pred:1
  //     â””â”€(neg)â”€ pred:0

  // Training configuration


  // Train a tree

  // Note: preprocessing is required for sorting_strategy=PRESORTED. For
  // sorting_strategy=IN_NODE, preprocessing is ignored.




  // Dataset




  // Training configuration


  // Train a tree

  // Note: preprocessing is required for sorting_strategy=PRESORTED. For
  // sorting_strategy=IN_NODE, preprocessing is ignored.







  // // Distribution of the gradients:



  // // Distribution of the gradients:





      auto label_col,








      auto label_col,








      auto label_col,




























  // Those values are similar to the section 3.2 of the "Linear Discriminant
  // Analysis: A Detailed Tutorial" by Tharwat et al.





















  auto examples_rb =






// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/uplift.h =====


// Classes used by the splitter to accumulate statistics for the uplift tasks.





// Distribution of uplift classification labels.
struct UpliftLabelDistribution {

  void InitializeAndClearLike(const UpliftLabelDistribution& guide) {

  void InitializeAndClearCategoricalOutcome(
    // The value "0" is reserved.


  void InitializeAndClearNumericalOutcome(
    // The value "0" is reserved.


  void AddCategoricalOutcome(int outcome_value, int treatment_value,

  void SubCategoricalOutcome(int outcome_value, int treatment_value,

  void InternalAddCategoricalOutcome(int outcome_value, int treatment_value,
    // Only support binary treatment and binary outcome.



    // The zero outcome is ignored.

  void AddNumericalOutcome(float outcome_value, int treatment_value,

  void SubNumericalOutcome(float outcome_value, int treatment_value,

  void InternalAddNumericalOutcome(float outcome_value, int treatment_value,
    // Only support binary treatment and binary outcome.



  void Add(const UpliftLabelDistribution& src) {




  void Sub(const UpliftLabelDistribution& src) {






  // Tests if the uplift can be computed.
  bool HasUplift() const {

  // Uplift value for bucket ordering.
    // Only support binary treatment and single dimension outcome.

    // Replacement value for missing output treatment.


  // Uplift value for leafs. All the treatments should have at least one value.
      // The minimum number of examples per treatment should make this case
      // not possible.

    // Only support binary treatment and single dimension outcome.


  // Returns the lower bound of the 9.7% confidence interval of the uplift.
  // Model the outcome as a normal distribution.

    // Only support binary treatment and single dimension outcome.

    // z-value for a ~9.7% confidence bound. This value was selected to give
    // reasonable results on the train/test SimPTE dataset.


    // Return the most conservative uplift value (i.e. the value closest to
    // zero; i.e. with the smaller absolute value) in [lb, ub]. For example, if
    // l=-0.1 and ub=0.3, return return 0.

  // Uplift for score splits.



          // The returned divergence should be infinite (or very high). However,
          // this would essentially discard all the possible splits. Returning
          // 0, would enable the search for splits, but would not be great to
          // break ties. Instead, we return a correlated with the split quality
          // that would be smaller than any real divergence values.
          // The returned divergence should be infinite (or very high). However,
          // this would essentially discard all the possible splits. Returning
          // 0, would enable the search for splits, but would not be great to
          // break ties. Instead, we return a correlated with the split quality
          // that would be smaller than any real divergence values.

  int MinNumExamplesPerTreatment() const {


  void ImportSetFromLeafProto(const proto::NodeUpliftOutput& leaf) {



  void ExportToLeafProto(proto::NodeUpliftOutput* leaf) const {


  // The fields have the same definition as the fields in
  // "proto::NodeUpliftOutput". See this proto documentation for the explanation
  // about the +1/-1 in this class.



struct LabelUpliftCategoricalScoreAccumulator {





template <bool categorical_label>
struct LabelUpliftGenericOneValueBucket {



  int treatment;
  float weight;


  void AddToScoreAcc(Accumulator* acc) const {

  void SubToScoreAcc(Accumulator* acc) const {

  class Initializer {

    void InitEmpty(Accumulator* acc) const {

    void InitFull(Accumulator* acc) const {


    bool IsValidSplit(const Accumulator& neg, const Accumulator& pos) const {



  class Filler {

    void InitializeAndZero(Bucket* acc) const {
      // Nothing to do.

    void Finalize(Bucket* acc) const {
      // Nothing to do.

    void ConsumeExample(const UnsignedExampleIdx example_idx,







template <bool categorical_label>
struct LabelUpliftGenericBucket {



  float signed_uplift;

  void AddToScoreAcc(Accumulator* acc) const { acc->label.Add(distribution); }

  void SubToScoreAcc(Accumulator* acc) const { acc->label.Sub(distribution); }

  bool operator<(const Bucket& other) const {

  class Initializer {

    void InitEmpty(Accumulator* acc) const {

    void InitFull(Accumulator* acc) const {


    bool IsValidSplit(const Accumulator& neg, const Accumulator& pos) const {




  class Filler {

    void InitializeAndZero(Bucket* acc) const {

    void Finalize(Bucket* acc) const {

    void ConsumeExample(const UnsignedExampleIdx example_idx,










// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/utils.cc =====






void SetPositiveAttributeSetOfCategoricalContainsCondition(
  // Estimate the memory usage of both representation.
  // What is the most compact way to encode the set of positive attribute
  // values?
    // The elements are expected to be sorted i.e. sorted by increasing item
    // index value.
    // Note: There are not guarantied correlation between the order of the
    // item indices and the ratio of positive label.

void SetPositiveAttributeSetOfCategoricalContainsCondition(


// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/utils.h =====







// The outcome of a split search.
  // A new split, better than the current best split, was found.
  // At least a split was found. None of the found splits are better than the
  // current best split.
  // No valid split found.

// Sets the condition to be "contains condition" i.e.
// "is_value_attribute_i in X".
//
// A "Contains" condition tests the presence (or absence) of an example
// categorical attribute value in a set of values. This set, called "positive
// attribute set",  is stored as a sorted array of "positive" values or as a
// bitmap (which ever is the most memory efficient).
//
// Given a "positive attribute set" sorted as a vector, this function set the
// "positive attribute set" of the condition according the most efficient
// solution.
//
// "ratio_true_label_by_attr_value" is expected to be sorted in
// increasing value of positive label ratio i.e. "{x.first for x \in
// ratio_true_label_by_attr_value}" is increasing.
//
// More precisely, it sets the positive attribute set to be
// ratio_true_label_by_attr_value[i].second for i in [
// begin_positive_idx, ratio_true_label_by_attr_value.size()-1].
//
// Note: This function only sets the "positive attribute set" i.e. it does not
// set any of the other fields of a categorical condition (e.g. split_score).
//
void SetPositiveAttributeSetOfCategoricalContainsCondition(

// Sets the condition to be "contains_condition" or "contains_bitmap_condition"
// with the positive equals to "positive_attribute_value". "contains_condition"
// or "contains_bitmap_condition" is selected to minimize the size in memory of
// the condition.
void SetPositiveAttributeSetOfCategoricalContainsCondition(

// Removes "l1" from the length of "value". Set to 0 is the "length/abs" of
// "value" is lower than "l1".
template <typename T1, typename T2>

// Returns r := (a+b)/2 for a>b.
// Ensure that:
// - r is finite.
// - r > a.
// - r = b if there is not float number in between a and b.
//
  float threshold = a + (b - a) / 2.f;

// Constraints applied to a node during training.
struct NodeConstraints {
  // Minimum and maximum value of the node output.
  // Note: Min or max can be -/+ infinity.
  struct MinMax {
    float min = -std::numeric_limits<float>::infinity();
    float max = std::numeric_limits<float>::infinity();




// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/vector_sequence.cc =====







// Evaluates, and if better than the current one, records a condition of the
// type "dist(example, anchor) <= threshold").
template <typename LabelType, typename Labels>
absl::Status TryCloserThanCondition(


    auto anchor = anchors.subspan(anchor_idx * attribute.vector_length(),

    auto local_projection = absl::MakeSpan(*projections)




// Evaluates, and if better than the current one, records a condition of the
// type "dot(example, anchor) >= threshold").
template <typename LabelType, typename Labels>
absl::Status TryProjectedMoreThanCondition(


    auto anchor = anchors.subspan(anchor_idx * attribute.vector_length(),
    auto local_projection = absl::MakeSpan(*projections)





template <typename LabelType>



  // TODO: Cache buffers.

    // Sample a subset of the selected training examples.
    // All the examples are used for the anchor search.


  // TODO: Cache buffers.




        // Example with missing vector sequence.
        // Example without vector.


  // Static anchors generated from training examples.



      // The anchor is the difference between two random values in the dataset.

      // The anchor is a random value in the dataset.
      // TODO: Better copy.

      // TODO: Cache buffers.
      // TODO: Global iota + use spans







  // Dispatch.


// ===== FILE: yggdrasil_decision_forests/learner/decision_tree/vector_sequence.h =====






// Find the best possible split on a numerical vector sequence attribute for a
// classification label.
//
// Uses dynamic dispatching on the label type.

// Find the best possible split on a numerical vector sequence attribute for a
// classification label.
//
// Uses template dispatching on the label type.
template <typename LabelType>



// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/label_accessor.h =====


// Give access to label values.
//
// A "label filler" is a class that adds/subtracts label values from label
// statistics accumulator (called "accumulator" for short) and label statistics
// buckets (called "label bucket" for short).
//
// An accumulator and a label bucket are similar objects from the point of view
// of the label filler. In practice, a dataset is always split into two
// accumulators while there can be one bucket per examples (therefore, they can
// have different internal structures / numerical precision). Additionally,
// examples can be removed from accumulator, but not from buckets.
//
// A label filler should implement the following methods:
//   InitializeAndZeroAccumulator(accumulator): Initialize an (label statistics)
//     accumulator to the empty state.
//   InitializeAndZeroBucket: Initialize a label bucket to the empty state.
//   Add(accumulator)/Sub(accumulator): Add/subtract an example to an
//     accumulator.
//   Add(label_bucket): Add an example to an accumulator.
//




// How to represent an example index.

// Categorical label filler. Alternative to LabelCategoricalBucket::Filler.
class ClassificationLabelFiller {
  // How to represent a label value.
  // TODO: Add special handling for unit weights.



  void InitializeAndZeroAccumulator(Accumulator* accumulator) const {

  void InitializeAndZeroBucket(LabelBucket* bucket) const {

  void Prefetch(const ExampleIndex example_idx) const {

  void Add(const ExampleIndex example_idx, Accumulator* accumulator) const {

  void Sub(const ExampleIndex example_idx, Accumulator* accumulator) const {

  void Add(const ExampleIndex example_idx, LabelBucket* bucket) const {


// Regression label filler. Alternative to LabelNumericalBucket::Filler.
class RegressionLabelFiller {
  // How to represent a label value.
  // TODO: Add special handling for unit weights.



  void InitializeAndZeroAccumulator(Accumulator* accumulator) const {

  void InitializeAndZeroBucket(LabelBucket* bucket) const {

  void Prefetch(const ExampleIndex example_idx) const {

  void Add(const ExampleIndex example_idx, Accumulator* accumulator) const {

  void Sub(const ExampleIndex example_idx, Accumulator* accumulator) const {

  void Add(const ExampleIndex example_idx, LabelBucket* bucket) const {


// Regression label filler. Alternative to
// LabelNumericalWithHessianBucket::Filler.
class RegressionWithHessianLabelFiller {
  // How to represent a label value.
  // TODO: Add special handling for unit weights.



  void InitializeAndZeroAccumulator(Accumulator* accumulator) const {

  void InitializeAndZeroBucket(LabelBucket* bucket) const {

  void Prefetch(const ExampleIndex example_idx) const {

  void Add(const ExampleIndex example_idx, Accumulator* accumulator) const {

  void Sub(const ExampleIndex example_idx, Accumulator* accumulator) const {

  void Add(const ExampleIndex example_idx, LabelBucket* bucket) const {


// Gives access to label values.
class AbstractLabelAccessor {

  // Classification.

  // Regression.

  // Regression with hessian information

class ClassificationLabelAccessor : public AbstractLabelAccessor {




class RegressionLabelAccessor : public AbstractLabelAccessor {




class RegressionWithHessianLabelAccessor : public AbstractLabelAccessor {






// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/splitter.h =====


// Utility methods to find the best splits (i.e. condition).
// See "training.h" header for the definition of the various concepts.
//



// Index of a feature column.

// Index of an open node within a single tree.
//
// (Currently) the code is optimized for GBT learner. There cannot be more than
// MaxValue(NodeIndex)-1 open nodes. In other words, no more than
// 2xMaxValue(NodeIndex) nodes. GBT generally have ~64-1024 nodes.

// Mapping between an example index and a node index.

// Special node index indicating that a node is closed.

// Definition and statistics of a split.
struct Split {
  // Label statistics for the negative (index 0) and positive (index 1)
  // branches.

// A splits+statistics for each of the currently open nodes.

// Label statistics for each of the currently open nodes.

// Common arguments for the "FindBestSplit" methods.
struct FindBestSplitsCommonArgs {


// Accumulators and other data for the learning of numerical split.
template <typename LabelFiller>
struct NumericalSplitAccumulator {
  // Distribution of the label on the two sides of the split.


  // Number of examples on the two sides of the split.

  // Number of delta-bit met so far i.e. number of unique feature value.

  // The "best_*" attributes below are related to the best split found so far.

  // Indices of the previous and next delta values that form the threshold.
  // If "best_next_delta_value_idx==0", no threshold is defined.



template <typename LabelFiller>
absl::Status InitializerNumericalAccumulator(


    // All the examples are in the "pos" side of the split.




template <typename LabelFiller, typename ExampleBucketSet>
absl::Status InitializeCategoricalFeatureBuckets(

template <typename LabelFiller>
absl::Status FillNumericalAccumulator(
  // Scan over the dataset in increasing order of feature value.
      auto example_it,



  // Number of times the delta bit was observed.



      // Check that the current and previous examples are separable.

      // TODO: Maybe. Compile time condition.
        // Retrieve the node containing the example.

        // Check that the node is not closed.

        // Check that the node is a target of this feature.


      // New feature values?
        // Update the delta bit.

        // Enough examples?
            // We found a better split.

      // Update the accumulators.

  // Check that the counts are as expected.


template <typename LabelFiller, typename ExampleBucketSet>
absl::Status FillCategoricalFeatureBuckets(
      auto value_it,





      // TODO: Maybe. Compile time condition.
        // Retrieve the node containing the example.

        // Check that the node is not closed and a target feature.



      // Update the bucket.


template <typename LabelFiller, typename ExampleBucketSet>
absl::Status FillDiscretizedNumericalAccumulatorPartial(
      auto value_it,




      // TODO: Maybe. Compile time condition.
        // Retrieve the node containing the example.

        // Check that the node is not closed and a target feature.



      // Update the bucket.


template <typename LabelFiller, typename ExampleBucketSet>
absl::Status FillDiscretizedNumericalAccumulator(
      auto value_it,




      // TODO: Maybe. Compile time condition.
        // Retrieve the node containing the example.

        // Check that the node is not closed and a target feature.



      // Update the bucket.


template <typename LabelFiller, typename ExampleBucketSet>
absl::Status FillBooleanFeatureBuckets(




        // Retrieve the node containing the example.

        // Check that the node is not closed and a target feature.



      // Update the bucket.


// Set the label statistics for the two children of a categorical split.
template <typename LabelFiller, typename ExampleBucketSet>
absl::Status ComputeSplitLabelStatisticsFromCategoricalSplit(
  // Compute the label statistics in the child nodes.

  // The positive split contains the examples containing the selected
  // items.

  // The negative split is complementary to the positive split.



// Set the label statistics for the two children of a discretized numerical
// split.
template <typename LabelFiller, typename ExampleBucketSet>
absl::Status ComputeSplitLabelStatisticsFromDiscretizedNumericalSplit(

  // The positive split contains all the bucket greater or equal to the
  // threshold.

  // The negative split is complementary to the positive split.



template <typename LabelFiller, typename ExampleBucketSet>
absl::Status ComputeSplitLabelStatisticsFromBooleanSplit(

  // The negative split is complementary to the positive split.



template <typename LabelFiller>
absl::Status FindSortedNumericalThreshold(
  // A delta value of interest to determine the threshold of a split.
  struct TargetDeltaValue {
    bool is_next;
    int node_idx;

  // Sorted (increasing order) mapping between the num_delta_bits and the node
  // index where this delta bit is the best split.

  // Assemble the final condition (without the thresholds).


    // The threshold value is set in the next loop.


    // Compute the label statistics in the child nodes.




  // Assemble the splitting thresholds.

        auto deta_value_it,

    auto delta_values = deta_value_it->Values();

    // Pre-split delta values indexed by node index.

      // TODO: Implement a "skip" method in the reader.







    // Make sure the threshold value is set.


template <typename LabelFiller, typename ExampleBucketSet>
absl::Status FindDiscretizedNumericalThreshold(
  // Test each of the nodes.



      // Transform the condition to a non discretized one.



template <typename LabelFiller, typename ExampleBucketSet>
absl::Status OneVsOtherClassificationAndCategoricalFeatureBuckets(
  // Custom ordering of the features.

  // Re-order the bucket and test all the splits defined by continuous buckets.


        // Never observed label value.
        // "True vs others" or "False vs others" are equivalent for binary
        // classification.

      // Order value of the buckets.

      // Sort the bucket indices.



template <typename LabelFiller, typename ExampleBucketSet>
absl::Status InOrderRegressionAndCategoricalFeatureBuckets(
  // Custom ordering of the features.

  // Test each of the nodes.


    // Order value of the buckets.

    // Sort the bucket indices.



template <typename LabelFiller, typename ExampleBucketSet>
absl::Status InOrderRegressionAndBooleanFeatureBuckets(
  // Test each of the nodes.




// Splitter for a numerical feature. Support all types of labels.
template <typename LabelFiller>
absl::Status TemplatedFindBestSplitsWithSortedNumericalFeature(
  // Initialize the score accumulators for the target nodes.

  // Scan the dataset to find good splits.

  // Determine the threshold value of the splits.


// Splitter for a numerical feature. Support all types of labels.
template <typename LabelFiller>
absl::Status TemplatedFindBestSplitsWithDiscretizedNumericalFeature(
  // Contains the condition conditional label statistics.




  // ExampleBucket for each of the open nodes.


  // Scan the dataset to find good splits.

  // Determine the threshold value of the splits.


// Same as "TemplatedFindBestSplitsWithDiscretizedNumericalFeature", but with
// multi-threading distribution.
template <typename LabelFiller>


  // Contains the condition conditional label statistics.




  // ExampleBucket for each of the threads and each of the open nodes.

  absl::Status status;





        // Scan the dataset to find good splits.





  // Determine the threshold value of the splits.


// Splitter for a categorical features and a classification label.
template <typename LabelFiller>
absl::Status TemplatedFindBestSplitsWithClassificationAndCategoricalFeature(


  // Contains the condition conditional label statistics.


  // ExampleBucket for each of the open nodes.

  // Initialize the buckets.

  // Aggregate the label values per bucket and open node.


// Splitter for a boolean features and a classification label.
template <typename LabelFiller>
absl::Status TemplatedFindBestSplitsWithClassificationAndBooleanFeature(

  // Contains the condition conditional label statistics.


  // ExampleBucket for each of the open nodes.

  // Initialize the buckets.

  // Aggregate the label values per bucket and open node.


// Splitter for a categorical features and a regression label.
template <typename LabelFiller>
absl::Status TemplatedFindBestSplitsWithRegressionAndCategoricalFeature(


  // Contains the condition conditional label statistics.



  // ExampleBucket for each of the open nodes.

  // Initialize the buckets.

  // Aggregate the label values per bucket and open node.


// Splitter for a boolean features and a regression label.
template <typename LabelFiller>
absl::Status TemplatedFindBestSplitsWithRegressionAndBooleanFeature(

  // Contains the condition conditional label statistics.


  // ExampleBucket for each of the open nodes.

  // Initialize the buckets.

  // Aggregate the label values per bucket and open node.




// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/training.cc =====






// Creates a vector of accumulator initializer. One for each open node.
template <typename AccumulatorInitializer>

// Creates a bitmap indicating that a node is a "target node" i.e. that the
// "feature" is a candidate feature for this node.

// Collections the label statistics of all the training examples.
template <typename Filler>
absl::Status TemplatedAggregateLabelStatistics(








  auto in_construction_tree = absl::WrapUnique<TreeBuilder>(new TreeBuilder(


absl::Status TreeBuilder::AggregateLabelStatistics(

absl::Status AggregateLabelStatistics(
              auto filler,

absl::Status TreeBuilder::FindBestSplits(

  // Allocates the splits.

  // List all the features tested by at least one node.

  // Find the best split per node and per feature.

absl::Status TreeBuilder::FindBestSplits(
  // List all the features tested by at least one node.

  absl::Status status;

  // Look for the splits.



absl::Status TreeBuilder::FindBestSplitsWithThreadPool(


  // Allocates the splits.


  // Find the best split per node and per feature.
      // Did another worker already failed?

      // Find the split.

      // Copy of common arguments to have an independent
      // space for the output argument "best_splits".
      auto local_common = common;


      // Merge the result.
        // Note: Making sure the mutex lock is destroyed before the mutex.


absl::Status TreeBuilder::FindBestSplitsWithFeature(
    int num_theads) const {
  // Compute the set of target nodes.







absl::Status TreeBuilder::FindBestSplitsWithFeatureSortedNumerical(





absl::Status TreeBuilder::FindBestSplitsWithFeatureDiscretizedNumerical(





absl::Status TreeBuilder::FindBestSplitsWithFeatureCategorical(
  // TODO: Implement the random splitter.






absl::Status TreeBuilder::FindBestSplitsWithFeatureBoolean(





absl::Status MergeBestSplits(


    // Break ties using the "attribute_priority".
    bool copy_condition = false;
        // The split with the greater attribute is selected


int NumValidSplits(const SplitPerOpenNode& splits) {
  int num_splits = 0;

bool IsSplitValid(const Split& split) {

absl::Status SetLeafValue(
  // This function creates the same label values as "SetLabelDistribution" in
  // "learner/decision_tree/training.cc". However, it uses a pre-computed label
  // statistic instead of a vertical dataset.






      // Non-leaf node.


      // Note: For historical reason, the
      // "num_pos_training_examples_without_weight" field in a node is
      // equivalent to the "num_training_examples_without_weight" field in a
      // condition.


      // Turning the node into a leaf.


absl::Status TreeBuilder::SetRootValue(

absl::Status EvaluateSplits(const ExampleToNodeMap& example_to_node,
  // Group the split per feature.







  absl::Status status;
      auto local_status = process(feature_idx, splits);

absl::Status EvaluateSplitsPerNumericalFeature(
  // Initializer the active nodes.
  struct ActiveNode {
    float threshold;
    // Total number of expected elements.

    // Only used to check the validity of the code.
    // Number of elements written so far.

    float threshold;


  // Scan the dataset and evaluate the split.






  // Finalize the writers.


absl::Status EvaluateSplitsPerDiscretizedNumericalFeature(
  // Initialize the active nodes.
  struct ActiveNode {
    float threshold;
    // Total number of expected elements.

    // Only use to check the validity of the code.
    // Number of elements written so far.



    float threshold;


  // Scan the dataset and evaluate the split.


      auto value_it,

      float value;



  // Finalize the writers.


absl::Status EvaluateSplitsPerCategoricalFeature(
  // Initializer the active nodes.
  struct ActiveNode {
    // Total number of expected elements.




  // Scan the dataset and evaluate the split.



  // Finalize the writers.


absl::Status EvaluateSplitsPerBooleanFeature(
  // Initializer the active nodes.
  struct ActiveNode {
    // Total number of expected elements.


        // Nothing to do.

  // Scan the dataset and evaluate the split.



  // Finalize the writers.


absl::Status UpdateExampleNodeMap(


  // TODO: In parallel.
      // The example is not is a closed node.

      // The example is in a node that is closed during this iteration.



absl::Status UpdateLabelStatistics(


        // Note: Due to floating point approximations, it is rare (in most
        // training this does not happens) but possible for label statistic
        // counts to be wrong. In such case, those counts need to be corrected.
        // The same observations and logic is used in non-distributed training.

void ConvertFromProto(const proto::SplitPerOpenNode& src,

void ConvertToProto(const SplitPerOpenNode& src, proto::SplitPerOpenNode* dst) {

void ConvertToProto(const SplitPerOpenNode& src,


// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/training.h =====


// Utility for the distributed training (compute and memory) of individual
// decision trees.
//
// Glossary:
//
// An "open node" is a leaf that is candidate to be split i.e. the node can
// be transformed in a non-leaf node. In layer-wise learning, the open nodes are
// all the nodes in the currently trained layer that satisfy the nodes splitting
// constraints (e.g. minimum number of examples).
//
// A "column index" is a dense index identifying a column. A column can be a
// label, input feature, weight, ranking group, ignored column, etc.
//
// A "feature index" is a "column index" where we know that the column is an
// input features.
//
// A "split evaluation" is an array of bits (one bit for each example in an open
// node) that describes the result of evaluating a split/condition on those
// examples.
//




// How to re-map the node index after a split.
//
// NodeRemapping a;
// a[i].indices[0(neg)] => The index of negative node created after the split of
// the i-th open node.
struct SplitNodeIndex {
  // Negative (0) and positive (1) node indices.
struct NodeRemapping {
  // mapping[i].indices[j] is the new node index for examples initially in node
  // i and that evaluate to j.
  // Number of destination nodes (excluding the kClosedNode).
  int num_dst_nodes;

// Bitmap of the evaluation of a split i.e. evaluation of the boolean condition
// defined by the split.
//
// SplitEvaluationPerOpenNode a;
// ReadBitmap(/*bitmap=*/a[i], /*bit_index=*/j) is the boolean evaluation of a
// split on the j-th example contained in the i-th open node.

// Selection of a label accessor.

// Default logic to set the value of a leaf.
absl::Status SetLeafValue(

// Signature of a function that sets the value (i.e. the prediction) of a leaf
// from the gradient label statistics.

// A decision tree being build.
//
// Usage example:
//
//   // Create the builder.
//   auto builder = TreeBuilder::Create();
//
//   // Compute the statistics of the labels.
//   builder->AggregateLabelStatistics(&label_satistics);
//
//   // Create the root node.
//   builder->SetRootValue(label_satistics);
//
//   // Initially, all the examples are in the root (which is the only node).
//   auto example_to_node = CreateExampleToNodeMap(num_examples);
//   LabelStatsPerNode label_stats_per_node({label_satistics});
//
//   for(int i=0; i<=5; i++) { // Train until depth 5
//
//     // Look for splits.
//     SplitPerOpenNode splits;
//     builder->FindBestSplits(example_to_node, label_stats_per_node, &splits);
//
//     // Update the tree structure with the splits.
//     auto node_remapping = tree_builder->ApplySplitToTree(splits);
//
//     // Evaluate the split on all the examples.
//     SplitEvaluationPerOpenNode split_evaluation;
//     EvaluateSplits(example_to_node, splits, &split_evaluation);
//
//     // Update the example->node mapping.
//     UpdateExampleNodeMap(splits, split_evaluation, node_remapping,
//     &example_to_node);
//
//     // Update the statistics of the label in the open nodes.
//     UpdateLabelStatistics(splits, node_remapping, &label_stats_per_node);
//   }
//
// // Do something with the tree.
// builder->mutable_tree()
//
class TreeBuilder {
  // Creates a new builder.

  // Computes the label statistics over all the examples.
  absl::Status AggregateLabelStatistics(

  // Finds the best splits for the open nodes.
  absl::Status FindBestSplits(const FindBestSplitsCommonArgs& common) const;
  absl::Status FindBestSplits(

  // Finds the best splits for the open nodes. Unlike "FindBestSplits",
  // "FindBestSplitsWithThreadPool" schedules the split finding in the thread
  // pool and returns immediately. Multiple "FindBestSplitsWithThreadPool" can
  // run at the same time in the same thread pool (as long as the output
  // variables are different or protected by the same mutex).
  //
  // Usage example:
  //
  //  utils::concurrency::BlockingCounter counter(2);
  //  utils::concurrency::Mutex mutex_1, mutex_2;
  //   FindBestSplitsWithThreadPool({&splits_1}, ..., &mutex_1, &counter,...);
  //   FindBestSplitsWithThreadPool({&splits_2}, ..., &mutex_2, &counter,...);
  //   // The two "FindBestSplitsWithThreadPool" are running in parallel. In
  //   // addition, the individual features (in both
  //   // FindBestSplitsWithThreadPool calls are evaluated in parallel).
  //   counter.Wait();
  //   // The work of both FindBestSplitsWithThreadPool calls is available.
  //
  // Other example (using the same mutex and splits):
  //
  //  utils::concurrency::BlockingCounter counter(2);
  //  utils::concurrency::Mutex mutex;
  //   FindBestSplitsWithThreadPool({&splits}, ..., &mutex, &counter,...);
  //   FindBestSplitsWithThreadPool({&splits}, ..., &mutex, &counter,...);
  //   // The two "FindBestSplitsWithThreadPool" are running in parallel. In
  //   // addition, the individual features (in both
  //   // FindBestSplitsWithThreadPool calls are evaluated in parallel).
  //   counter.Wait();
  //   // The work of both FindBestSplitsWithThreadPool calls is available.
  //
  // Args:
  //   common: Input and outputs arguments. Same as "FindBestSplits".
  //   unique_active_features: Number of unique features to test among all the
  //     leaves i.e. the intersection of "common.features_per_open_node".
  //   thread_pool: Thread pool where to run the jobs. Will run one jobs per
  //     unique active feature.
  //   mutex: Protect "status" and "common.best_splits".
  //   counter: "counter.DecrementCount" is called each time a job is done i.e.
  //     once per unique active features.
  //   status: Aggregated status of the jobs.
  absl::Status FindBestSplitsWithThreadPool(

  // Applies a list of splits (one for each open node) to the tree structure.

  // Creates a remapping of example->node that will close all the open nodes.
  //
  // Note that this function has not impact on the tree being build.

  // Initializes the "value" of the tree root i.e. the predicted value is the
  // root was a leaf.
  absl::Status SetRootValue(






  // Specialization of "FindBestSplits" for a single feature.
  absl::Status FindBestSplitsWithFeature(const FindBestSplitsCommonArgs& common,
                                         int feature, int num_theads) const;

  // Specialization of "FindBestSplitsWithFeatureNumerical" for a single
  // sorted numerical feature.
  absl::Status FindBestSplitsWithFeatureSortedNumerical(

  // Specialization of "FindBestSplitsWithFeatureNumerical" for a single
  // discretized numerical feature.
  absl::Status FindBestSplitsWithFeatureDiscretizedNumerical(

  // Specialization of "FindBestSplitsWithFeatureNumerical" for a single
  // categorical feature.
  absl::Status FindBestSplitsWithFeatureCategorical(

  // Specialization of "FindBestSplitsWithFeatureNumerical" for a single
  // boolean feature.
  absl::Status FindBestSplitsWithFeatureBoolean(

  // Training configuration of the tree.

  // Tree in construction.

  // List of open nodes i.e. leaf that can later be divided.

  // How to build the label accessor.


// Computes the label statistics over all the examples.
absl::Status AggregateLabelStatistics(

// Initialize an example->node mapping where all the examples are in node 0.

// Merges two sets of splits element per element. "dst" will contain the split
// (from "src" or "dst") with the highest scores.
//
// "attribute_priority"  determines which condition is selected if multiple
// conditions have the same score. If "attribute_priority=={}", the condition
// with the smallest attribute index is selected. If "attribute_priority" is not
// empty, "attribute_priority[node_idx][attribute_idx]" specifies the priority
// of attribute **attribute_idx** for node **node_idx** (with smaller priority
// values being selected over higher priority values).
absl::Status MergeBestSplits(

// Evaluates a collection of splits.
absl::Status EvaluateSplits(const ExampleToNodeMap& example_to_node,

// Evaluates a collection of splits on a specific numerical feature.
absl::Status EvaluateSplitsPerNumericalFeature(

absl::Status EvaluateSplitsPerDiscretizedNumericalFeature(

// Evaluates a collection of splits on a specific categorical feature.
absl::Status EvaluateSplitsPerCategoricalFeature(

// Evaluates a collection of splits on a specific boolean feature.
absl::Status EvaluateSplitsPerBooleanFeature(

// Number of examples for each of the open nodes. Used by "UpdateExampleNodeMap"
// and "UpdateLabelStatistics".

// Updates the node index of each example according to the split. Also populate
// "num_examples_per_node" with the number of example in each nodes as computed
// during the split evaluation.
absl::Status UpdateExampleNodeMap(

// Updates the label statistics for each node. If the number of examples in the
// condition definition and in "num_examples_per_node" don't match, return an
// error (allow_statistics_correction=false) or print a warning
// (allow_statistics_correction=true).
absl::Status UpdateLabelStatistics(

// Gets the number of valid splits.
int NumValidSplits(const SplitPerOpenNode& splits);

// Tests if a split is valid.
bool IsSplitValid(const Split& split);

// Converts between the proto and struct representation.
void ConvertFromProto(const proto::SplitPerOpenNode& src,
void ConvertToProto(const SplitPerOpenNode& src, proto::SplitPerOpenNode* dst);

// Only converts the splits with index in "split_idxs". Other splits are set to
// be invalid.
void ConvertToProto(const SplitPerOpenNode& src,



// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/training_test.cc =====






// Generic training loop. Displays all the intermediate results.
template <typename LabelAccessor, typename Tester>
void GenericTrainingLoop(LabelAccessor* label_accessor, Tester* tester,

  auto example_to_node =

  // Initializer the decision tree.
  auto tree_builder = TreeBuilder::Create(tester->config_, tester->config_link_,

  // Computes the entire training dataset label statistics.


  // Set the root tree value using the global label statistics.



    // Collect the input features. All the nodes are seeing all the same
    // features.

    // Look for the best splits.


    // Merge the best split with empty splits. This is a No-op operation.


    // Add the found splits to the tree structure.


    // Evaluate the split for all the active training examples.


    // Update the example->node map.


    // Update the label statistics.

// Prepare the dataset for training: Create the dataspec, split the dataset into
// shards, and generate the dataset cache.
void PrepareDataset(const model::proto::TrainingConfig& train_config,

  // Infer the dataspec.


  // Shard the dataset.

  // Create the dataset cache.

  // Multi-threads distribution strategy.




class AdultClassificationDataset : public ::testing::Test {
  void SetUp() override {


    // Load the dataset cache.



  // Training configuration.


  auto example_to_node = CreateExampleToNodeMap(dataset_->num_examples());

  // Accessor to the label data.

  // Initializer the decision tree.
  auto tree_builder =

  // Computes the entire training dataset label statistics.



  auto example_to_node = CreateExampleToNodeMap(dataset_->num_examples());

  // Accessor to the label data.

  // Initializer the decision tree.
  auto tree_builder =

  // Computes the entire training dataset label statistics.



    // Best split on "workclass" feature.




    // Best split on "education_num" feature.




  // Best split on "age" feature.









  // Make the statistics wrong as to force the fix.




// Trains a tree with multiple-node layers.
  // Accessor to the label data.


// Trains a tree with multiple-node layers.
  // Accessor to the label data.


class AbaloneRegressionDataset : public ::testing::Test {
  void SetUp() override {


    // Load the dataset cache.



  // Training configuration.

// Trains a tree with multiple-node layers.
  // Accessor to the label data.


// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/dataset_cache/column_cache.cc =====






// Converts a buffer of integer values from one precision to the other.
// The destination buffer should be already allocated .
template <typename SrcValue, typename DstValue>
void ConvertIntegerBuffer(const char* const src_buffer, size_t num_values,

// Similar as "ConvertIntegerBuffer", but with the input format being an
// argument instead of a template value.
template <typename DstValue>
absl::Status ConvertIntegerBuffer(const char* const src_buffer,
  // Convert to the user requested precision.


absl::Status PrepareOutputFile(absl::string_view path) {

absl::Status FinalizeOutputFile(absl::string_view path) {


int NumBytes(uint64_t max_value) {

absl::Status IntegerColumnWriter::Open(absl::string_view path,

template <typename Value>
absl::Status IntegerColumnWriter::WriteValues(absl::Span<const Value> values) {

template absl::Status IntegerColumnWriter::WriteValues(
template absl::Status IntegerColumnWriter::WriteValues(
template absl::Status IntegerColumnWriter::WriteValues(
template absl::Status IntegerColumnWriter::WriteValues(
template absl::Status IntegerColumnWriter::WriteValues(

template <typename Value, typename DstValue>
absl::Status IntegerColumnWriter::WriteValuesWithCast(
  // TODO: Keep a buffer in between calls.

absl::Status IntegerColumnWriter::Close() {

template <typename Value>
absl::Status IntegerColumnReader<Value>::Open(absl::string_view path,
                                              int max_num_values) {


template <typename Value>

template <typename Value>

template <typename Value>
absl::Status IntegerColumnReader<Value>::Next() {
    // Convert to the user requested precision.



template <typename Value>
absl::Status IntegerColumnReader<Value>::Close() {

template class IntegerColumnReader<int8_t>;
template class IntegerColumnReader<int16_t>;
template class IntegerColumnReader<int32_t>;
template class IntegerColumnReader<int64_t>;
template class IntegerColumnReader<uint64_t>;

template <typename Value>
absl::Status ShardedIntegerColumnReader<Value>::ReadAndAppend(
    int end_shard_idx, std::vector<Value>* output) {


template <typename Value>
absl::Status ShardedIntegerColumnReader<Value>::Open(
    int begin_shard_idx, int end_shard_idx) {

template <typename Value>

template <typename Value>
absl::Status ShardedIntegerColumnReader<Value>::Next() {

template <typename Value>
absl::Status ShardedIntegerColumnReader<Value>::Close() {

template class ShardedIntegerColumnReader<int8_t>;
template class ShardedIntegerColumnReader<int16_t>;
template class ShardedIntegerColumnReader<int32_t>;
template class ShardedIntegerColumnReader<int64_t>;
template class ShardedIntegerColumnReader<uint64_t>;

template <typename Value>
absl::Status InMemoryIntegerColumnReaderFactory<Value>::Load(
    int begin_shard_idx, int end_shard_idx, size_t reserve) {


    auto file_buffer = file_reader.ActiveFileBuffer();
    // TODO: Estimate the final buffer size and pre-allocate it.



template <typename Value>
absl::Status InMemoryIntegerColumnReaderFactory<Value>::Borrow(

template <typename Value>

template <typename Value>

template <typename Value>

template <typename Value>

template <typename Value>

template <typename Value>



template <typename Value>
absl::Status InMemoryIntegerColumnReaderFactory<
  // Nothing to do.

template class InMemoryIntegerColumnReaderFactory<int8_t>;
template class InMemoryIntegerColumnReaderFactory<int32_t>;
template class InMemoryIntegerColumnReaderFactory<int64_t>;

absl::Status FloatColumnWriter::Open(absl::string_view path) {

absl::Status FloatColumnWriter::WriteValues(absl::Span<const float> values) {

absl::Status FloatColumnWriter::Close() {

absl::Status FloatColumnReader::Open(absl::string_view path,
                                     int max_num_values) {


absl::Status FloatColumnReader::Next() {

absl::Status FloatColumnReader::ReadAndAppend(absl::string_view path,

absl::Status FloatColumnReader::Close() { return file_->Close(); }

absl::Status ShardedFloatColumnReader::ReadAndAppend(


absl::Status ShardedFloatColumnReader::Open(absl::string_view base_path,
                                            int max_num_values,
                                            int begin_shard_idx,
                                            int end_shard_idx) {


absl::Status ShardedFloatColumnReader::Next() {

absl::Status ShardedFloatColumnReader::Close() { return sub_reader_.Close(); }




absl::Status InMemoryFloatColumnReaderFactory::InMemoryFloatColumnReader::

absl::Status InMemoryFloatColumnReaderFactory::Load(absl::string_view base_path,
                                                    int max_num_values,
                                                    int begin_shard_idx,
                                                    int end_shard_idx,


    // TODO: Estimate the final buffer size and pre-allocate it.



absl::Status InMemoryFloatColumnReaderFactory::Borrow(



// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/dataset_cache/column_cache_test.cc =====







// Create a sharded record with the pattern: value_i = value_idx * 2.
// Returns the base path.

// Create a sharded record with the pattern: value_i = value_idx * 2 + 0.5.
// Returns the base path.


template <typename WriteValue, typename ReadValue, uint64_t max_value,
          bool write_negatives = true>
void TestIntegerColumn() {
  // Create 11 values.


  auto write_span = absl::MakeConstSpan(write_values);
  auto read_span = absl::MakeConstSpan(read_values);

  // Write the 11 values.

  // Read the 11 values.






  // Special case to handle non signed integers.








  auto reader = reader_factory.CreateIterator();




  auto reader = reader_factory.CreateIterator();

  // Write the 5 values.

  // Read the 5 values.


  // Write the 5 values.

  // Read the 5 values.







  auto reader = reader_factory.CreateIterator();


// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/dataset_cache/dataset_cache.cc =====






// Number of request that each worker is expected to execute in parallel.
// This value impact the communication overhead (lower is better), the
// sensitivity to slow worker (higher is better), and the Ram usage of the
// workers (lower is better).
//
// TODO: Parametrize or set automatically according to the amount of
// available RAM / CPU on each worker.

// Number of shards in the cache dataset. Increasing this value make the
// creation of the dataset cache more robust to slow workers (good) but increase
// the number of files each worker has to open when creating and reading the
// dataset cache.

// List the typed shards and prefix from a typed sharded dataset path.
// TODO: Distribute or multi-thread the listing of shards for large
// datasets.
//
// For example:
//     ListShards("csv:/a/b@2", &shards, &type)
//     // type <= "csv"
//     // shards <= { "/a/b-00000-of-00002", "/a/b-00001-of-00002"}
//
absl::Status ListShards(const absl::string_view typed_path,

// Returns "column_idxs" if "column_idxs" is set. Else, returns all the column
// indices in the dataset.



absl::Status CreateDatasetCacheFromPartialDatasetCache(

  // Check if the cache is already there.

  // Create the directory structure.

  // Initialize the distribution manager.
      auto distribute_manager,
          // Each worker is expected to do up to QueryPerWorker tasks in
          // parallel.



  // Load the partial meta-data.

  // TODO: Index the categorical-string features.

  // Copy / transform the raw feature values.

  // Pre-sort the numerical columns.

  // Export the cache header.





absl::Status CreateDatasetCacheFromShardedFiles(

  // Check if the cache is already there.

  // Create the directory structure.

  // Initialize the distribution manager.
      auto distribute_manager,
          // Each worker is expected to do up to QueryPerWorker tasks in
          // parallel.

  // List the columns in the dataset.


  // List the shards in the input dataset.

  // Separate the columns of individual shards.

  // Pre-sort the numerical columns.

  // Export the cache header.




  // List the feature used to compute the statistics.
    // Use all the available features.



  // Converts the "CacheMetadata::Column::type" integer type to a string
  // representation.






absl::Status SeparateDatasetColumns(


  // Common part of the requests.

  // Each request will combine "shards_per_requests" input shards (from the
  // input dataset; all the column values are in the same file) into 1 output
  // shards (each column in a separate file).
  //
  // See definition of kNumShardPerWorkers for a high level explanation.



  int pending_requests = 0;
    // Check if the job was already executed.



    // Create the job.
    int begin_shard_idx = output_shard_idx * shards_per_requests;
    int end_shard_idx = std::min(static_cast<int>(dataset_shards.size()),

  // Receive and rename the results.

    // Save the meta-data information.





absl::Status ConvertPartialToFinalRawData(

  // Common part of the requests.

  int pending_requests = 0;



  // Receive and rename the results.



absl::Status SortNumericalColumns(

  // Common part of the requests.

  // We assume that a cache entry takes 4 bytes.


  int pending_requests = 0;

    // Check if the job was already executed.


    // Create the job.


  // Receive and rename the results.


    // Save the meta-data information.



absl::Status InitializeMetadata(
  // Label values, if any.

  // Column meta-data.








// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/dataset_cache/dataset_cache.h =====


// A dataset cache is a directory containing a dataset indexed / rearranged for
// distributed training of a decision forest model. It can be created from files
// with the "CreateDatasetCacheFromShardedFiles" method.
//
// The structure of one such directory is as follows:
//
//   metadata.pb: Global meta-data about the cache. CacheMetadata proto stored
//     in binary.
//   raw/column_{i}/shard_{j}-of-{k}: Features values ordered by example index:
//     Miss values are replaced with the corresponding
//     "replacement_missing_value" field in the proto meta data. Numerical
//     values are stored as floats. Categorical values (integer or string) are
//     stored as variable precision integer (optimized according to the maximum
//     number of possible values).
//   indexed/column_{i}/{example_idx_with_delta,discretized_values,
//     boundary_value}_{j}-of-{k}: Pre-sorted index of numerical features.
//   tmp: Temporary files for the creation of the cache. Empty after a cache
//     finished being created.
//
// Use the constant in "dataset_cache_common.h" for the exact names.
//
// A "partial dataset cache" is a format close but different from the (final)
// "dataset cache". Unlike the "dataset cache" that contains global meta-data
// (e.g. the mean value of a feature), indexes (e.g. example indices ordered
// according to specific feature index), and dictionary (e.g. the string->int
// mapping of a categorical-string feature), the "partial dataset cache" is a
// pure "dump" of data without aggregated global information. A "partial dataset
// cache" can be created in parallel by different workers without need for
// synchronization. A partial dataset cache can then be converted into a (final)
// dataset cache with the "CreateDatasetCacheFromPartialDatasetCache" method.
//
// The format is as follows:
//
//   partial_metadata.pb: The *only* global meta-data about the cache. Contains
//     the name of the features and the number of shards (generally each worker
//     will write in own shards). PartialDatasetMetadata proto stored in binary.
//   raw/column_{i}/shard_{j}-of-{k}: Features values ordered by example index:
//     Miss values can be present. Numerical values are stored as floats.
//     Categorical integer values are stored as int32. Categorical string values
//     are stored as int32 with a per-feature+per-shard dictionary stored in the
//     per-feature+per-shard meta-data proto.
//   raw/column_{i}/shard_{j}-of-{k}_metadata.pb: Meta-data of this blob of
//     data. PartialColumnShardMetadata proto stored in binary.
//




// Creates a dataset cache from a dataset.
//
// If distributed computation is used, the "CreateDatasetCacheFromShardedFiles"
// calling process does not ready any dataset file directly.
//
// If "effective_columns" is null, all the columns in the dataspec are used.
absl::Status CreateDatasetCacheFromShardedFiles(

// Create a dataset cache from a partially created dataset cache.
//
// A partial dataset cache only contains raw per-column and per-shard
// data (i.e. no indexed data, no meta-data about multiple columns or multiple
// shards).
//
// If "delete_source_file=True", the content of the partial dataset cache is
// deleted as it is being consumed. Note that this does not present to resume
// "CreateDatasetCacheFromPartialDatasetCache" if either the manager or the
// workers are interrupted.
//
absl::Status CreateDatasetCacheFromPartialDatasetCache(
    bool delete_source_file);


// Human readable report about the metadata. If "features" is specified,
// computes the statistics on the "features" features only. Otherwise, computes
// the statistics on all the features.


// Splits the dataset column by column and shards by shards.
//
// Set the following fields in the cache metadata: num_examples, num_shards.
absl::Status SeparateDatasetColumns(

// Converts the raw value in the partial dataset cache to the value in the final
// dataset cache. This is a simple copy of the features values with some
// transformation:
//   - Replace missing numerical values with a specific value.
//   - Re-encode the integer categorical features to optimal integer precision.
absl::Status ConvertPartialToFinalRawData(

// Sort the numerical columns.
absl::Status SortNumericalColumns(

// Initializes the meta-data content from the dataspec, column and configs.
// TODO: Make "InitializeMetadata" return a "CacheMetadata" directly.
absl::Status InitializeMetadata(



// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/dataset_cache/dataset_cache_common.cc =====








                              int num_shards) {

                              int shard_idx, int num_shards) {

                                     int column_idx, int shard_idx) {

                                   int column_idx) {

                                          int column_idx) {


int DeltaBitIdx(uint64_t num_examples) {







float DiscretizedNumericalToNumerical(


  // Get the sorted list of unique values.

  // Get the list of boundary values.

  // Gather the unique values and observation count.

  int current_count = 0;
  float current_value = std::numeric_limits<float>::quiet_NaN();


bool HasAllRequiredFiles(absl::string_view cache_path, const int num_columns,



    // Parse all the metadata.pb files.


// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/dataset_cache/dataset_cache_reader.cc =====







  auto report = MetaDataReport(meta_data(), features_);

  auto cache = absl::WrapUnique(new DatasetCacheReader(path, options));

  // List the features available to the reader.
      // Make all the features available.


    // Load the weight values.


        // Load the categorical label values.

        // Load the numerical label values.








absl::Status DatasetCacheReader::release_ranking_groups() {

absl::Status DatasetCacheReader::NonBlockingLoadingAndUnloadingFeatures(






bool DatasetCacheReader::IsNonBlockingLoadingInProgress() {



    // Still running.
    // Not running.

  // Wait for the loading thread.

  // Propagate the loading thread error (if any).


  // Update the meta-data. After this, the changes are visible to the user.

absl::Status DatasetCacheReader::LoadingAndUnloadingFeatures(




      absl::Status worker_status;




absl::Status DatasetCacheReader::ApplyLoadingAndUnloadingFeaturesToMetadata(

bool DatasetCacheReader::has_feature(const int feature) const {

absl::Status DatasetCacheReader::UnloadInMemoryCacheColumn(






absl::Status DatasetCacheReader::LoadInMemoryCacheColumn(int column_idx,




        // Raw numerical value.









absl::Status DatasetCacheReader::InitializeAndLoadInMemoryCache() {



  absl::Status worker_status;



int DatasetCacheReader::delta_bit_idx() const {






    int column_idx) const {



  auto reader = std::make_unique<ShardedIntegerColumnReader<ExampleIdxType>>();

    int column_idx) const {


  auto reader = std::make_unique<ShardedFloatColumnReader>();



  auto reader = std::make_unique<ShardedFloatColumnReader>();

    int column_idx) const {



  auto reader = std::make_unique<ShardedIntegerColumnReader<CategoricalType>>();




  auto reader = std::make_unique<ShardedIntegerColumnReader<HashType>>();




  auto reader = std::make_unique<ShardedIntegerColumnReader<BooleanType>>();

    int column_idx) const {




  auto reader = std::make_unique<

    int column_idx, size_t begin_idx, size_t end_idx) const {





    int column_idx) const {



absl::Status PartialDatasetCacheDataSpecCreator::InferColumnsAndTypes(



    // Load the column+shard meta-data.

    // Create the column name and type.


    // We only count the number of examples in the first columns.





        // Maximum value of "number_of_unique_values" seen in all the shards.
          auto it_dst = dst_categorical->mutable_items()->find(src_item.first);
            // A new item.
            // Increase the count of the known item.


absl::Status PartialDatasetCacheDataSpecCreator::ComputeColumnStatistics(


  absl::Status thread_status;






// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/dataset_cache/dataset_cache_reader.h =====


// Reader utility for a cache created with dataset_cache.h.
//




// Utility class to access a "dataset cache".
class DatasetCacheReader {
  // Creates the accessor utility class.
  //
  // Args:
  //   path: Path to the dataset cache i.e. the directory passed to the
  //     "cache_directory" argument of the CreateDatasetCacheFromShardedFiles
  //     dataset cache creation method.
  //   options: Configure how the cache is read. The default options are
  //     satisfying in most cases.
  //

    // TODO: Interrupt the non-blocking feature loading (if any).
    // CHECK_OK(WaitFeatureLoadingIsDone());

  // Number of examples in the cache.

  // Index of the deltabit in an example-idx.
  int delta_bit_idx() const;

  // Classification labels. Empty if there is not classification labels.

  // Regression or ranking labels. Empty otherwise.

  // Ranking labels and groups. Empty if there is no ranking label.

  // Trainings weights. Empty if the training examples are not weighted.

  // Iterator over the delta-bit example indices ordered according to the
  // "column_idx"-th numerical column.
  //
  // See the documentation of "MaskDeltaBit" and "MaskExampleIdx" for an
  // explanation of the "delta-bit example indices" concept.

  // Iterator over the sorted unique values of the "column_idx"-th numerical
  // column.

  // Iterator over the "column_idx"-th numerical column ordered by example
  // index.

  // Iterator over the "column_idx"-th hash column ordered by example
  // index.

  // Iterator over the "column_idx"-th categorical column ordered by example
  // index.

  // Iterator over the "column_idx"-th boolean column ordered by example index.

  // Iterator over the "column_idx"-th discretized numerical column ordered by
  // example index.

  // Iterator over a subset of the "column_idx"-th discretized numerical column
  // ordered by example index.

  // Discretization boundaries of the "column_idx"-th discretized numerical
  // column.
      int column_idx) const;


  // Compact human readable information about the metadata.

  // Features, sorted by index value, available in the reader.

  // Tests if a feature is available in the reader.
  bool has_feature(int feature) const;

  // Load and unload a set of features.
  absl::Status LoadingAndUnloadingFeatures(

  // Start loading and unloading a set of features asynchronously. Returns
  // immediately.
  //
  // Progresses can be checked regularly with
  // "CheckAndUpdateNonBlockingLoading()".
  //
  // Calling another method that changes the dataset reader (e.g.
  // "LoadingAndUnloadingFeatures") while features are being loaded will raise
  // an error.
  //
  // DatasetCacheReader's destructor will try to interrupt and wait for the
  // feature loading to be done. I.e. it is safe to destruct a
  // DatasetCacheReader that is loading features.
  //
  // Usage example:
  //
  //   DatasetCacheReader reader(features={0,1});
  //   reader.NonBlockingLoadingAndUnloadingFeatures(load_features={2,3},
  //      unload_features={1})
  //
  //   while(CheckAndUpdateNonBlockingLoading().value()) {
  //      // Use the features 0 and 1 in the reader.
  //   }
  //   // Can use the features 0, 2 and 3 in the reader.
  //
  absl::Status NonBlockingLoadingAndUnloadingFeatures(

  // Indicates if features are currently loaded with
  // "NonBlockingLoadingAndUnloadingFeatures". This value is updated by
  // "CheckAndUpdateNonBlockingLoading()".
  bool IsNonBlockingLoadingInProgress();

  // Checks for the completion of the non-blocking dataset loading.
  // If no loading is in progress, return false.
  // If there is a loading in progress, return true.
  // If the loading in progress failed, return the error.
  // If the loading in progress just completed, finalize it (i.e. the user can
  // access the feature values and "NonBlockingLoadingInProgress()" will now
  // return false), and return false.

  // Features being loaded. Empty if there are not features being pre-loaded at
  // this time.

  // Duration of the initial loading of features in memory i.e. duration of
  // "InitializeAndLoadInMemoryCache".

  // Releases the ranking group memory. Fails if the ranking group is not
  // available.
  absl::Status release_ranking_groups();



  // Initialize the internal structure and load the feature columns in RAM.
  absl::Status InitializeAndLoadInMemoryCache();

  // Loads a single column in RAM.
  absl::Status LoadInMemoryCacheColumn(int column_idx, size_t* memory_usage);

  // Unloads a single column from RAM.
  absl::Status UnloadInMemoryCacheColumn(int column_idx);

  // Updates the meta-data to make the specified features available. Note: This
  // method is not in charge of actually loading/unloading the features (i.e.
  // LoadInMemoryCacheColumn and UnloadInMemoryCacheColumn). Instead, the
  // feature loading/unload should already have been done.
  absl::Status ApplyLoadingAndUnloadingFeaturesToMetadata(


  // Example weights. Empty is the examples are not weighted.

  // Classification label values. Empty if the dataset does not have a
  // classification label.

  // Regression or ranking label values. Empty if the dataset does not have a
  // regression label.

  // Ranking label values. Empty if the dataset does not have a
  // ranking label.

  // List of the features available for reading. Sorted in increasing order.


  struct NonBlocking {
    // Loading thread.

    // Status of the loading thread. True set by the manager. False set by the
    // thread.

    absl::Status status;

    // Features being loaded / unloaded.

  struct {
    // Sorted numerical.

    // Discretized numerical

    // Categorical.

    // Boolean.

    // Hash



// Creates a dataspec for a partial dataset cache.
//
// See "dataset_cache.h" for an explanation of the dataset cache.
class PartialDatasetCacheDataSpecCreator : public AbstractDataSpecCreator {
  absl::Status InferColumnsAndTypes(

  absl::Status ComputeColumnStatistics(


  // Compute the statistics of a single column on a single shard.
      int col_idx,




// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/dataset_cache/dataset_cache_reader_test.cc =====







class End2End : public ::testing::Test {
  void Prepare(absl::string_view dataset_name, absl::string_view label_key,
    // Prepare the dataspec.

    // Shard the dataset.

    // Multi-threads distribution.


    // The "age" column will not be discretized (72 unique values), while the
    // "education_num" will be (15 unique values).







  auto reader = DatasetCacheReader::Create(cache_path_, options).value();

  // Sorted numerical
    int count = 0;
    auto col_reader =

  // In order numerical
    int count = 0;
    auto col_reader = reader->InOrderNumericalFeatureValueIterator(0).value();

  // In order categorical
    int count = 0;
    auto col_reader = reader->InOrderCategoricalFeatureValueIterator(1).value();

  // Discretized, in order, numerical

    int count = 0;
    auto col_reader =





  auto reader = DatasetCacheReader::Create(cache_path_, options).value();

  // The label and hash columns are not loaded in memory.



// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/dataset_cache/dataset_cache_test.cc =====






class End2End : public ::testing::Test {
  void SetUp() override {
    // Prepare the dataspec.

    // Shard the dataset.

    // Multi-threads distribution.


    // The "age" column will not be discretized (72 unique values), while the
    // "education_num" will be (15 unique values).



    // Try to generate the cache again. Will be instantaneous as the cache is
    // already there.



// Check an in order numerical column.



// Check an in order categorical column.


      // Note: Missing values are replaced in the cache creation.



// Check an sorted numerical categorical column.

  // List the delta values.

  float last_value = column_spec.numerical().min_value() - 1;
      // The value are stored in strict increasing order.

  // List the example indices.

  // Grab and sort the values of the feature.
  auto ground_truth_values =
  auto sorted_ground_truth_values = ground_truth_values;


  int num_delta_bit_idx = 0;



// Check an in order numerical column.



class TestCreateDatasetCacheFromPartialDatasetCache : public ::testing::Test {
  void SetUp() override {

    // Create the dataspec from the partial dataspec.

    // Convert the partial cache into a (final) cache.

    // Multi-threads distribution.



    // Try to generate the cache again. Will be instantaneous as the cache is
    // already there.


  void WriteFloatValues(absl::string_view path,

  void WriteInt32Values(absl::string_view path,

    // Create a partial cache.




  void CreatePartialCacheFeature0(absl::string_view partial_dataset_cache) {








  void CreatePartialCacheFeature1(absl::string_view partial_dataset_cache) {








  void CreatePartialCacheFeature2(absl::string_view partial_dataset_cache) {
















  auto reader = DatasetCacheReader::Create(cache_path_, options).value();



    auto iter =
    // Discretized version of: 1, 5, 2, 4, 3, 1, 2, 3, 2.625, 2.625

    auto iter = reader->InOrderCategoricalFeatureValueIterator(1).value();

    auto iter = reader->InOrderCategoricalFeatureValueIterator(2).value();


// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/dataset_cache/dataset_cache_worker.cc =====








// Number of threads for IO operations.
//
// TODO: Parametrize.

// Moves the files "filenames" from "src_dir" to "dst_dir". If anything goes
// wrong, print a warning and continue.
void MoveFilenamesNoFailures(const std::string_view src_dir,

absl::Status SeparateNumericalColumn(const int column_idx,


absl::Status SeparateCategoricalColumn(


absl::Status SeparateHashColumn(const int column_idx,

absl::Status SeparateBooleanColumn(const int column_idx,



absl::Status CreateDatasetCacheWorker::SeparateDatasetColumn(


  // Move the file from the tmp to its final destination.



absl::Status CreateDatasetCacheWorker::SeparateDatasetColumns(

  // TODO: Use a dataset reader directly with multi-threaded reading.





  absl::Status worker_status;
  int exported_columns = 0;





absl::Status CreateDatasetCacheWorker::SortNumericalColumn(

  // Read the values.
  // TODO: Read the shards in parallel.

  // Sort the values.

  // Export the sorted values.



  // Count the number of unique values.

  // Select how export the values (pre-sorted or discretized).




absl::Status CreateDatasetCacheWorker::ExportSortedNumericalColumn(


  // Number of remaining examples the current shard can receive. When a shard
  // is full, a new one is created. If remaining_examples_in_shard=0, a new
  // shard will be created on the next example.
  int next_output_shard_idx = 0;

  // Filename of the created files.


  // Output buffers;

  // TODO: Distribute writing.


        // Close the current shard.
      // Open a new shard.



  // Flush buffers.



  // Move the files to their final location.


absl::Status CreateDatasetCacheWorker::ExportSortedDiscretizedNumericalColumn(





  // Filename of the created files.

  // Export the boundary values.

  // Indexed the values.
  bool indexed_values_writer_is_open = false;



  int next_output_shard_idx = 0;


          // Close the current shard.
        // Open a new shard.









  // Move the files to their final location.

absl::Status ConvertPartialToFinalRawDataNumerical(







absl::Status ConvertPartialToFinalRawDataCategoricalInt(







absl::Status ConvertPartialToFinalRawDataCategoricalString(
  // Compute the re-mapping.
  // Value "i" is transformed into "mapping[i]". If i<0, replace the value with
  // "nan_value_replacement".


      // This item is unknown in the final dictionary. It is transformed to
      // Out-of-vocabulary.








absl::Status CreateDatasetCacheWorker::ConvertPartialToFinalRawData(

  // Get the various paths.

  // Meta-data specific for the shard+column.










absl::Status CreateDatasetCacheWorker::Setup(Blob serialized_welcome) {



// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/dataset_cache/dataset_cache_worker.h =====


// Remote worker for the computation of a dataset cache.




class CreateDatasetCacheWorker : public distribute::AbstractWorker {

  absl::Status Setup(distribute::Blob serialized_welcome) override;


  absl::Status Done() override { return absl::OkStatus(); }

  absl::Status SeparateDatasetColumns(

  absl::Status SeparateDatasetColumn(const dataset::VerticalDataset& dataset,
                                     int column_idx, int shard_idx,

  absl::Status ConvertPartialToFinalRawData(

  absl::Status SortNumericalColumn(

  // Export sorted numerical values. Used by "SortNumericalColumn".
  absl::Status ExportSortedNumericalColumn(

  // Export discretized numerical values. Used by "SortNumericalColumn".
  absl::Status ExportSortedDiscretizedNumericalColumn(






// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/load_balancer/load_balancer.cc =====






absl::Status LoadBalancer::Initialize(

  // Reset the attributes.


  // Score the features.

  // Initial feature assignment.
  int cur = 0;













    // TODO: Use circular buffer.


        // Iteration criterion.
        // Time criterion.


void LoadBalancer::AddFeatureLoadingDurationMeasurement(double time) {

bool LoadBalancer::HasPendingOrder() const { return !pending_orders_.empty(); }

    int worker) const {

absl::Status LoadBalancer::ApplyPendingOrder() {














absl::Status LoadBalancer::CreateRandomBalancingOrders() {


  // Randomly re-assign each feature to a worker.
  int num_unit_orders = 0;


absl::Status LoadBalancer::TryCreateBalancingOrders() {
  int num_unit_orders = 0;

  auto time_per_worker = CreateWorkTimeEstimatePerWorker();

    // Print debug infos.

  // List of features already used in one of the order.

    // From the fastest (good) to the slowest (bad) workers.

    // Print debug infos.


      // even
      // odd

    // Get the source and destination workers.



    // TODO: After the first one, all the other transfer costs are free.

    // Select an available feature i.e. a feature not already used.
    int feature = -1;







    int feature, const dataset_cache::proto::CacheMetadata& cache_metadata) {
  // TODO: Tune these costs.




int LoadBalancer::GetWorstCandidateWallTime(

int LoadBalancer::GetBestCandidateWallTime(


  // Group the features by ownership.


  int next_round_1_dst_worker = 0;
    // Index of the workers currently having the evaluation data.

    // The owner of the feature has always the evaluation data.

    // First round




    // Second round.
    int next_local_src_worker_idx = 0;
      // Check if this "dst_worker" worker already has the feature data.


  // Set the "last_request_of_plan" flag.
    // We make sure that all the workers are running the final logic at the same
    // time.



// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/load_balancer/load_balancer.h =====


// The LoadBalancer class helps balancing the workload among workers for
// distributed decision tree learning when workers have non-uniform and
// dynamically changing working speed -- commonly the case on shared
// cloud servers.
//
// It achieves that by dynamically controlling the mapping between features
// and workers.
//
// At each iteration, the user calls "AddWorkDurationMeasurement()" to provide
// the amount of time each worker took to execute the feature dependent tasks
// i.e. finding the best splits. Following multiple time measures were some
// workers are significantly slower than the others, the load balancer will
// propose a set of orders to transfer the ownership of features from the slow
// workers to the fast workers. Those orders are pending while the workers are
// loading from disk the data of the newly received features. Once all the
// workers have been prepared for the change (e.g. loading in the background the
// new features), the order can be applied
// ("ApplyPendingOrder()").
//
// Internally, the work balancer algorithm works as follows:
//
// 1. The computation speed of each worker is estimated over a rolling period of
//    time (the last `estimation_window_length` observations).
// 2. Workers that run slower than "median_margin_ratio x the medium worker
//    time" (e.g. median_margin_ratio=2) are considered "slow".
// 3. The computation time per feature of all the workers are estimated. The
//    balancer is proposing a balancing plan where the features of slow workers
//    are re-assigned to faster workers (while making sure not to create new
//    slow workers using the speed estimates).
// The plan is created with the following constraints:
//   - No worker can get assigned with more than "(1 + max_unbalance_ratio) x
//   number of initially assigned features".
//   - A plan cannot change more than
//   "max_balancing_changes_per_dynamic_balancing" feature allocations at the
//   same time.
// 4. Once a plan is created, workers start loading the new features in the
//    background.
// 5. Once all the workers are done loading the new features (which can take a
//    while), the new plan is put into effect (i.e. the manager will make
//    queries with the new feature assignation). The unused features on the slow
//    workers are also unloaded from memory.
//



class LoadBalancer {
  // Time measurement of a work done by a worker.
  struct Measure {
    int num_features = 0;  // Number of features processed in the work.
    bool operator<(const Measure& other) { return time < other.time; }

  // A unit order defines a change in the attribution of the features to the
  // workers e.g. transferring feature #1 ("feature") from worker #5
  // ("source_worker") to worker #7 ("destination_worker").
  struct FeatureAssignedWorkerChange {
    int source_worker;
    int destination_worker;
    int feature;

  // An order is composed of multiple UnitOrders. A given feature cannot be in
  // multiple unit orders of an order. A feature cannot be transferred from a
  // worker to itself.

  // A change as seen by a worker.
  struct ChangePerWorker {

  // Initialize the load balancer.

  // Gets the index of the workers currently owning a given feature.

  // Adds a new work time measurement. Possibly, triggers the dynamic
  // balancing of the features and propose some pending changes (return true).

  // Adds a new feature loading time measurement. At least one measure of
  // feature loading time should be provided before any call to
  // "AddWorkDurationMeasurement".
  void AddFeatureLoadingDurationMeasurement(double time);

  // Checks is a change is pending for execution.
  bool HasPendingOrder() const;

  // Applies the current pending changes. If a change in pending, no new
  // order/optimization will be proposed.
  absl::Status ApplyPendingOrder();

  // Pending change for a specific worker.

  // Human readable information about the balancer.

  // Mapping feature -> worker.

  // Mapping worker -> {list of features}. If possible use
  // "FeaturesPerWorker(worker)" instead.

  // Features owned by the worker. More efficient than
  // FeaturesPerWorkers()[worker].

  // Number of workers.
  int NumWorkers() const { return workers_.size(); }

  // Checks if the dynamic balancing is active i.e. the worker->feature
  // assignment can change.
  bool is_dynamic_balancing_active() const {

  // Creates a "plan" to share efficiently the evaluation split values of a set
  // of features.
  //
  // A "plan" is a partially ordered sequence of instructions of the form
  // "worker i request the evaluation of split j to worker k".
  //
  // Plans are currently made with two rounds such that each worker only talks
  // to sqrt(num_workers) at each round.
  //
  // For example, if a single worker needs to share a split value to 99 other
  // workers. It will first share it to 9 other workers. Then, in a second
  // round, the 10 workers than now have the evaluation split (the original
  // worker and the 9 new ones) will share it to the remaining 90 workers (9
  // output communication for each).

  // Data about a worker.
  struct Worker {
    // Features owned by this worker.

    // Time-series of measures. Each measure is a

    // Pending orders for this specific worker.

  // Data about a dataset feature.
  struct Feature {
    // Estimation of the "cost" of a feature. The absolute values does not
    // matter, only the order and the relative magnitude in between scores.

    // Currently, owning worker.
    int worker = -1;

    // True the feature is used for training and should be assigned to a
    // worker.
    bool active = false;


  // Initialize the load balancer.
  absl::Status Initialize(

  // Estimation of the working time of a worker (total and per features) of a
  // worker.
  struct WorkTimeEstimate {
    int num_features;
    int worker_idx;


    bool operator<(const WorkTimeEstimate& other) const {

  // Tries to balance the worker. After "TryCreateBalancingOrders" is called,
  // and if a new change was proposed, "HasPendingOrder()" will be true.
  absl::Status TryCreateBalancingOrders();

  // Creates a random balancing of the workers. The balancing does not take
  // any previous feature assignation or worker speed. This method is only
  // used for unit testing.
  absl::Status CreateRandomBalancingOrders();

  // Estimate the time is takes for a worker to load a new feature.

  // Relative cost of each feature.
      int feature, const dataset_cache::proto::CacheMetadata& cache_metadata);

  int GetWorstCandidateWallTime(

  int GetBestCandidateWallTime(

  // Balancer options.

  // Indices of the features that need assignment.

  // Maximum number of features to assign to a worker.
  int max_num_features_per_workers_;

  // Per worker data.

  // Per feature data.

  // Number of time measurements received so far.
  int num_measures_;

  // Sum of the leading time measurement received so far.

  // Number of loading time measurement received so far.
  int num_feature_loading_time_;

  // Random generator.

  // Number of feature reassignment proposed so far.
  int num_unit_orders_ = 0;

  // Number of re-balancing optimizations done so far.
  int num_rebalances_ = 0;

  // Number of orders emitted so far i.e. number of re-balancing that lead to
  // a change in feature assignation.
  int num_orders = 0;

  // Pending orders.

  // Value "num_measures_" the last time re-balancing was tried.
  int num_measures_last_balancing_ = 0;

  // Time  at that last  re-balancing was tried.



// ===== FILE: yggdrasil_decision_forests/learner/distributed_decision_tree/load_balancer/load_balancer_test.cc =====






  auto balancer = LoadBalancer::Create(/*features=*/{0, 1, 2, 3},



  auto balancer =


  // Worker #0 is faster than worker #1.


  // Transfer of feature expected.

  // Now, worker #1 is faster than worker #0.


  // Two transfers of feature expected.

  auto balancer =
  // Features 9 and 10 are owned by worker #0.
  // Feature 8 is owned by worker #1.




// ===== FILE: yggdrasil_decision_forests/learner/distributed_gradient_boosted_trees/common.cc =====







absl::Status EndIterTreeProtoWriter::Write(



                                               int evaluation_worker_idx,
                                               int num_evaluation_workers) {


// ===== FILE: yggdrasil_decision_forests/learner/distributed_gradient_boosted_trees/common.h =====






// Gets the path to the snapshot directory from the working directory.

// Gets the path to the validation predictions file in a checkpoint.
                                               int evaluation_worker_idx,
                                               int num_evaluation_workers);

// Encodes a tree into a EndIter::Tree proto.
class EndIterTreeProtoWriter

  absl::Status Write(const decision_tree::proto::Node& value) override;


// Decode a tree from an EndIter::Tree proto.
class EndIterTreeProtoReader





// ===== FILE: yggdrasil_decision_forests/learner/distributed_gradient_boosted_trees/distributed_gradient_boosted_trees.cc =====









absl::Status DistributedGradientBoostedTreesLearner::SetHyperParametersImpl(
  // Use the non-distributed GBT learner to set the configuration.









  // Extract a subset of supported non-distributed GBT parameters.








  // Extract and check the configuration.
  auto config = training_config();

  // Working directory.
  auto work_directory = deployment().cache_path();
  auto updated_deployment = deployment();

  // Detect if the training dataset is a stored in the dataset cache format
  // directly, or if the conversion should be done first.

    // The dataset is stored in the partially cache format.

    // TODO: Delete the partial dataset cache.
    // The dataset is stored in a generic format.

    // Create / resume the creation of the dataset cache.

    // TODO: Support for validation dataset in cache format.

  // Train the model.


absl::Status SetDefaultHyperParameters(

  // TODO: Call "SetDefaultHyperParameters" of GBT.

  // Select the loss function.


absl::Status CheckConfiguration(

absl::Status CreateDatasetCacheFromPartialDatasetCache(
  auto create_cache_config = spe_config.create_cache();


absl::Status CreateDatasetCache(
  auto create_cache_config = spe_config.create_cache();


  // Loss to optimize.

  // Allocate each feature to a worker.

  // Determine the number of each worker type.
  int num_train_workers, num_eval_workers;

    // Split the validation dataset among the evaluation workers.

  // Accumulator of validation loss and metrics.


  // Initializer the distribute manager.

  // Warn the workers that the training will start.


  // Initializer or restore the model.
  int iter_idx = 0;

  // Minimum iter index for the creation of a new checkpoint.
  int minimum_iter_for_new_checkpoint = -1;

  auto last_checkpoint_idx =
    // Restoring the model from the checkpoint.
    // Initializing a new model.
    // TODO: Send a ping to all the workers to make sure they all start
    // loading the dataset cache immediately (instead of waiting the first
    // request).




  // Name of the evaluated metrics.

  // The weak learners are predicting the loss's gradient.
  auto weak_learner_train_config = config;


  auto time_last_checkpoint = absl::Now();

    // Create a checkpoint.


      // A worker was restarted and is missing data.

      auto resync_iter_idx_status =
        // TODO: Restart training without rebooting the trainer.
      auto resync_iter_idx = resync_iter_idx_status.value();


      // Restart this iteration.

    // Create the final checkpoint

  // Display the final training logs.

  // Finalize the model with the assumption that all the trees are used i.e. not
  // early stopping.

  // Export training logs.

  // Stop the workers.

absl::Status SkipAsyncAnswers(int num_skip,

  // Progression.
  auto log = absl::Substitute(

  // Validation metrics.

  // Training metrics.

  // Computation resource monitor.

  // Load balancer.

absl::Status RunIteration(




    // Check if there is at least one open node.
    bool has_open_node = false;

    // Update the tree structure and update the label statistics.

    // Request for the workers to evaluate the splits.

    // Request for the workers to share the evaluation results,
    // update the tree structures, example->node mapping and label
    // statistics



  // Move the new trees in the model.

  // TODO: Early stopping.
  // TODO: Maximum training time.
  // TODO: Training interruption.

  // Note: The validation evaluation is asynchronous and is generally (unless a
  // checkpoint was just made, or this is the last iteration) late by one
  // iteration.

  // Display training logs.

  // Record training logs.

    // In the last iteration, the last validation is done synchronously.

  // Export training logs.


  auto model =


    // The model output might not be a probability.


absl::Status InitializeDirectoryStructure(
  // Create the directory structure.

absl::Status CreateCheckpoint(

  // Number of workers participating in the creation of the checkpoint.
  // A larger value reduces the cost of per worker, but increase the overhead
  // cost as well the chance to send a request to an interrupted worker.


  // Save the worker-side checkpoint content.
  // Also retrieve the "validation_aggregator" with any pending evaluation on
  // the validation dataset.

  // Save the model structure.


  // Record the snapshot.


absl::Status RestoreManagerCheckpoint(
    int iter_idx, absl::string_view work_directory,


bool ShouldCreateCheckpoint(
    int iter_idx, const absl::Time& time_last_checkpoint,





  // Copy "load_balancer" to "welcome.feature_ownership()".

      // Number of evaluation split sharing at the same time.

  // The request has not payload.

  // Select one worker at random.
      auto generic_answer,

absl::Status EmitSetInitialPredictions(

  // TODO: Implement multicast operations.

  // TODO: No need for an answer.



  // TODO: Implement multicast operations.

  // TODO: No need for an answer.





  // During of the self reported work duration for each worker. Does take into
  // account network communication or worker restarts.


  // Send the requests.
  int num_requests = 0;

    int num_selected_features;


    // Load balancing updates.

    // TODO: Only ask for splits is num_selected_features>0. Note: The
    // worker's code for FindSplit is responsible to clear the local split
    // evaluation.


  // Allocates the merged split objects.

  // Parse the replies.

  // Number of workers doing pre-loading work.
  int num_worker_preloading = 0;




    // Load balancing updates.







absl::Status EmitEvaluateSplits(


  // TODO: Implement multicast operations.



absl::Status EmitShareSplits(





    // Send requests.


    // Wait for the answer.


absl::Status EmitEndIter(int iter_idx, bool is_last_iteration,

  // Request for the trainer workers.

  // Request for the evaluation workers.
    // Copy the new tree.
    // TODO: Serialize the tree only one time instead of for each worker.

  // TODO: Implement multicast operations.
        // The first worker is in charge of computing the training loss.

  // TODO: No need for an answer.

      // Get the training loss value.


      // Get the validation evaluation of the previous or current iteration.


absl::Status EmitRestoreCheckpoint(


  // TODO: Implement multicast operations.

  // TODO: No need for an answer.

absl::Status EmitCreateCheckpoint(

  int retries = 0;

  // Examples contained in the "shard_idx" shard of a checkpoint.

  // Send the checkpoint request to a subset of the training workers.
  int num_requests = 0;

  // Send the checkpoint request to all the evaluation workers.
  //
  // Note: Exceptionnaly, we are sending two different types of requests at the
  // same time to make the checkpoint creation more efficient.



        // Evaluation worker. Just fail.
        // Training worker. Try another worker.

        // The worker was restarted and it misses the data required to create
        // the checkpoint. Re-send the request to another worker.


        // Send the request to another worker.

        // Get the validation evaluation of the previous or current iteration.

absl::Status EmitStartTraining(


  // TODO: Implement multicast operations.



    // Most of the time is used for the workers to load the dataset.



absl::Status SetLoadBalancingRequest(

      auto dst_order = generic_request->mutable_future_owned_features();


absl::Status SampleInputFeatures(

  // How many features to select for each split.
  int num_sampled_features = features.size();
    // Note: Default behavior (num_candidate_attributes=0) is to select all the
    // features.

  // Allocate output structure.

  // Sample for each weak learner and open node.
      // Sample

      // Export the sample for each worker.
          // Select worker with load balancer.


absl::Status SampleFeatures(const std::vector<int>& features,
                            int num_sampled_features,

  // TODO: Use std::sample when available.


absl::Status ExactSampledFeaturesForWorker(

  // TODO: implement if internal.duplicate.



void Monitoring::BeginTraining() {}

void Monitoring::BeginDatasetCacheCreation() {}

bool Monitoring::ShouldDisplayLogs() {

void Monitoring::BeginStage(Monitoring::Stages stage) {


void Monitoring::EndStage(Monitoring::Stages stage) {







void Monitoring::NewIter() {

void Monitoring::FindSplitWorkerReplyTime(int worker_idx,






absl::Status SetSplitsInPlan(

absl::Status DivideWorkers(const int num_all_workers,


  // List the files.

  // List the files for each workers.

  // Compile the files into a single comma separated path.


absl::Status PartialEvaluationAggregator::AddPartial(

    // Record the evaluation.
    // Merges two metrics weighted by "sum_weights".

    // Merge the loss and emtrics weighted by "sum_weights".

    // Add the example count and weights.




absl::Status PartialEvaluationAggregator::Export(

absl::Status PartialEvaluationAggregator::Import(


// ===== FILE: yggdrasil_decision_forests/learner/distributed_gradient_boosted_trees/distributed_gradient_boosted_trees.h =====


// Implementation of Gradient Boosted Trees learning algorithm using distributed
// training. Unless stated otherwise, the output model is the same as it would
// be if trained with the (non distributed) GradientBoostedTreesLearner.
//
// This algorithm is an extension of https://arxiv.org/abs/1804.06755
//
// DistributedGradientBoostedTreesLearner might only support a subset of the
// features (e.g. hyper-parameters) proposed in GradientBoostedTreesLearner. In
// this case, an error message will be raised during the
// DistributedGradientBoostedTreesLearner object construction.
//
// DistributedGradientBoostedTreesLearner only support training from a dataset
// path (i.e. training on in-memory dataset is not allowed).
//
// At the start of the training, the dataset is divided by columns and shards
// and then indexed.
//
// The learning algorithm support worker and manager interruption.
//
// Internal remarks
//   - The workers are divided into training and evaluation workers. The first
//     worker indices are used for training workers while the last indices are
//     used for evaluation workers.
//




class DistributedGradientBoostedTreesLearner : public AbstractLearner {





  absl::Status SetHyperParametersImpl(





// For the manager, a validation dataset is simply dataset's path.

// One weak model being constructed.
struct WeakModel {

// List of weak models being build.

// List of worker indices.

// Map worker idx -> features.

// Loss and metric values. The metrics are controlled by the loss
// implementation.
struct Evaluation {
  float loss = std::numeric_limits<float>::quiet_NaN();

// Monitoring of training resources.
//
// Used to measure and display the time each stage last.
class Monitoring {
  // Those stage names match the stage name in worker.proto.

  void BeginTraining();
  void BeginDatasetCacheCreation();
  bool ShouldDisplayLogs();
  void BeginStage(Stages stage);
  void EndStage(Stages stage);
  void NewIter();
  void FindSplitWorkerReplyTime(int worker_idx, absl::Duration delay);

  // Index of the current stage. -1 if not stage is enabled.
  int current_stage_ = -1;

  // Starting time of the current stage.

  bool logs_already_displayed_ = false;

  // Last time the logs were displayed.

  bool verbose_ = false;

  // Worker index and work duration of the last split stage.

  int last_fastest_worker_idx_ = -1;
  int last_slowest_worker_idx_ = -1;

  int count_reply_times_ = 0;

  struct StageStats {

  // Statistics for each of the stages.

  // Number of ran iterations so far.
  int num_iters_ = 0;

  // Starting time of the first iteration i.e. ~ start of the training.

// Aggregates partial evaluations (evaluation of the weak models trained in a
// single iteration and on a subset of a dataset). Used to compute the
// validation loss and metrics from the evaluation worker output.
class PartialEvaluationAggregator {
  // num_fragments: Number of partial evaluations required to form a complete
  //   evaluation. If 0, the aggregator is not "active".

  // Adds a partial evaluation.
  absl::Status AddPartial(const proto::Evaluation& evaluation);

  // Retrieves a full evaluation. Fails if the evaluation cannot be fulled build
  // i.e. if there are less than "num_fragments" evaluations with the requested
  // "iter_idx".

  // The aggregator can receive evaluations.
  bool Active() const { return data_.num_fragments() > 0; }

  absl::Status Export(proto::PartialEvaluationAggregator* proto);

  absl::Status Import(const proto::PartialEvaluationAggregator& proto);


absl::Status SetDefaultHyperParameters(

absl::Status CheckConfiguration(

// Create the dataset cache (i.e. indexed dataset values) from a generic datast
// path.
absl::Status CreateDatasetCache(

// Finalize the creation of a dataset cache from a partial dataset cache.
absl::Status CreateDatasetCacheFromPartialDatasetCache(

// Initialize the model for training.

// Train the model from a dataset cache.

// Run a single iteration of training.
absl::Status RunIteration(
    int iter_idx, const model::proto::TrainingConfigLinking& config_link,

// Skips the next "num_skip" asynchronous answers from the manager.
absl::Status SkipAsyncAnswers(int num_skip,

// If true, a checkpoint should be created as soon as possible.
bool ShouldCreateCheckpoint(
    int iter_idx, const absl::Time& time_last_checkpoint,

// Restores the content of a checkpoint.
absl::Status RestoreManagerCheckpoint(
    int iter_idx, absl::string_view work_directory,

// Creates a checkpoint of the model.
absl::Status CreateCheckpoint(
    int iter_idx,

// Console line showing the progress of training.

absl::Status InitializeDirectoryStructure(absl::string_view work_directory);


// The following "Emit{stage_name}" function are calling the workers with the
// {stage_name} work. All these functions are blocking. See "worker.proto" for
// the definition of the stages.


absl::Status EmitSetInitialPredictions(



// Returns the list of workers that owns the split evaluation data.
absl::Status EmitEvaluateSplits(

absl::Status EmitShareSplits(

absl::Status EmitEndIter(int iter_idx, bool is_last_iteration,

absl::Status EmitRestoreCheckpoint(
    int iter_idx, int num_shards, int num_weak_models,

absl::Status EmitCreateCheckpoint(
    int iter_idx, size_t num_examples, int num_shards,

absl::Status EmitStartTraining(

// Sets the load-balancing fields of in a worker request/result.
// The load balancing fields are only necessary for worker tasks that require
// access to the feature data.
absl::Status SetLoadBalancingRequest(
    int worker, distributed_decision_tree::LoadBalancer* load_balancer,

// Computes the list of active workers (i.e. workers having a job to do) and the
// features they have to check. Returns a mapping worker_idx -> weak_model_idx
// -> split_idx.

// Lists the features used in a set of splits.
struct ActiveFeature {
  struct Item {
    int weak_model_idx;
    int split_idx;

// Sets the "weak learner" and "split idx" fields in a plan from the "feature"
// field.
absl::Status SetSplitsInPlan(

// Samples the input features to send to a worker/weak model/node.
absl::Status SampleInputFeatures(
    int num_workers, const std::vector<int>& features,

// Randomly selects "num_sampled_features" features from "features".
absl::Status SampleFeatures(const std::vector<int>& features,
                            int num_sampled_features,

// Extracts the sampled features for a specific worker.
absl::Status ExactSampledFeaturesForWorker(

// Determines the number of workers per type.
absl::Status DivideWorkers(int num_all_workers,

// Splits the validation dataset among the evaluation workers.




// ===== FILE: yggdrasil_decision_forests/learner/distributed_gradient_boosted_trees/distributed_gradient_boosted_trees_test.cc =====







class DatasetAdult : public utils::TrainAndTestTester {
  void SetNumWorkers(const int num_workers) {

  void SetUp() override {




// Train and test a model on the adult dataset (with various number of workers).
  // Note: This result does not take early stopping into account.

  // Note: This result does not take early stopping into account.




  // Note: This result does not take early stopping into account.

// Train and test a model on the adult dataset with workers continuously
// failing and requiring checkpoint restoration.
  // Note: This result does not take early stopping into account.

// Train and test a model on the adult dataset with workers continuously
// failing and requiring checkpoint restoration for both the training and
// validation workers.
  // Note: This result does not take early stopping into account.

// Train and test a model on the adult dataset with aggressive (64 unique
// values) forced discretization of the numerical features.
  // Note: This result does not take early stopping into account.

// Train and test a model on the adult dataset.


  // Note: This result does not take early stopping into account.

// Train and test a model on the adult dataset.
  // Note: This result does not take early stopping into account.



  // Note: With early stopping, the non-distributed implementation of GBT has a
  // validation loss of 0.57404.
  // (currently) There is not any early stopping.
  // (currently) There is one evaluation for each iteration.

// Makes sure the exact same tree structure is learned by the distributed and
// classical algorithms. Note: We remove the features "education_num" as it is a
// duplication of "education" (i.e. same split scores) and can lead to different
// models.
  // Classical algorithm.
  // Note: The description includes a details of the tree structure.

  // Distributed algorithm.
  // Note: The description includes a details of the tree structure.


// The load balancer continuously change the worker<->feature mapping.
  // Note: This result does not take early stopping into account.

class DatasetIris : public utils::TrainAndTestTester {
  void SetUp() override {




// Train and test a model on the adult dataset.
  // Note: This result does not take early stopping into account.
  // Note: R RandomForest has an OOB accuracy of 0.9467.

class DatasetDna : public utils::TrainAndTestTester {
  void SetUp() override {




// Train and test a model on the adult dataset.
  // Note: This result does not take early stopping into account.
  // Note: R RandomForest has an OOB accuracy of 0.909.

class DatasetAbalone : public utils::TrainAndTestTester {
  void SetUp() override {




// Train and test a model on the adult dataset.
  // Note: This result does not take early stopping into account.




// Test training on 2.2B examples (i.e. more than 2^31). The test will fail if
// YDF is compiled with 32-bits example index.
//
// This test is disabled in presubmit as it requires a large amount of disk
// (~100GB) and RAM. The test takes ~15 minutes to run on a workstation. You can
// make the test run fast / require less resource by reducing the number of
// examples.
//
// The test can be run manually as follow:
//
// bazel test -c opt --test_strategy=local --test_output=streamed --test_ \
// arg=--alsologtostderr --copt=-mfma --copt=-mavx2 \
// --define=ydf_example_idx_num_bits=64 \
// //yggdrasil_decision_forests/learner/distributed_gradient_boosted_trees:distributed_gradient_boosted_trees_test\
// --test_filter=LargeDataset.Base --test_timeout=1000000
//
// This test creates a large amount of data in the
// "/tmp/ydf_large_distributed_test" directory. Executing the test multiple time
// will re-use this data possibly skipping some of the computation (e.g.,
// creation of the synthetic dataset, creation of the dataset cache, training of
// the model) and run significantly faster.
//
// This test can be run on a large (but still smaller than 2B examples) dataset
// by decreasing the "num_shards" value.

  // Create a large csv dataset.



      auto writer = file::OpenOutputFile(path).value();
        // Note: f1 will be discretized, hence faster to train than f2.





  auto model =


class DatasetSyntheticRanking : public utils::TrainAndTestTester {
  void SetUp() override {





// Train and test a model on the adult dataset.


// ===== FILE: yggdrasil_decision_forests/learner/distributed_gradient_boosted_trees/worker.cc =====










// Index of the worker in the training worker pool.
int DistributedGradientBoostedTreesWorker::TrainingWorkerIdx() const {

// Index of the worker in the evaluation worker pool.
int DistributedGradientBoostedTreesWorker::EvaluationWorkerIdx() const {

int DistributedGradientBoostedTreesWorker::NumEvaluationWorkers() const {

absl::Status DistributedGradientBoostedTreesWorker::Setup(



    // Load the dataset.
    auto read_options = spe_config.dataset_reader_options();



    // Load evaluation worker datasets.

    // Load the dataset in memory.
    // TODO: Only load the necessary columns.
      // Empty dataset.

    // Extract / compute the example weights.


  // Training loss.

  // Threadpool.



  auto status_or = RunRequestImp(std::move(serialized_request));





  // [For unit testing only] Simulate failure of the workers.
  // Each message type (i.e. request.type_case()) will fail one on each worker.

  // Determine if the worker is in the right state for this request. If not,
  // this indicates that either the worker or the manager was restarted.
    bool missing_data = false;


      // The worker was restarted during the training of this tree. Tell the
      // manager to restart the training of this tree.

  // Make sure the requested features are available.
  // Such change open append when the worker is restarted.

  // Non-blocking pre-loading of the features that will be required in the
  // future.








      // TODO: Since the answer is large and the same for all the workers,
      // can the answer be somehow not duplicated in memory?








    // Update the manager about training dataset loading status (in case the
    // dataset allocation was changed by the load balancer).


void DistributedGradientBoostedTreesWorker::MaybeSimulateFailure(

  // The index of the requests in WorkerRequest::type.


      // Reset the worker to its initial state.

absl::Status DistributedGradientBoostedTreesWorker::Done() {

absl::Status DistributedGradientBoostedTreesWorker::GetLabelStatistics(


      // The ranking loss does not use label statistics.


    int num_weak_models) {


  // Allocate the memory for the gradient and hessian. Create a
  // "label_accessor" for each gradient+hessian buffer.



  // The "gradient_ref" is used by some methods to access the gradient+hessian
  // buffers.


absl::Status DistributedGradientBoostedTreesWorker::SetInitialPredictions(





absl::Status DistributedGradientBoostedTreesWorker::StartNewIter(


  // Computes the initial gradient.

  // The weak learners are predicting the loss's gradient.
  auto weak_learner_train_config = welcome_.train_config();





    // Initialize the statistics of the root node (the only node in the tree).


    // Label statistics for the manager.




absl::Status DistributedGradientBoostedTreesWorker::FindSplits(


  // Collect the active features i.e. the features on which to find splits.
  // Model idx -> node idx -> list of features.
  // Model idx -> list of features.



    // List the features to test.

    // List all the features tested by at least one node.


  absl::Status worker_status;

  // Find the best splits.

    // Look for the splits.


  // Save the best splits into the reply.

absl::Status DistributedGradientBoostedTreesWorker::EvaluateSplits(


    // Clear the last split evaluation.




absl::Status DistributedGradientBoostedTreesWorker::UpdateOwnedFeatures(

  // New features to load i.e. target_features / initial_features

  // Features to unload i.e. initial_features / target_features.


    // TODO: Just wait?


  // We ignore the unloading instructions.

  // Is the request similar at the already running process?

    // Pre-loading is already running.

    // Check and update running status.

      // Still running.
      // Just done running.

        // Quickly start the pre-loading of the request (because it was
        // different from the execution).




absl::Status DistributedGradientBoostedTreesWorker::


absl::Status DistributedGradientBoostedTreesWorker::ShareSplits(
  // Request the split evaluation from other workers.

  // Aggregate all the split evaluations.
    auto reply_status =
    auto generic_other_result = std::move(reply_status).value();

      // The target worker does not have the required data.

      // const auto& iter = layer_per_weak_models[weak_model_idx];






    int num_skip) {

absl::Status DistributedGradientBoostedTreesWorker::GetSplitValue(

  // Allocate the answer.

  // Copy the split evaluations.


absl::Status DistributedGradientBoostedTreesWorker::EndIter(

absl::Status DistributedGradientBoostedTreesWorker::EndIterTrainingWorker(

    // Closing all the remaining open nodes.



absl::Status DistributedGradientBoostedTreesWorker::EndIterEvaluationWorker(
  // Grab + send the result of any pending evaluation.

  // Deserialize the new trees into a GBT model without bias (since the bias is
  // already applied at the initialization of the prediction accumulator).
  auto partial_model =

  // The model is used to accumulate pre-activation predictions.
    auto dst_weak_model = std::make_unique<decision_tree::DecisionTree>();

  // Compile the model into an engine.
  // Note: We currently only support models that are compatible with at least
  // one fast engine.

  // Start the evaluation thread.

  // If the evaluation is blocking; wait for it to finish.


absl::Status DistributedGradientBoostedTreesWorker::RunValidationThread(

absl::Status DistributedGradientBoostedTreesWorker::JoinValidationThread() {

bool DistributedGradientBoostedTreesWorker::HasPendingValidationThread() {


  // Run the weak model in batch mode over the validation dataset.
  // TODO: Multi-thread evaluation.


  // Cache of temporary working data for each thread. This helps reducing the
  // amount of heap allocations.
  struct CachePerThread {

  // Allocate the caches.

  // Schedule the prediction updates.




  // Note: We don't care about the results.

  // Evaluate the validation predictions.



absl::Status DistributedGradientBoostedTreesWorker::RestoreCheckpoint(

    // Reads all the predictions.

    // The worker will receive a "StartNewIter" message with
    // "iter_idx=iter_idx_" (before --) next.

    // Evaluation worker.

    // Read the predictions of my worker only.


absl::Status DistributedGradientBoostedTreesWorker::CreateCheckpoint(

  // Export the predictions of a single shard (even though I have all the shards
  // in memory).

absl::Status DistributedGradientBoostedTreesWorker::CreateEvaluationCheckpoint(
  // Flush the pending validation evaluations.

  // Export the worker predictions.


absl::Status DistributedGradientBoostedTreesWorker::StartTraining(


absl::Status UpdateClosingNodesPredictions(
  // Collect the increase of prediction value for each closing node.

  // Increase the prediction values.
            // The example is not is a closed node.

            // This example remains in an open node.

          // The example is in a node that is closed during this iteration.




// ===== FILE: yggdrasil_decision_forests/learner/distributed_gradient_boosted_trees/worker.h =====





class DistributedGradientBoostedTreesWorker

  absl::Status Setup(distribute::Blob serialized_welcome) override;


  absl::Status Done() override;


  // Internal data related to one weak model e.g. a decision tree.
  struct WeakModel {
    // Gradient / hessian of the GBT i.e. pseudo response of the weak model.

    // Accessor to the pseudo response data.

    // How to access the data in the "label_accessor".

    // Example to open node assignation.

    // Decision tree being build.

    // Label statistics (from the pseudo response) for each open node.

    // Last evaluation of the split value requested in "EvaluateSplits".

    // Split evaluation of the current iteration. Those contains both the split
    // evaluation computed by this worker (as the end of the "EvaluateSplits"
    // stage) as well as the split evaluation received from other workers (as
    // the end of the "ShareSplits" stage).

    bool has_multiple_node_idxs;

  // Internal data related to the training of one layer of a weak model.
  struct WeakModelLayer {
    // Split for each open nodes.

    // Evaluation of the splits for each open nodes.

  // The type of worker.
    // Find optimal splits and evaluate the model on the training dataset.
    // Evaluate the model on the validation dataset (if any).


  // Simulates worker failures and restart. A failure is artificially generated
  // for each worker (by index) and for each task exactly once. Failure are
  // generated according to the iteration index. For example, the failure on
  // request #4 and worker #2 might be generated on iteration #16 (not exact
  // values). Used for testing.
  void MaybeSimulateFailure(proto::WorkerRequest::TypeCase request_type);

  // The following methods with stage names are defined in "worker.proto".

  absl::Status GetLabelStatistics(

  absl::Status SetInitialPredictions(

  absl::Status StartNewIter(const proto::WorkerRequest::StartNewIter& request,

  absl::Status FindSplits(const proto::WorkerRequest::FindSplits& request,

  absl::Status EvaluateSplits(

  absl::Status ShareSplits(const proto::WorkerRequest::ShareSplits& request,

  absl::Status GetSplitValue(const proto::WorkerRequest::GetSplitValue& request,

  absl::Status EndIter(const proto::WorkerRequest::EndIter& request,
  absl::Status EndIterTrainingWorker(
  absl::Status EndIterEvaluationWorker(

  absl::Status RestoreCheckpoint(

  absl::Status CreateCheckpoint(

  absl::Status CreateEvaluationCheckpoint(

  absl::Status StartTraining(const proto::WorkerRequest::StartTraining& request,

  // End of stage names.

  // Change the features owned by the worker.
  absl::Status UpdateOwnedFeatures(std::vector<int> features);

  // Initiate the pre-loading of features for future usage.
  //
  // True true if any preloading work is in progress.

  // Merges the split evaluation into the "last_split_evaluation" field of each
  // weak model. After this function call, "src_split_values" is invalid. This
  // method is thread safe (can be called at the same time from different
  // threads).
  absl::Status MergingSplitEvaluationToLastSplitEvaluation(

  // Loss of a set of predictions.

  // Initialize the working memory of the worker. This stage requires the number
  // of weak models and cannot be done during the "setup" stage. This method is
  // called by the "SetInitialPredictions" message (i.e. the start of the
  // training) or "RestoreCheckpoint" message (i.e. resuming training from a
  // checkpoint).
  absl::Status InitializeTrainingWorkerMemory(int num_weak_models);

  // Skips the next "num_skip" answers on the worker-to-worker async channel.
  absl::Status SkipAsyncWorkerToWorkerAnswers(int num_skip);

  // The worker's type define its job.

  // Index of the worker in the training worker pool.
  int TrainingWorkerIdx() const;

  // Index of the worker in the evaluation worker pool.
  int EvaluationWorkerIdx() const;

  // Number of evaluation workers.
  int NumEvaluationWorkers() const;

  // Integrates the weak model "validation_weak_model_" into the prediction
  // cache, and update the validation loss and metrics "validation_evaluation_".
  absl::Status EvaluateWeakModelOnvalidationDataset();

  // Starts "EvaluateWeakModelOnvalidationDataset" in the validation thread.
  absl::Status RunValidationThread(int iter_idx);

  // Join a running "EvaluateWeakModelOnvalidationDataset" thread.
  absl::Status JoinValidationThread();

  // Test if a validation thread was started and can be joined.
  bool HasPendingValidationThread();

  // Static configuration provided by the manager. Initialized during the
  // "setup" stage.

  // Training dataset. Initialized during the "setup" stage.
  // Only present if the worker is a training worker.

  // Current training iteration. -1 before the training starts. Updated during
  // training.
  int iter_idx_ = -1;

  // Unique identifier of the current iteration. Updated during training.

  // Random seed for the current iteration. Updated during training.

  // Training loss.  Initialized during the "setup" stage.

  // Accumulated predictions of the current model on the training dataset.
  // Array of size num_examples x num_output_dim. Initialized with the
  // "SetInitialPredictions" message.

  // Training data for each weak model in the current iteration. Initialized
  // with the "SetInitialPredictions" message.

  // Fields related to the evaluation of the model on the validation dataset.
  // Only available for evaluation workers.
  struct {
    // Validation dataset. Initialized during the "setup" stage.
    // Only present if the worker is an evaluation worker.

    // Weight of the examples.

    // Accumulated predictions of the current model on the validation dataset.
    // Array of size num_examples x num_output_dim. Initialized with the
    // "SetInitialPredictions" message.

    // Weak model currently being evaluated on the validation dataset.

    // Thread that run the method "EvaluateWeakModelOnvalidationDataset" that
    // evaluate the weak model "validation.weak_model_" and export the results
    // to "validation.evaluation_".

    // Last evaluation. Computed by "EvaluateWeakModelOnvalidationDataset".

    // Status of the last run of "EvaluateWeakModelOnvalidationDataset".
    absl::Status status;

    // Ranking index for the validation dataset.

  // Accessor to the pseudo response. Initialized with the
  // "SetInitialPredictions" message.

  // The worker received the initial model predictions from the manager.
  // Initialized with the "SetInitialPredictions" message.


  // List of worker request (i.e. proto::WorkerRequest::k*) where an failure was
  // already simulate. Only for unit testing.

  // Working threads. Initialized during the "setup" stage.

  // True iff. the worker was asked to stop i.e. "Done" was called.

  // Number of running "RunRequest".
  int num_running_requests_ = 0;

  // Time taken to load the features of the dataset in memory.
  int dataset_num_features_loaded_ = 0;

  // Prints details about the computation with LOG(INFO).
  bool worker_logs_ = true;

  // Ranking index for the training dataset.

// Extract the requested features in a FindSplits request.

// Update the "predictions_" of the examples in nodes closed in
// "node_remapping".
absl::Status UpdateClosingNodesPredictions(
    int weak_model_idx, int num_weak_models, std::vector<float>* predictions,





// ===== FILE: yggdrasil_decision_forests/learner/generic_worker/generic_worker.cc =====







absl::Status GenericWorker::TrainModel(




  // TODO: Interrupt the training if override_model=false and the model is
  // present.







absl::Status GenericWorker::EvaluateModel(


  auto options = request.options();

  // Evaluation weighting.

  // Load dataset.

  // Evaluate model.


absl::Status GenericWorker::Setup(Blob serialized_welcome) {



// ===== FILE: yggdrasil_decision_forests/learner/generic_worker/generic_worker.h =====


// Collection of generic workers for simple distributed training algorithms e.g.
// train a model, evaluate a model.
//
// See generic_worker.proto for the list of available works.
//


class GenericWorker : public distribute::AbstractWorker {

  absl::Status Setup(distribute::Blob serialized_welcome) override;


  absl::Status Done() override {

  absl::Status TrainModel(const proto::Request::TrainModel& request,

  absl::Status EvaluateModel(const proto::Request::EvaluateModel& request,


  // Set to true when Done is called on the "GenericWorker".





// ===== FILE: yggdrasil_decision_forests/learner/generic_worker/generic_worker_test.cc =====





// Create a single thread manager with 5 workers.

  auto manager = CreateSingleThreadManager();



  auto train_result =


  auto evaluate_result =





// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.cc =====







// Generic hyper parameter names.






// During training, the training dataset is duplicated (shallow copy) and
// modified to train each individual tree. This modified dataset is called the
// "gradient dataset" (because the label is the gradient of the loss).
// Base name of the gradient and hessian column in the gradient dataset.
// Note: The hessian column is only created if necessary (e.g. non-constant
// hessian and use_hessian_gain=true).

// Filename of the file containing the early stopping state in a checkpoint.

// Name of the gradient column in the gradient dataset.

// Name of the hessian column in the gradient dataset.


// Removes snapshots if they exist. Do not fail if the snapshot does not exist.
void RemoveSnapshotsIfExist(const absl::string_view cache_path,

// Creates the training configurations to "learn the gradients".
// Note: Gradients are the target of the learning.
void ConfigureTrainingConfigForGradients(

// Returns the task used to train the individual decision trees. This task might
// be different from the task that the GBT model is trained to solve.
//
// For example, in case of loss=BINOMIAL_LOG_LIKELIHOOD (which implies a binary
// classification), the trees are "regression trees".
  // GBT trees are always (so far) regression trees.

// Splits the training shards between effective training and validation.
absl::Status SplitShards(std::vector<std::string> all,


    // No validation.


// Sample a subset of the candidate shards without replacement.
  // Note: Could use std::sample in C++17.

// Truncate the model (if early stopping is enabled), update the validation loss
// and display the final snippet.
absl::Status FinalizeModelWithValidationDataset(
    int early_stopping_initial_iteration =




  // Final snippet




absl::Status MaybeExportTrainingLogs(const absl::string_view log_directory,

absl::Status FinalizeModel(const absl::string_view log_directory,
  // Cache the structural variable importance in the model data.



// Restores all training states from the last available snapshot. If no snapshot
// is available, do nothing. All arguments have the same name to the
// corresponding variables in the main training loop defined in
// "TrainWithStatus".
//
// A failure occurs only if a snapshot is found but cannot be loaded. In other
// words, not finding any snapshot does not return a failure.
absl::Status TryLoadSnapshotFromDisk(
  // Find the last snapshot, if any.

  // Load the model in the snapshot.

  // Load the model structure.

  // Load the state of the early stopping state manager.

  // Recompute the prediction caches.


    // Recompute the prediction caches for the validation dataset.

// Creates and record a snapshot.
absl::Status CreateSnapshot(const model::proto::DeploymentConfig& deployment,

  // Save the model structure.

  // Save the early stopping manager state.

  // Record the snapshot.

  // Remove old snapshots.



absl::Status GradientBoostedTreesLearner::CheckConfiguration(






absl::Status GradientBoostedTreesLearner::BuildAllTrainingConfiguration(





  // Select the loss function.



  int trees_per_iteration = all_config->loss->Shape().gradient_dim;
  int specified_num_trees = all_config->gbt_config->num_trees();
  int specified_initial_iteration =


  auto mdl = std::make_unique<GradientBoostedTreesModel>();

    // The model output might not be a probability.

    // Regular training.


  // The logic of this method is similar to "TrainWithStatus", with the
  // exceptions:
  // - The loss on the training dataset is  computed.
  // - Instead of using a dataset loaded in memory, each tree is trained on a
  //   dataset sampled using shards.
  // - No support for the DART algorithm.
  // - No support for Ranking.

  // TODO: Splitting method.


  // Initialize the configuration.

  // Initialize the model.
  auto mdl = InitializeModel(config, data_spec);


  // Get the dataset shards.



  // Split the shards between train and validation.
  bool has_validation_dataset;

    // The user provided a validation dataset.
    // Extract a validation dataset from training dataset.
    int num_validation_shards = std::lround(

  // Load and prepare the validation dataset.


  // Load the first sample of training dataset.
  int num_sample_train_shards =

  // Timer accumulators.
  struct {
    // Amount of time spent waiting for the preparation thread (IO + parsing +
    // preprocess).
    // Amount of time in the shard loading logic (IO + parsing).
    // Amount of time in the shard preprocess logic (running the previously
    // learned trees).

  // Fast version of the model. The fast engine is cheaper to run but more
  // expensive to construct.
  int num_trees_in_last_engine = 0;

  // Load a random sample of training data, prepare it for the weak learner
  // training, and compute the cached predictions with "trees".
  auto load_and_prepare_next_sample =
    auto time_begin_load = absl::Now();


    auto time_begin_predict = absl::Now();

    auto time_end_all = absl::Now();

  // List of selected examples. Always contains all the training examples.

  // Thread loading the sample of shard for the next tree.
  // Note: The shard loaded in multi-threaded by the vertical dataset IO lib.

  // Begin time of the training, excluding the model preparation. Used to
  // compute the IO bottle neck.

  // Gets the fraction of time spend waiting for the loader thread (and not
  // training).

  // Amount of time spend in preprocessing in the preparation of the shards.

    // If true, the sample in "current_train_dataset" will be re-used (instead
    // of discarded and replaced by "next_train_dataset").

    // Same as "recycle_current", but for the next iteration.

      // Retrieve the set of sharded being loaded.
        // Wait for the loading thread.

        // Note: At this point, the pre-computed predictions do not take into
        // account the trees added in the last iteration.

        // Add the predictions of the trees learned in the last iteration(s).

          // Caches the predictions of the trees.


      // Start the loading of the next training sample.
      //
      // Note: We don't need to do it for the last tree.
        // Compile the trees into an engine.
        auto engine_or = mdl->BuildFastEngine();

        // Extract the trees of the current model.


      // Select all the training examples in the sample.

      // The first dataset is used to compute the initial predictions (a.k.a
      // bias).



    // Compute the gradient.
    // Compute the gradient of the residual relative to the examples.

    // Train a tree on the gradient.
      auto tree = std::make_unique<decision_tree::DecisionTree>();



      // Update the predictions on the validation dataset.

      // Update the predictions on the sample because it will be recycled.

    // Add the tree to the model.

    // Validation & training logs






        // Early stopping.





    // Export intermediate training logs.

  // Wait for the loader to stop. This is possible if the training was stopping
  // by early stopping.


  // The training of the model works as follows:
  //
  // 1. Determine the "constant prediction" part of the model
  // 2. Compute the predictions and the loss of the model.
  // 3. Determine the gradient of the loss according to each training example.
  // 4. Train a tree to predict this loss gradient.
  // 5. Add this tree to the model (with a small negative factor e.g. -0.1 i.e.
  // minus the shrinkage).
  // 6. Goto 2.
  //
  // Extra:
  //   - The algorithm extracts a validation dataset to monitor the
  //     performances of the model during training (i.e. at regular interval,
  //     the model is evaluated on the validation dataset).
  //   - Each tree is trained on a random sample of the gradients (instead of
  //     all the gradients).


  // Initialize the configuration.



  // Initialize the model.
  auto mdl = InitializeModel(config, train_dataset.data_spec());


  bool has_validation_dataset;

    // Divide the original training dataset into a validation and a training
    // dataset.


  // Compute the example weights.

  // Determines if the training code supports `weights` to be empty if
  // all the examples have the same weight. This triggers special handling for
  // improved performance.
  //
  // This feature is not supported for GOSS (where weights are always used),
  // uplifting and custom losses.
  bool use_optimized_unit_weights = true;

  // Initialize the training dataset for individual trees (called gradient
  // dataset). Note: Individual trees are trained to predict a gradient (instead
  // of the actual dataset labels).
  // Gradient specific information (values and training configuration).
  // Current predictions of the model on the "sub_train_dataset" dataset BEFORE
  // the activation function. Stored example wise i.e. sub_train_predictions[i +
  // j * col_size] is the i-th coordinate the prediction for the j-th example.
  // Initialize the gradient dataset.
  // Note: At each iteration, one tree is created for each gradient dimensions.


  // Training configuration for individual trees i.e. to predict the gradient.

  // Controller for the adaptive subsample parameter.

  // Compute and set the initial prediction of the model i.e. the "constant
  // prediction" independent of the trees.

  bool dart_extraction = config.gbt_config->forest_extraction_case() ==

  // Initialize the Dart accumulator (if Dart is used).






  // Time of the next snapshot if training resume is enabled.
  auto next_snapshot =

  // Path to the root snapshot directory used to resume interrupted training.
  // Empty if resuming training is disabled.

  // Current training iteration.
  int iter_idx = 0;

  // Sorted deque of the past iterations with snapshots.

  // Try to resume training.

  // Train the trees one by one.

  // Switch between weights and GOSS-specific weights if necessary.
    // The user interrupted the training.


    // Compute the gradient of the residual relative to the examples.

    float subsample_factor = 1.f;
    // Select a random set of examples (without replacement).

        // Reset train weights.

        // Sample examples with GOSS and adjust train weights accordingly.

    // Train a tree on the gradient.
      auto tree = std::make_unique<decision_tree::DecisionTree>();

      auto internal_config = internal::BuildWeakLearnerInternalConfig(


    // Note: Since the batch size is only impacting the training time (i.e.
    // not the update prediction time), and since the adaptive work manager
    // assumes a linear relation between work and time, we only measure the
    // duration of the training step.

      // Update the Dart cache and the predictions on the training dataset.

        // Update the dart cache and the predictions on the validation dataset.
      // Update the predictions on the training dataset.

        // Update the predictions on the validation dataset.

    // Add the tree to the model.







        // Early stopping.




    // Export intermediate training logs.

    // Export a snapshot.

  // Create a final snapshot.


    // Scale the trees output values.




absl::Status GradientBoostedTreesLearner::SetHyperParametersImpl(
  // Decision tree specific hyper-parameters.



  // Determine the sampling strategy.
    // The preferred way of choosing a sampling method and setting its params.


      // If no other sampling method is selected, we activate the "subsample"
      // field for backwards compatibility.

  // Set sampling strategy's hyperparameters.
        // Note: Force stochastic gb if the sampling method is "NONE" and the
        // "subsampling" parameter is set.

















  // Note: We don't optimize the number of tree as it is always beneficial
  // metric wise, and we don't optimize the inference time or model size (yet).





    // Random sampling method (aka, subsample).

    // Note: GOSS is not part the HP sampling domain as this parameter is only
    // expected to speed-up training (and not impact the model in a good way).

    // Selective Gradient Boosting sampling method.






















    // kSamplingMethodSelGB is only available for Ranking, but is not excluded
    // here.
















  // Timeout in the tree training.


      // Note: "number_of_unique_values() == 3" because of the reserved
      // "out-of-dictionary" item.




  auto complete_dataset =





  // Training configuration for individual trees i.e. to predict the gradient.


absl::Status ExtractValidationDataset(const VerticalDataset& dataset,

      // Sampling per example.
      // Extract groups.

        // Get the value of the group.



      // Sampling per groups.


absl::Status CreateGradientDataset(const dataset::VerticalDataset& dataset,

    // Allocate gradients.







  // Allocate predictions.

absl::Status ComputePredictions(
    // Prediction using the engine (fast).


    auto examples =




  // Predictions using the trees (slow).

void SampleTrainingExamples(

    // Ensure at least one example is selected.

void SampleTrainingExamplesWithGoss(
  // Compute L1 norm of the gradient vector for every example.
    float example_l1_norm = 0.f;

  // Sort examples by the L1 norm of gradients in decreasing order.

  // Add examples with a large gradient to the set of selected examples.

  int cutoff = std::ceil(alpha * num_rows);

  // From the remaining examples, randomly select a subset and adjust weights.

  // Ensure at least one example is selected.

absl::Status SampleTrainingExamplesWithSelGB(




    // Add positive examples to the set, and prepare negative examples for
    // down-sampling.

    // Sort negative examples by prediction scores in decreasing order.
    // Add the top examples to the set of selected examples.

absl::Status ExportTrainingLogs(const proto::TrainingLogs& training_logs,
  // Add methods to plot training logs here.

  // Export the logs in a .csv file.



void InitializeModelWithTrainingConfig(

void DartPredictionAccumulator::Initialize(

    float dropout, utils::RandomEngine* random) const {

  // Select each iter with the "dropout" probability.

  // Ensure at least one iter is selected.


absl::Status DartPredictionAccumulator::GetAllPredictions(

absl::Status DartPredictionAccumulator::GetSampledPredictions(
    float acc = predictions_[example_idx];

absl::Status DartPredictionAccumulator::UpdateWithNewIteration(
    int num_gradient_dimensions, double* mean_abs_prediction) {
  // Compute the predictions of the new trees.


  // Update the global predictions.





  // Update the weight of the selected iterations.



void SetInitialPredictions(const std::vector<float>& initial_predictions,

absl::Status SetDefaultHyperParameters(


    // The basic definition of GBT does not have any attribute sampling.


    // Clear deprecated fields.

    // Clear deprecated fields.
    // No sub-sampling.


  // Select sorting strategy.






// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees.h =====


// Implementation of Gradient Boosted Trees (one of the popular versions of
// Gradient Boosted Model).
//
// A GBT model is a set of shallow decision trees whose predictions are computed
// by summing the output of each tree. The space of the summation depends on the
// model e.g. logit space is common for binary classification.
//
// The trees are trained sequentially. Each tree is trained to predict and then
// "correct" for the errors of the previously trained trees. More precisely,
// each tree is trained to predict the gradient of a loss function relative to
// each training example. The prediction of the tree is then set to a small
// negative value (called the shrinking) times the predicted gradient.
//
// For more details:
// https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting
//
// Uses the CART implementation of YDF i.e. this implementations supports
// numerical, categorical and categorical set input features.
//
// This implementation supports Classification (binary and multi-classes),
// regression and ranking.
//
// Use the loss functions and implementation details detailed in:
//   https://statweb.stanford.edu/~jhf/ftp/trebst.pdf
//   https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf
//   https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf





struct AllTrainingConfiguration;

// A GBT learner i.e. takes as input a dataset and outputs a GBT model.
// See the file header for a description of the GBT learning algorithm/model.
class GradientBoostedTreesLearner : public AbstractLearner {


  // Generic hyper parameter names.





  // Detects configuration errors and warnings.


  absl::Status SetHyperParametersImpl(





  // Generates, checks and groups all the configuration objects.
  absl::Status BuildAllTrainingConfiguration(

  // Sets the custom loss function to use if loss is CUSTOM.
  void SetCustomLossFunctions(

  // Initializes and returns a model.

  // Training with dataset sampling using shards.




// Builds the internal (i.e. generally not accessible to user) configuration for
// the weak learner.

// Divide "dataset" into a training and a validation dataset.
//
// If "validation_set_ratio==0", "train" becomes a shallow copy of "dataset" and
// no data is copied.
//
// If "group_column_idx" is -1, the sampling is uniform. Otherwise, the split
// between train and validation ensures that all given "group_column_idx" values
// are either in the train or in the validation dataset. In this case, the
// sampling is a greedy heuristic that tries to optimize the balance.
absl::Status ExtractValidationDataset(const dataset::VerticalDataset& dataset,
                                      float validation_set_ratio,
                                      int group_column_idx,

// Initialize the gradient dataset "gradient_dataset" used to train individual
// trees. This dataset contains a shallow copy of "dataset" and a new column
// for each gradient dimension (named with "GradientColumnName").
//
// If "hessian_splits=true", a column containing the hessian is also created
// with a name generated by "HessianColumnName".
//
// "gradients" is initialized with the name of the gradient columns as well as a
// non-owning pointer to the gradient data in "gradient_dataset".
//
// "predictions" is initialized to contain the predictions.
absl::Status CreateGradientDataset(const dataset::VerticalDataset& dataset,
                                   int label_col_idx, bool hessian_splits,

// Copy the initial model predictions to the accumulator of predictions.
template <typename V>
void SetInitialPredictions(const std::vector<float>& initial_predictions,

// Computes the predictions and gradient of the model without relying on
// existing predictions or gradient buffers.
//
// Only the meta-data are used from "mdl". If "optional_engine" is non-null, it
// will be used in conjunction with "trees".
absl::Status ComputePredictions(

// Sample (without replacement) a set of example indices.
void SampleTrainingExamples(UnsignedExampleIdx num_rows, float sample,

// Sample a set of example indices using the GOSS algorithm.
void SampleTrainingExamplesWithGoss(
    float alpha, float beta, utils::RandomEngine* random,

// Sample a set of example indices using the Selective Gradient Boosting
// algorithm. The algorithm always selects all positive examples, but selects
// only those negative training examples that are more difficult (i.e., those
// with larger scores).
absl::Status SampleTrainingExamplesWithSelGB(

// Export the training logs. Creates:
// - A static plot (.svg) of the training/validation loss/secondary metric
//   according to the number of trees.
// - An interactive plot of the same type.
absl::Status ExportTrainingLogs(const proto::TrainingLogs& training_logs,

void InitializeModelWithTrainingConfig(

// Accumulator of predictions for individual trees in the Dart algorithm.
// Supports the operations required by the Dart training (e.g. extracting
// predictions from a large number of randomly selected trees).
class DartPredictionAccumulator {
  // Initialize the prediction accumulator. This function must be called before
  // any other function. After this call, the accumulator represents a GBDT
  // model without any tree.
  void Initialize(const std::vector<float>& initial_predictions,

  // Sample a set of iteration indices (Note: one iteration = one tree for
  // regression or binary classification) to be EXCLUDED (i.e. dropped) for the
  // computation of the gradient of the next iteration. "dropout=0" means only
  // one iteration will be excluded (i.e. the algorithm become almost equivalent
  // to a GBDT; modulo that at least one tree should be sampled), "dropout=1"
  // means maximum dropout i.e. all the iterations will be excluded i.e. the
  // algorithm is similar to a Random Forest (modulo the loss).

  // Gets the predictions from the current model but without the iterations
  // specified in "dropout_iter_idxs". The predictions vector should already be
  // initialized.
  absl::Status GetSampledPredictions(const std::vector<int>& dropout_iter_idxs,

  // Gets the predictions from the current model i.e. from all the iterations.
  // The predictions vector should already be initialized.
  //
  // Equivalent to "GetSampledPredictions" with "dropout_iter_idxs" empty.
  absl::Status GetAllPredictions(std::vector<float>* predictions);

  // Updates the accumulator with a set of trees obtained thought a single
  // iteration.
  absl::Status UpdateWithNewIteration(
      int num_gradient_dimensions, double* mean_abs_prediction = nullptr);

  // Returns the optimal multiplicative factor for each tree in the order
  // provided to "UpdateWithNewIteration". For example, if the model contains
  // two tree t_1 and t_2, and the multiplicative factors are f_1 and f_2, the
  // final predictions of the model is:  Activation(f_1 * t_1 + f_2 * t_2).

  struct TreePredictions {
    // Weights over all the predictions.
    float weight;

    // Predictions of the tree before weighing.

  // Predictions of all the trees summed and weighed i.e. current predictions of
  // the model.
  //
  // Note:
  //   predictions_[i] = \sum_j prediction_per_tree_[j].predictions[i] *
  //   prediction_per_tree_[j].weights + initial_prediction.

  // Predictions of individual trees.

// All the configuration object derived from the user training configuration.
struct AllTrainingConfiguration {

  // The effective training configuration i.e. the user training configuration
  // with set default values and generic hyper-parameters.

  // Similar to "train_config_", but with feature name replaced by feature
  // indices.

  // Configuration specific to the gbt in "train_config_".

  // Implementation of the loss. Have non-owning dependencies to
  // "train_config_"'s content.

  // Grouping column for the train/validation dataset split. A value of "-1"
  // indicates that the split is unconstrained.
  //
  // Note: Ranking problem with RMSE loss is not grouped.
  int effective_validation_set_group = -1;

// All the training dataset information from the point of view of the weak
// learner i.e. "classical" training dataset + gradient + weights.
struct CompleteTrainingDatasetForWeakLearner {

  // Shallow copy of the training dataset with the gradient as label.

  // Gradient data in "dataset".

  // Training weights.

  // Predictions of the model.

  // Number of trees used to compute "predictions".
  int predictions_from_num_trees = 0;

// Loads a dataset for a weak learner.

// Computes the loss best adapted to the problem.

void SetInitialPredictions(const std::vector<float>& initial_predictions,

// Sets the default hyper-parameters of the learner.
absl::Status SetDefaultHyperParameters(

// Returns a non owning vector of tree pointers from a vector of tree
// unique_ptr.




// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees_hparams_templates.cc =====


// This files defines the pre-configured hyper-parameter templates.




      auto field = config.mutable_parameters()->add_fields();

      auto field = config.mutable_parameters()->add_fields();
      auto field = config.mutable_parameters()->add_fields();
      auto field = config.mutable_parameters()->add_fields();
      auto field = config.mutable_parameters()->add_fields();
      auto field = config.mutable_parameters()->add_fields();



// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees_test.cc =====


// Gradient Boosted Trees algorithms tests.








// Shards a dataset. Returns the sharded path in the temp directory.

  // Down-sample the number of examples.


// Custom loss for mean squared error loss.
  auto initial_predictions =

  auto loss =
  auto gradient_and_hessian = [](const absl::Span<const float>& labels,


// Custom loss for binomial loss.
  auto initial_predictions =


  auto loss =

      // The loss function expects a 0/1 label.
  auto gradient_and_hessian =


// Custom Loss for equal to three-class multinomial loss.
  auto initial_predictions =
  auto loss =
      float sum_exp = 0;
      // Loss: - log(predict_proba[true_label])
  auto gradient_and_hessian =

      // Compute normalization term.
      float sum_exp = 0;
        float exp_val =
      // Update gradient.
        // Examples should be blocks

// Utility to set the configured and expected sorting strategy.
void SetSortingStrategy(Internal::SortingStrategy set,








  // Note: The most popular group contains ~32% of the dataset.


  // Ensure the intersection of the groups is empty.


















// Helper for the training and testing on two non-overlapping samples from the
// adult dataset.
class GradientBoostedTreesOnAdult : public utils::TrainAndTestTester {
  void SetUp() override {

    // By default, we expect all the learners to use pre-sortings.



  // Note: Accuracy is similar as RF (see :random_forest_test). However logloss
  // is significantly better (which is expected as, unlike RF,  GBT is
  // calibrated).




  // Note: Accuracy is similar as RF (see :random_forest_test). However logloss
  // is significantly better (which is expected as, unlike RF,  GBT is
  // calibrated).



// Train and test a model on the adult dataset.

  // Note: Accuracy is similar as RF (see :random_forest_test). However, logloss
  // is significantly better (which is expected as, unlike RF, GBT is
  // calibrated).



// Train and test a model on the adult dataset.

  // Note: Accuracy is similar as RF (see :random_forest_test). However logloss
  // is significantly better (which is expected as, unlike RF,  GBT is
  // calibrated).





  // Ensures that the tree is monotonic.






  // Show the tree structure.




  // Ensures that the tree is monotonic.






  // Show the tree structure.




  // Ensures that the tree is monotonic.






  // Show the tree structure.











  // Show the tree structure.


// Train and test a model on the adult dataset.

  // The expected accuracy is the same as for the built-in loss.

// Train and test a model on the adult dataset with focal loss, but with gamma
// equals zero, which equals to log loss.

  // Similar metrics as with log loss.

// Train and test a model on the adult dataset with focal loss, now with
// an effective gamma of 0.5.

  // Slightly better accuracy, but worse log loss; we are not
  // optimizing for log loss directly any more.

// Train and test a model on the adult dataset with focal loss, now with
// an effective gamma of 2.

  // Even slightly better accuracy (could be just noise, but illustrative),
  // log loss deviates even more

// Train and test a model on the adult dataset with focal loss, adding a
// non-default, 0.25 alpha parameter to the gamma of 2.0

  // Worse accuracy but smaller log loss due to low alpha

// Separate the examples used for the structure and the leaves of the model.
// Train a GBT with a validation dataset provided as a VerticalDataset.

// Train a GBT with a validation dataset provided as a path.





  // Top 3 variables.


class PerShardSamplingOnAdult : public ::testing::Test {
  void SetUp() override {
    // Load the datasets.

    // Configure model training.


// Training a model with the shard sampler algorithm, but with all the shards
// used for each tree.
  auto learner = BuildBaseLearner();

  // Shard the training dataset.



  // Evaluate the models.

  // Sharded model is "good".

// Model trained with the sharded algorithm and sampling.
  auto learner = BuildBaseLearner();

  // Shard the training dataset.

  // Model trained with the sharded algorithm and sampling.

  // Evaluate the models.


// Model trained with the sharded algorithm and sampling.
  auto learner = BuildBaseLearner();

  // Shard the training dataset.

  // Model trained with the sharded algorithm and sampling.

  // Evaluate the models.


// Train and test a model on the adult dataset using random categorical splits.

  // Note: Accuracy is similar as RF (see :random_forest_test). However logloss
  // is significantly better (which is expected as, unlike RF,  GBT is
  // calibrated).


// Train and test a model on the adult dataset with too much nodes for the
// QuickScorer serving algorithm.

  // Note: Accuracy is similar as RF (see :random_forest_test). However logloss
  // is significantly better (which is expected as, unlike RF,  GBT is
  // calibrated).


// Train and test a model on the adult dataset.

  // Note: Accuracy is similar as RF (see :random_forest_test). However logloss
  // is significantly better (which is expected as, unlike RF,  GBT is
  // calibrated).


// Train and test a model on the adult dataset.

  // Note: Accuracy is similar as RF (see :random_forest_test). However logloss
  // is significantly better (which is expected as, unlike RF,  GBT is
  // calibrated).

// Train and test a model on the adult dataset with Goss sampling.


// Train and test a model on the adult dataset with Goss sampling.


// Train and test a model on the adult dataset.


  // Note: Accuracy is similar as RF (see :random_forest_test). However logloss
  // is significantly better (which is expected as, unlike RF,  GBT is
  // calibrated).

// Train and test a model on the adult dataset.


  // Note: Accuracy is similar as RF (see :random_forest_test). However logloss
  // is significantly better (which is expected as, unlike RF,  GBT is
  // calibrated).




// Train and test a model on the adult dataset.

  // The expected accuracy is the same as for the built-in loss.



  // Note: Accuracy is similar as RF (see :random_forest_test). However logloss
  // is significantly better (which is expected as, unlike RF,  GBT is
  // calibrated).

// Train and test a model on the adult dataset.



// Train and test a model on the adult dataset with L2 regularization.


  // Note: Accuracy is similar as RF (see :random_forest_test). However logloss
  // is significantly better (which is expected as, unlike RF,  GBT is
  // calibrated).

// Multiclass version of the algorithm on the binary class adult dataset.


  // Note: As expected, the results are similar to the binary class
  // implementation.

// Multiclass version of the algorithm on the binary class adult dataset with L2
// regularization.


  // Note: As expected, the results are similar to the binary class
  // implementation.

// Train and test a model on the adult dataset for a maximum given duration.

  // Note: The "TrainAndEvaluateModel" function last a bit more because it is
  // also preparing the dataset and evaluating the final model.

  // Would take a very long time without a timeout inside of the tree building.


  // Note: The "TrainAndEvaluateModel" function last a bit more because it is
  // also preparing the dataset and evaluating the final model.

  // Would take a very long time without a timeout inside of the tree building.


  // Note: The "TrainAndEvaluateModel" function last a bit more because it is
  // also preparing the dataset and evaluating the final model.


  // Note: The "TrainAndEvaluateModel" function last a bit more because it is
  // also preparing the dataset and evaluating the final model.


// Train and test a model on the adult dataset.



  // Note: Dart seems to be unstable.

























// Helper for the training and testing on two non-overlapping samples from the
// Abalone dataset.
class GradientBoostedTreesOnAbalone : public utils::TrainAndTestTester {
  void SetUp() override {



  // We expect the same error as for the built-in loss.








  // Ensures that the tree is monotonic.




  // Show the tree structure.


class GradientBoostedTreesOnIris : public utils::TrainAndTestTester {
  void SetUp() override {

  // Note: R RandomForest has an OOB accuracy of 0.9467.

// Train and test a model on the adult dataset.


// Train and test a model on the adult dataset.

  // Note: R RandomForest has an OOB accuracy of 0.9467.

class GradientBoostedTreesOnDNA : public utils::TrainAndTestTester {
  void SetUp() override {

  // Note: R RandomForest has an OOB accuracy of 0.909.


  // Note: R RandomForest has an OOB accuracy of 0.909.



  // Defaults


  // Set

  // Extra test for the sampling.

  // Old / simple way to specify Random sampling.

  // Goss

  // Random sampling

  // No sampling.

  // Ransom sampling with compatibility mode







  auto tree = std::make_unique<decision_tree::DecisionTree>();








  // Same value as above.







  // Speed up this test.



  // Create a dataspec.


  // Retrieve the preconfigured parameters.


  // Train a model for a few seconds, interrupt its training, and resume it.


  // Configure a training that would take a long time.
  // Note: The quality of this model will be poor as it will overfit strongly
  // the training dataset.

  // Train for 10 seconds.
  auto interrupted_model = std::move(model_);


  // Check snapshots
  // The training will create 9 or 10 snapshots, but we guarantee that only 3 of
  // them will be there.

  // Resume the training with 100 extra trees.
  auto resumed_model = std::move(model_);

  // Ensures that the final model contains +100 iterations from the interrupted
  // model.

  // Check snapshots again.

  // Configure model training.


  // Train a model for a few seconds, interrupt its training, and resume it.


  // Configure a training that would take a long time.
  // Note: The quality of this model will be poor as it will overfit strongly
  // the training dataset.

  // Train for 5 seconds.
  auto interrupted_model = std::move(model_);



  // Resume the training with 100 extra trees.
  // There are 3 trees per iterations (because the dataset contains 3 classes).
  auto resumed_model = std::move(model_);

  // Ensures that the final model contains +100 iterations from the interrupted
  // model.

// Make sure that the confusion table of the training logs agrees with the
// confusion table computed a posteriori on the validation dataset.

  auto training_logs_confusion_table =


  auto evaluation_confusion_table =

// Make sure that the confusion table of the training logs agrees with the
// confusion table computed a posteriori on the validation dataset.
// This test specifically tests sharded datasets.

  auto training_logs_confusion_table =


  auto evaluation_confusion_table =

// TODO - b/311636358 Refactor the GBT tests to be more cohesive and
// comprehensive.
struct RegressionEnd2EndTestParams {







  auto model_1 = std::move(model_);

  auto model_2 = std::move(model_);


  auto model_1 = std::move(model_);

  auto model_2 = std::move(model_);


class GradientBoostedTreesOnSyntheticRanking
  void SetUp() override {






// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/gradient_boosted_trees_tuner_test.cc =====






class AutotunedGradientBoostedTreesOnAdult : public utils::TrainAndTestTester {
  void SetUp() override {











// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/plot_training.cc =====





// Create a multi-plot with a log of training. The plot contains:
// - The training and validation loss according to the number of trees.
// - The secondary metric (e.g. accuracy) according to the number of trees.
  // Labels of the plot.

  // Create the sub-plots:
  //   - row 0 -> number of trees vs loss.
  //   - row 1 and above -> number of trees vs the secondary metrics.


  // Set the axis labels.

  // Create curves.

  // Set the legend.

  // Set curves' drawing style.

  // Mean Average prediction i.e. mean(abs(model output)).

  // Set the axis labels.

  // Create curves.



    // Create the plot.

    // Set the axis labels.

    // Create curves.

    // Set the legend.
    // Set curves' drawing style.

    // Same for the validation curve.

  // Set the curves' x,y values.
    // X axis

    // Y axis





absl::Status PlotAndExportTrainingLogs(const proto::TrainingLogs& training_logs,



// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/plot_training.h =====


// Create plots about the training of a GBDT.




absl::Status PlotAndExportTrainingLogs(



// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/early_stopping/early_stopping.cc =====






absl::Status EarlyStopping::Update(

bool EarlyStopping::ShouldStop(const int current_iter_idx) {



absl::Status EarlyStopping::Load(const proto::EarlyStoppingSnapshot& p) {



// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/early_stopping/early_stopping.h =====






class EarlyStopping {

  // Updates the internal state of the early stopping controller.
  //
  // "set_trees_per_iterations" should be called before the first update.
  absl::Status Update(const float validation_loss,

  // Should the training stop?
  bool ShouldStop(const int current_iter_idx);

  // Best model.
  int best_num_trees() const { return best_num_trees_; }
  float best_loss() const { return best_loss_; }

  // Last model.
  float last_loss() const { return last_loss_; }

  // Number of trees trained at each iteration. "set_trees_per_iterations"
  // should be called before the first update.
  int trees_per_iterations() const { return trees_per_iterations_; }
  void set_trees_per_iterations(const int trees_per_iterations) {

  // Exports the internal representation of the class to a proto.

  // Restores the internal representation of the class from a proto.
  absl::Status Load(const proto::EarlyStoppingSnapshot& p);


  // Minimum validation loss over all the step of the model. Only valid if
  // "min_validation_loss_num_trees>=0".
  float best_loss_ = 0.f;
  float last_loss_ = 0.f;


  // Number of trees in the model with the validation loss
  // "minimum_validation_loss_value".
  int best_num_trees_ = -1;
  int last_num_trees_ = 0;

  int num_trees_look_ahead_;

  // Iteration that starts the computation of the validation loss.
  //
  // See GradientBoostedTreesTrainingConfig::early_stopping_start_iteration for
  // more details.
  int initial_iteration_;

  int trees_per_iterations_ = -1;




// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/early_stopping/early_stopping_test.cc =====






  int iter_idx = 0;






  // This is the lowest (i.e. best) loss.





  // Make some updates.

  // Check the internal representation of "a".

  // At this point "a" and "b" should be different.
  // Synchronize "a" and "b".

  // At this point "a" and "b" should be equal.

  // Makes the same updates to "a" and "b".


  // At this point "a" and "b" should still be equal.


// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_binary_focal.cc =====







absl::Status BinaryFocalLoss::Status() const {

// Calculate log(pt) for formula (5) from page 3 and other reusable stuff from
// https://arxiv.org/pdf/1708.02002.pdf
// Note: 'prediction' is in log odds space.
                                           float gamma, float alpha) {
  // Pt is probability of predicting the right label (1-p for negative labels).
  // Note: It is better to calculate log(pt) this way, to avoid NaNs when
  // pt is very close to zero.
  // Why is calculation of log_pt correct?
  // Let d denote "prediction' in log odds space.
  // If label = 0:
  //   log(pt) = log(1-p) = log(1-1/(1+exp(-d))) =
  //   = log([1 + exp(-d) - 1] / [1 + exp(-d)]) =
  //   = log(exp(-d)) - log(1 + exp(-d)) =
  //   = log(1/exp(d)) - log([exp(d) + 1] / exp(d)) =
  //   = -log(exp(d)) - log(exp(d) + 1) + log(exp(d)) =
  //   = - log(1+ exp(d))
  //   = label * d - log(1 + exp(d))         Q.E.D.
  //
  // If label = 1:
  //   log(pt) = log(p) = log(1/(1+exp(-d))) =
  //   = -log(1+exp(-d)) = -log([exp(d) + 1] / exp(d)) =
  //   = -[log(exp(d) + 1) - log(exp(d))] =
  //   = -log(exp(d) + 1) + d =
  //   = label * d - log(1+ exp(d))          Q.E.D.

// We have a separate function to only calculate what's necessary for gradient
// (and not for the hessian - to save time).
                                                 float prediction, float gamma,
                                                 float alpha) {
  // We calculate and store the two terms of the first derivative separately
  // to be reused in the hessian (when needed)

float CalculateFocalLossHessian(FocalLossGradientData gradient_data,
                                float gamma, float alpha) {
  // Derivative of term1 (see term1 in CalculateFocalLossGradient)
  // Derivative of term2 (see term2 in CalculateFocalLossGradient)



// Local help functions end.

template <typename T>
absl::Status BinaryFocalLoss::TemplatedUpdateGradients(
  // TODO: Implement thread_pool.





absl::Status BinaryFocalLoss::UpdateGradients(

absl::Status BinaryFocalLoss::UpdateGradients(

template <typename T>
void BinaryFocalLoss::TemplatedUpdateGradientsImp(



template <bool use_weights, typename T>
void BinaryFocalLoss::TemplatedLossImp(

template <typename T>


    struct PerThread {




    float loss = sum_loss / sum_weights;




// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_binary_focal.h =====


// Implementation of binary focal loss following
// https://arxiv.org/pdf/1708.02002.pdf.






// Local helper functions for calculating the (slightly more involved)
// focal loss gradients and hessians at a common place to avoid code duplication
struct FocalLossBasicData {
  float y;      // Label as from {-1, 1} set to follow paper's notation.
  float label;  // Label as from {0, 1} set.
  float pt;     // Probability of "being right".
  // Log of prob of being right. Calculated directly from log odds.
  float log_pt;
  float mispred;  // Probability of miss-prediction.
  float at;       // Sample weight (alpha_t, depends on ground truth label)

struct FocalLossGradientData {
  float gradient;
  float term1;  // The first derivative term of the first derivative
  float term2;  // The second derivative term of the first derivative

                                                 float prediction, float gamma,
                                                 float alpha);

float CalculateFocalLossHessian(FocalLossGradientData gradient_data,
                                float gamma, float alpha);

// Focal loss.
// Suited for binary classification, implementation based on log loss
// For implementation details, see paper: https://arxiv.org/pdf/1708.02002.pdf
// Note: uses the label set of {-1, 1} in formula derivation
class BinaryFocalLoss : public BinomialLogLikelihoodLoss {
  // For unit testing.


  absl::Status Status() const override;

  template <typename T>
  absl::Status TemplatedUpdateGradients(

  template <typename T>
      float alpha, std::vector<float>* gradient_data,

  absl::Status UpdateGradients(

  absl::Status UpdateGradients(


  template <typename T>

  template <bool use_weights, typename T>
                               float gamma, float alpha,



  // Focusing parameter, when 0.0, then falls back to log loss.
  float gamma_;
  // Class weighting parameter, positives with alpha,
  // negatives with (1-alpha) weight. Usually tuned together with gamma.
  float alpha_;




// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_binary_focal_test.cc =====






// Margin of error for numerical tests.

  // TODO Replace by a modern function when possible.

class BinaryFocalLossTest : public testing::TestWithParam<bool> {};


  // Initial predictions for binary focal loss are the same as for binomial log
  // loss.




  // Values validated with tensorflow focal loss implementation
  // (tfa.losses.sigmoid_focal_crossentropy).









  // The loss for every example is log(2)/8, with gamma=2, alpha=.5 and all
  // predictions 0.


  // The loss for every example is log(2)/8, with gamma=2, alpha=.5 and all
  // predictions 0.






// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_binomial.cc =====






absl::Status BinomialLogLikelihoodLoss::Status() const {

  // Return: log(y/(1-y)) with y the ratio of positive labels.


  // Return: log(y/(1-y)) with y the ratio of positive labels.

template <typename T>
void BinomialLogLikelihoodLoss::TemplatedUpdateGradientsImp(
  // Set the gradient to:
  //   label - 1/(1 + exp(-prediction))
  // where "label" is in {0,1} and prediction is the probability of
  // label=1.

template <typename T>
absl::Status BinomialLogLikelihoodLoss::TemplatedUpdateGradients(





absl::Status BinomialLogLikelihoodLoss::UpdateGradients(

absl::Status BinomialLogLikelihoodLoss::UpdateGradients(


template <bool use_weights, typename T>
void BinomialLogLikelihoodLoss::TemplatedLossImp(
    // The loss function expects a 0/1 label.
      // Loss:
      //   -2 * ( label * prediction - log(1+exp(prediction)))

template <typename T>
  int confusion_matrix_size =


    struct PerThread {




    float loss = sum_loss / total_example_weight;




// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_binomial.h =====







// Binomial log-likelihood loss.
// Suited for binary classification.
// See "AbstractLoss" for the method documentation.
class BinomialLogLikelihoodLoss : public AbstractLoss {
  // For unit testing.


  absl::Status Status() const override;


  // Returns the initial predictions on the dataset.
  //
  // `weights` may be empty, which is interpreted as unit weights.
  // Returns log(y/(1-y)) with y the weighted ratio of positive labels.


  template <typename T>
  absl::Status TemplatedUpdateGradients(

  template <typename T>

  absl::Status UpdateGradients(

  absl::Status UpdateGradients(


  template <typename T>

  template <bool use_weights, typename T>

  // Returns the loss of the given predictions.
  //
  // `weights` may be empty, which is interpreted as unit weights.

  // Returns the loss of the given predictions.
  //
  // `weights` may be empty, which is interpreted as unit weights.




// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_binomial_test.cc =====







// Margin of error for numerical tests.

  // TODO Replace PARSE_TEST_PROTO by a modern function when
  // possible.

// Returns a simple dataset with gradients in the second column.
  // TODO Replace PARSE_TEST_PROTO by a modern function when
  // possible.

class BinomialLogLikelihoodLossTest : public testing::TestWithParam<bool> {};

      auto init_pred,











// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_cross_entropy_ndcg.cc =====






absl::Status CrossEntropyNDCGLoss::Status() const {



absl::Status CrossEntropyNDCGLoss::UpdateGradients(
  // TODO: Implement thread_pool.



  // Reset gradient accumulators.

  // A vector of predictions for items in a group.
  // An auxiliary buffer of parameters used to form the ground-truth
  // distribution and compute the loss.


    // Skip groups with too few items.

    // Extract predictions.


    // Turn scores into a probability distribution with Softmax.
    float sum_exp = 0.0f;
    float log_sum_exp = max_pred + std::log(sum_exp + 1e-20f);
      float probability = std::exp(preds[idx] - log_sum_exp);

    // Approximate Newton's step.
    // First-order terms.
    float inv_denominator = 0;
      // Params is currently a \gamma but becomes the numerator of the
      // first-order approximation terms.

    float sum_l1 = 0.f;

      // Params will now store terms needed to compute second-order terms.
    // Second-order terms.
    float sum_l2 = 0.f;

      // Params will now store terms needed to compute third-order terms.

    // Third-order terms and the Hessian.


  float loss_value =


// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_cross_entropy_ndcg.h =====







// Cross Entropy Normalized Discounted Cumulative Gain (XE-NDCG) loss.
// Suited for ranking.
// See "AbstractLoss" for the method documentation.
class CrossEntropyNDCGLoss : public AbstractLoss {
  // For unit testing.


  absl::Status Status() const override;

  bool RequireGroupingAttribute() const override { return true; }




  absl::Status UpdateGradients(







// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_custom_binary_classification.cc =====






absl::Status CustomBinaryClassificationLoss::Status() const {


  auto labels_span = absl::MakeConstSpan(labels->values());
  auto weights_span = absl::MakeConstSpan(weights);

      float initial_prediction,


absl::Status CustomBinaryClassificationLoss::UpdateGradients(
  auto labels_span = absl::MakeConstSpan(labels);
  auto predictions_span = absl::MakeConstSpan(predictions);
  auto gradient_data = absl::MakeSpan(*(gradients->front().gradient));
  auto hessian_data = absl::MakeSpan(*(gradients->front().hessian));




  auto labels_span = absl::MakeConstSpan(labels);
  auto predictions_span = absl::MakeConstSpan(predictions);
  auto weights_span = absl::MakeConstSpan(weights);
      float loss_value,



// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_custom_binary_classification.h =====







// Functionals for computing a custom binary classification loss.
struct CustomBinaryClassificationLossFunctions {
  // Functional to return the initial predictions.
  // Functional to return the loss of the current predictions.
  // Functional to compute the gradient and the hessian of the current
  // predictions.

// Custom loss implementation suited for binary classification.
// The loss function is specified by the user through the constructor.
// See the tests for a sample loss.
// See `AbstractLoss` for the method documentation.
class CustomBinaryClassificationLoss : public AbstractLoss {
  // For unit testing.


  absl::Status Status() const override;




  absl::Status UpdateGradients(






// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_custom_binary_classification_test.cc =====







  // TODO Replace by a modern function when possible.

// Creates a very simple loss for testing binary classification.
// The initial prediction is the sum of weight*labels.
// The gradient is the (prediction + label).
// The hessian is ((prediction + label)**2)
// The loss is the sum of weight*(prediction + label)

    float loss =

  auto loss_shape = loss_imp.Shape();








  // There are no secondary metrics.






// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_custom_multi_classification.cc =====







absl::Status CustomMultiClassificationLoss::Status() const {


  auto labels_span = absl::MakeConstSpan(labels->values());
  auto weights_span = absl::MakeConstSpan(weights);



absl::Status CustomMultiClassificationLoss::UpdateGradients(
  auto labels_span = absl::MakeConstSpan(labels);
  auto predictions_span = absl::MakeConstSpan(predictions);




  auto labels_span = absl::MakeConstSpan(labels);
  auto predictions_span = absl::MakeConstSpan(predictions);
  auto weights_span = absl::MakeConstSpan(weights);
      float loss_value,



// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_custom_multi_classification.h =====







// Functionals for computing a custom multi-class classification loss.
struct CustomMultiClassificationLossFunctions {
  // Functional to compute the initial predictions, one per class.
  // Functional to return the loss of the current predictions.
  // Functional to compute the gradient and the hessian of the current
  // predictions. The function must one gradient per class.

// Custom loss implementation suited for multi-class classification.
// The loss function is specified by the user through the constructor.
// See the tests for a sample loss.
// See `AbstractLoss` for the method documentation.
class CustomMultiClassificationLoss : public AbstractLoss {
  // For unit testing.


  absl::Status Status() const override;




  absl::Status UpdateGradients(



  int dimension_;



// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_custom_multi_classification_test.cc =====







  // TODO Replace by a modern function when possible.

// Creates a very simple loss for testing 3-class classification.
// The initial prediction is the sum of weight*labels * class_idx.
// The gradient is the (prediction + label) * class_idx.
// The hessian is (gradient**2)
// The loss is the sum of weight*(prediction + label)
  // Number of classes in the dataset.

    auto weight_times_labels =
    float loss = 0.0;


  auto loss_shape = loss_imp.Shape();








  // There are no secondary metrics.






// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_custom_regression.cc =====






absl::Status CustomRegressionLoss::Status() const {


  auto labels_span = absl::MakeConstSpan(labels->values());
  auto weights_span = absl::MakeConstSpan(weights);
      float initial_prediction,


absl::Status CustomRegressionLoss::UpdateGradients(
  auto labels_span = absl::MakeConstSpan(labels);
  auto predictions_span = absl::MakeConstSpan(predictions);
  auto gradient_data = absl::MakeSpan(*(gradients->front().gradient));
  auto hessian_data = absl::MakeSpan(*(gradients->front().hessian));




  auto labels_span = absl::MakeConstSpan(labels);
  auto predictions_span = absl::MakeConstSpan(predictions);
  auto weights_span = absl::MakeConstSpan(weights);
      float loss_value,



// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_custom_regression.h =====







// Functionals for computing a custom regression loss.
struct CustomRegressionLossFunctions {
  // Functional to return the initial predictions.
  // Functional to return the loss of the current predictions.
  // Functional to compute the gradient and the hessian of the current
  // predictions.

// Custom loss implementation suited for univariate regression.
// The loss function is specified by the user through the constructor.
// See the tests for a sample loss.
// See `AbstractLoss` for the method documentation.
class CustomRegressionLoss : public AbstractLoss {
  // For unit testing.


  absl::Status Status() const override;




  absl::Status UpdateGradients(






// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_custom_regression_test.cc =====







  // TODO Replace by a modern function when possible.

// Creates a custom loss.
// The initial prediction is the sum of weight*labels.
// The gradient is the (prediction + label).
// The hessian is ((prediction + label)**2)
// The loss is the sum of weight*(prediction + label)

    float loss =








  // There are no secondary metrics.






// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_mean_average_error.cc =====






void UpdateGradientsSingleThread(const absl::Span<const float> labels,

  // We use "table" to avoid a branch in the following loop.
  // This optimization was found to improve the code speed. This should be
  // revisited as new compilers are likely to do this optimization
  // automatically one day.



absl::Status MeanAverageErrorLoss::Status() const {

  // The initial value is the weighted median value.


  float initial_prediction;
    struct Item {
      float label;
      float weight;




absl::Status MeanAverageErrorLoss::UpdateGradients(






// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_mean_average_error.h =====


// Implementation of the "mean average error" a.k.a. laplace error.
//
// loss = \sum abs(x_i - y_i); where x_i are the predictions and y_i the labels.
// gradient = (x_i>=y_i) ? +1 ; -1
// hessian = 1
// initial_predictions = median[y_i]
//
// Note: The hessian should be zero, but this is not allowed with a Newton's
// method (division by zero). Different libraries have different tricks e.g.
// R.gbm set the leaves to be the median of the gradient value or set the
// hessian to 1. We use this last option.





class MeanAverageErrorLoss : public AbstractLoss {
  // For unit testing.


  absl::Status Status() const override;




  absl::Status UpdateGradients(






// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_mean_average_error_test.cc =====








// Margin of error for numerical tests.

  // TODO Replace by a modern function when possible.




class MeanAverageErrorLossWeightAndThreadingTest
  void SetUp() override {

  void TearDown() override { thread_pool_.reset(); }

  // The thread pool is only set if "UseMultithreading=kYes".

class MeanAverageErrorLossWeightTest












    // MAE = \sum (abs(prediction_i - label_i) * weight_i) / \sum weight_i
    // RMSE = sqrt(\sum ((prediction_i - label_i)^2 * weight_i) / \sum weight_i)
    // MAE = \sum abs(prediction_i - label_i) / num_examples
    // RMSE = sqrt(\sum (prediction_i - label_i)^2 / num_examples)





// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_mean_square_error.cc =====






absl::Status MeanSquaredErrorLoss::Status() const {

  // Note: The initial value is the weighted mean of the labels.
  // Note: Null and negative weights are detected by the dataspec
  // computation.


absl::Status MeanSquaredErrorLoss::UpdateGradients(
  // TODO: Implement thread_pool.

  // Set the gradient to:
  //   label - prediction



  float loss_value;
  // The RMSE is also the loss.



// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_mean_square_error.h =====







// Mean squared Error loss.
// Suited for univariate regression.
// See "AbstractLoss" for the method documentation.
class MeanSquaredErrorLoss : public AbstractLoss {
  // For unit testing.


  absl::Status Status() const override;




  absl::Status UpdateGradients(






// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_mean_square_error_test.cc =====






// Margin of error for numerical tests.

  // TODO Replace by a modern function when possible.

class MeanSquareErrorLossTest








    // For regression, the only secondary metric is also RMSE.
    // For regression, the only secondary metric is also RMSE.


    //  For ranking, first secondary metric is RMSE, second secondary metric is
    //  NDCG@5.
    //  For ranking, first secondary metric is RMSE, second secondary metric is
    //  NDCG@5.





// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_multinomial.cc =====






absl::Status MultinomialLogLikelihoodLoss::Status() const {

  // YDF follows Friedman's paper "Greedy Function Approximation: A Gradient
  // Boosting Machine" (https://statweb.stanford.edu/~jhf/ftp/trebst.pdf),
  // setting the initial prediction to 0 for multi-class classification
  // (Algorithm 6).
  // TODO: Experiment with different initial predictions.


template <typename T>
absl::Status MultinomialLogLikelihoodLoss::TemplatedUpdateGradients(
  // TODO: Implement thread_pool.

  // Set the gradient to:
  //   label_i - pred_i
  // where "label_i" is in {0,1}.
    // Compute normalization term.
    float sum_exp = 0;
      float exp_val =
    // Update gradient.


absl::Status MultinomialLogLikelihoodLoss::UpdateGradients(

absl::Status MultinomialLogLikelihoodLoss::UpdateGradients(


template <bool weighted, typename T>
void MultinomialLogLikelihoodLoss::TemplatedLossImp(

    int predicted_class = -1;
    float predicted_class_exp_value = 0;
    float sum_exp = 0;
      // Loss:
      //   - log(predict_proba[true_label])
      // Loss:
      //   - log(predict_proba[true_label])

template <typename T>
  int confusion_matrix_size = dimension_ + 1;


    struct PerThread {








// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_multinomial.h =====







// Multinomial log likelihood loss.
// Suited for binary and multi-class classification.
// See "AbstractLoss" for the method documentation.
class MultinomialLogLikelihoodLoss : public AbstractLoss {
  // For unit testing.


  absl::Status Status() const override;


  // Returns the initial predictions on the dataset.
  //
  // `weights` may be empty, which is interpreted as unit weights.


  template <typename T>
  absl::Status TemplatedUpdateGradients(

  absl::Status UpdateGradients(

  absl::Status UpdateGradients(


  // Returns the loss of the given predictions.
  //
  // `weights` may be empty, which is interpreted as unit weights.
  template <typename T>

  template <bool use_weights, typename T>

  // Returns the loss of the given predictions.
  //
  // `weights` may be empty, which is interpreted as unit weights.
  // Returns the loss of the given predictions.
  //
  // `weights` may be empty, which is interpreted as unit weights.

  int dimension_;




// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_multinomial_test.cc =====







// Margin of error for numerical tests.

  // TODO Replace PARSE_TEST_PROTO by a modern function when
  // possible.

// Returns a simple dataset with gradients in the second column.
  // TODO Replace PARSE_TEST_PROTO by a modern function when
  // possible.

struct TestParameters {
  bool weighted;
  bool threaded;

class MultinomialLogLikelihoodLossTest



  // Initial predictions for multinomial loss are always 0.







  auto label_column = dataset.data_spec().columns(1);


  auto label_column = dataset.data_spec().columns(1);



// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_ndcg.cc =====






absl::Status NDCGLoss::Status() const {



absl::Status NDCGLoss::UpdateGradients(
  // TODO: Implement thread_pool.




  // Reset gradient accumulators.

  // "pred_and_in_ground_idx[j].first" is the prediction for the example
  // "group[pred_and_in_ground_idx[j].second].example_idx".
    // Extract predictions.

    // NDCG normalization term.
    // Note: At this point, "pred_and_in_ground_idx" is sorted by relevance
    // i.e. ground truth.
    float utility_norm_factor = 1.;
      float max_ndcg = 0;

    // Sort by decreasing predicted value.
    // Note: We shuffle the predictions so that the expected gradient value is
    // aligned with the metric value with ties taken into account (which is
    // too expensive to do here).

    auto it = pred_and_in_ground_idx.begin();
      auto next_it = std::next(it);


    // Compute the "force" that each item apply on each other items.

      // Accumulator for the gradient and second order derivative of the
      // example
      // "group[pred_and_in_ground_idx[item_1_idx].second].example_idx".


        // Skip examples with the same relevance value.

        // "delta_utility" corresponds to "Z_{i,j}" in the paper.
        float delta_utility = 0;

        // "sign" correspond to the sign in front of the lambda_{i,j} terms
        // in the equation defining lambda_i, in section 7 of "From RankNet
        // to LambdaRank to LambdaMART: An Overview".
        // The "sign" is also used to reverse the {i,j} or {j,i} in the
        // "lambda" term i.e. "s_i" and "s_j" in the sigmoid.

        // sign = in_ground_idx_1 < in_ground_idx_2 ? +1.f : -1.f;
        // signed_lambda_loss = sign * lambda_loss;


        // "sigmoid" corresponds to "rho_{i,j}" in the paper.

        // "unit_grad" corresponds to "lambda_{i,j}" in the paper.
        // Note: We want to minimize the loss function i.e. go in opposite
        // side of the gradient.








// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_ndcg.h =====







// Normalized Discounted Cumulative Gain loss.
// Suited for ranking.
// See "AbstractLoss" for the method documentation.
class NDCGLoss : public AbstractLoss {
  // For unit testing.


  absl::Status Status() const override;

  bool RequireGroupingAttribute() const override { return true; }




  absl::Status UpdateGradients(




// LAMBDA_MART_NDCG5 also creates this loss.



// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_ndcg_test.cc =====






// Margin of error for numerical tests. Note that this is by a factor of 10
// larger than for the other loss functions.


  // TODO Replace by a modern function when possible.

class NDCGLossTest : public testing::TestWithParam<bool> {};




  // Initial predictions for NDCG loss are always 0.





  // Explanation:
  // - Element 0 is pushed down by element 2 (and in reverse).
  // - Element 1 is pushed down by element 3 (and in reverse).






  // Explanation:
  // - Element 0 is pushed down by element 2 (and in reverse).
  // - Element 1 is pushed down by element 3 (and in reverse).


  // Perfectly wrong predictions.
    // R> 0.7238181 = (sum((2^c(1,3)-1)/log2(seq(2)+1)) /
    // sum((2^c(3,1)-1)/log2(seq(2)+1)) +  3(sum((2^c(2,4)-1)/log2(seq(2)+1)) /
    // sum((2^c(4,2)-1)/log2(seq(2)+1))) )/4
    // R> 0.7238181 = (sum((2^c(1,3)-1)/log2(seq(2)+1)) /
    // sum((2^c(3,1)-1)/log2(seq(2)+1)) +  sum((2^c(2,4)-1)/log2(seq(2)+1)) /
    // sum((2^c(4,2)-1)/log2(seq(2)+1)) )/2


  // Perfectly wrong predictions.
    // R> 0.7238181 = (sum((2^c(1,3)-1)/log2(seq(2)+1)) /
    // sum((2^c(3,1)-1)/log2(seq(2)+1)) +  3(sum((2^c(2,4)-1)/log2(seq(2)+1)) /
    // sum((2^c(4,2)-1)/log2(seq(2)+1))) )/4
    // R> 0.7238181 = (sum((2^c(1,3)-1)/log2(seq(2)+1)) /
    // sum((2^c(3,1)-1)/log2(seq(2)+1)) +  sum((2^c(2,4)-1)/log2(seq(2)+1)) /
    // sum((2^c(4,2)-1)/log2(seq(2)+1)) )/2


  // Perfect predictions.


  // Perfect predictions again (the ranking across groups has no effect).






// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_poisson.cc =====






  // The initial value is the logarithm of the weighted mean of the labels.
  // Note: Null and negative weights are detected by the dataspec
  // computation.
  // The initial prediction is the expected alpha coefficient of the poisson
  // distribution i.e. the log of the label mean.


absl::Status PoissonLoss::Status() const {

absl::Status PoissonLoss::UpdateGradients(


void PoissonLoss::UpdateGradientsImp(const absl::Span<const float> labels,
  // loss = exp(prediction) - label * prediction
  // -gradient = label - exp(prediction)
  // hessian = exp(prediction)



    struct PerThread {

          auto &block = per_threads[block_idx];


  auto float_poisson_loss = static_cast<float>(sum_loss / total_example_weight);
  auto float_rmse = static_cast<float>(sum_square_error / total_example_weight);

template <bool use_weights>
void PoissonLoss::LossImp(const absl::Span<const float> labels,
    // The loss is the log-likelihood of the poisson distribution without the
    // term "-\sum log(factorial label)" (since this term is independent of the
    // predictions).
    //
    // loss = exp(prediction) - label * prediction
    //
    // TODO: Figure what a 2x factor was added.


// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_poisson.h =====






// This class implements the Poisson log loss. This loss is suitable for
// regression problem where the label follows a Poisson distribution.
class PoissonLoss : public AbstractLoss {
  // For unit testing.


  absl::Status Status() const override;




  absl::Status UpdateGradients(




  template <bool use_weights>



// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_imp_poisson_test.cc =====







// Margin of error for numerical tests.

  // TODO Replace by a modern function when possible.

class PoissonLossTest : public testing::TestWithParam<std::tuple<bool, bool>> {





    float log_mean = std::log((2. + 8. + 18. + 32.) / 20.);
    float log_mean = std::log((1.f + 2.f + 3.f + 4.f) / 4.f);



  float log_mean = std::log((1.f + 2.f + 3.f + 4.f) / 4.f);




  // Initial predictions are log(2.5) (unweighted) and log(3) (weighted).






  // Initial predictions are log(2.5) (unweighted) and log(3) (weighted).

  // Initial predictions are log(2.5) (unweighted) and log(3) (weighted).





    // For classification, the only secondary metric is also RMSE.
    // The only secondary metric is RMSE.





// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_interface.cc =====







absl::Status AbstractLoss::UpdateGradients(







absl::Status RankingGroupsIndices::InitializeFromTmpGroups(
  // Sort the group items by decreasing ground truth relevance.



  // Sort the group by example index to improve the data locality.

absl::Status RankingGroupsIndices::Initialize(
  // Fill index.
    // Get the value of the group.

absl::Status RankingGroupsIndices::Initialize(
    int group_col_idx) {
  // Access to raw label and group values.
  // TODO: Update.


  // Fill index.
    // Get the value of the group.










void RankingGroupsIndices::ExtractPredAndLabelRelevance(


// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_interface.h =====


// Losses for the GBT algorithm.
//
// Losses are implemented by extending the "AbstractLoss" class, and registering
// it the "CreateLoss" function.
//





struct LossResults {
  float loss;

// One dimension of gradient and hessian values.
struct GradientData {
  // Values of the gradient. "values[i]" is the gradient of the i-th example.
  // The data is NOT owned by this pointer. In practice, this field is only
  // initialized by "CreateGradientDataset" and points to the data owned by the
  // "sub_train_dataset" VerticalDataset.

  // Second order derivative of the loss according to the prediction.
  //
  // Used to set the leaf values with a Newtonian step. Also used to find the
  // best split if the "use_hessian_gain" hyper-parameter is True.

  // Column containing the gradient in the virtual dataset.

  // Column containing the hessian in the virtual dataset.

  // Name of the column containing the gradient data in the virtual training
  // dataset. The virtual training dataset is a shallow copy of the training
  // dataset, with extra columns for the gradients.

  // Training configuration for the learning of gradient.

struct UnitGradientDataRef {
  // Gradient and hessian for each example.

// Gradient/hessian for each output dimension e.g. n for n-classes
// classification.

// Shapes of the loss's outputs.
struct LossShape {
  // Number of dimensions of the gradient.
  int gradient_dim;

  // Number of dimensions of the predictions.
  int prediction_dim;

// Index of example groups optimized for query. Used for ranking.
class RankingGroupsIndices {
  // An "Item" is the unit object that is being ranked. For example, a document
  // is an item in a query/document ranking problem.
  struct Item {
    // Ground truth relevance.
    float relevance;
    // Index of the example.

  // A "group" of examples is a set of examples that share the same "group
  // value" e.g. the same query in a query/document ranking problem.
  struct Group {
    // Value of the group column.
    // Items in the group. Sorted in decreasing order of relevance.

  // Constructs the index. No other function should be called before
  // "Initialize".
  absl::Status Initialize(const dataset::VerticalDataset& dataset,
                          int label_col_idx, int group_col_idx);

  absl::Status Initialize(absl::Span<const float> labels,



  absl::Status InitializeFromTmpGroups(


  // "groups[i]" is the list of relevance+example_idx of examples with group
  // column equal to "i". "Items" are sorted in decreasing order of relevance.

  // TODO: Use a banking system for "groups_" to reduce memory usage and
  // improve cache locality.

  // Total number of items.

// Loss to optimize during the training of a GBT.
//
// The life of a loss object is as follows:
//   1. The loss is created.
//   2. Gradient, hessian and predictions buffer are created using "Shape()".
//   3. "InitialPredictions" is called to get the initial prediction of the
//      model.
//   4. The gradient of the model is updated using "UpdateGradients".
//   5. A new tree is trained to predict each gradient dimension. Tree nodes are
//      set using "SetLeafFunctor".
//   6. The prediction buffer is updated using "UpdatePredictions".
//   7. Optionally (for logging or early stopping), the "Loss" is computed.
//   8. The training stops or goes back to step 4.
//
class AbstractLoss {


  // Check that the loss is valid. To be called after the constructor.

  // Shape / number of dimensions of the gradient, prediction and hessian
  // buffers required by the loss.

  // Initial prediction of the model before any tree is trained. Sometime called
  // the "bias".

  // Initial predictions from a pre-aggregated label statistics.

  // Returns true iff. the loss needs for the examples to be grouped i.e.
  // "ranking_index" will be set in "UpdateGradients" and "Loss". For example,
  // grouping can be used in ranking.

  // The "UpdateGradients" methods compute the gradient of the loss with respect
  // to the model output. Different version of "UpdateGradients" are implemented
  // for different representation of the label.
  //
  // Currently:
  // UpdateGradients on float should be implemented for numerical labels.
  // UpdateGradients on int32_t should be implemented for categorical labels.
  // UpdateGradients on int16_t should be implemented for categorical labels
  // (only used for distributed training).

  // Updates the gradient with label stored in a vector<float>.

  // Updates the gradient with label stored in a vector<int32_t>.

  // Updates the gradient with label stored in a vector<int16_t>.

  // Updates the gradient with label stored in a VerticalDataset.
  // This method calls the UpdateGradients defined above depending on the type
  // of the label column in the VerticalDataset (currently, only support float
  // (Numerical) and int32 (Categorical)).
  absl::Status UpdateGradients(

  // Gets the name of the metrics returned in "secondary_metric" of the "Loss"
  // method.

  // The "Loss" methods compute the loss(es) for the currently accumulated
  // predictions. Like for "UpdateGradients", different version of "Loss" are
  // implemented for different representation of the label.
  //
  // See the instructions of "UpdateGradients" to see which version of "Loss"
  // should be implemented.
  //
  // The "Loss" method exports the loss value to the "loss_value" output
  // argument. The "Loss" method should be called with "secondary_metric"
  // containing as many items as the loss secondary metrics (as defined by
  // SecondaryMetricNames()). The "Loss" method populates "secondary_metric"
  // accordingly.









// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_library.cc =====










  auto loss_key = proto::Loss_Name(loss);


// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_library.h =====







// Creates a training loss.



// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_utils.cc =====






// Set the value of a leaf node.
template <bool weighted>
absl::Status SetLeafValueWithNewtonRaphsonStep(










  // Save the label statistics used to train the child nodes.

  float value = gbt_config.shrinkage() * numerator / denominator;
  // TODO - b/311636358: Move this information to the AbstractLoss class.

template absl::Status SetLeafValueWithNewtonRaphsonStep<true>(

template absl::Status SetLeafValueWithNewtonRaphsonStep<false>(


absl::Status SetLeafValueWithNewtonRaphsonStep(





  float value = gbt_config_.shrinkage() * numerator / denominator;



void UpdatePredictionWithSingleUnivariateTree(

void UpdatePredictionWithMultipleUnivariateTrees(

absl::Status UpdatePredictions(


// ===== FILE: yggdrasil_decision_forests/learner/gradient_boosted_trees/loss/loss_utils.h =====







// Maximum number of items in a ranking group (e.g. maximum number of documents
// for a query). While possible, it is very unlikely that a user would exceed
// this value. The most likely scenario would be a
// configuration/dataset-preparation error. Since the running time is quadratic
// with the number of documents in a group, increasing this value further might
// allow very slow configurations.
//
// Since there exist a few valid use cases for large ranking groups, violating
// this maximum only triggers a stern warning.

// Index of the secondary metrics according to the type of loss.

// Minimum length of the hessian (i.e. denominator) in the Newton step
// optimization.

// Ensures that the value is finite i.e. not NaN and not infinite.
// This is a no-op in release mode.
template <typename T>
void DCheckIsFinite(T v) {

// Set a leaf's value using one step of the Newtonâ€“Raphson method by using
// pre-computed and PRE-AGGREGATED gradient and hessians. The label statistics
// should contain gradients + hessian values.
absl::Status SetLeafValueWithNewtonRaphsonStep(

// Creates a function to set leaf's values using one step of the Newtonâ€“Raphson
// method by using pre-computed (but not pre-aggregated) gradient and hessians.

template <bool weighted>
absl::Status SetLeafValueWithNewtonRaphsonStep(

absl::Status UpdatePredictions(



// ===== FILE: yggdrasil_decision_forests/learner/hyperparameters_optimizer/hyperparameters_optimizer.cc =====








// Set the default values of the training configuration.
absl::Status SetTrainConfigDefaultValues(

  // The training duration of each individual trial should be less than the
  // overall tuning.




absl::Status HyperParameterOptimizerLearner::SetHyperParametersImpl(

    // The base learner is not set. This is possible during the automated
    // documentation generation.

  auto base_learner_config = spe_config.base_learner();

  // Apply the hyperparameters to the base learner.
  // Copy-back the hyper-parameter of the base learner into the base learner
  // configuration in the hyperparameter optimizer configuration.

  // Copy-back some of the generic hyperparameters into the optimizer
  // configuration.

  // The maximum training duration applies both to the optimizer and the base
  // learner.

  // Returns the hyper-parameters of the base learner.


  auto spe_config_with_pb_definition = spe_config;

      auto base_learner,





    // Export the dataset to file and run the training on file.


  // The effective configuration is the user configuration + the default value +
  // the automatic configuration (if enabled) + the copy of the non-specified
  // training configuration field from the learner to the sub-learner (e.g. copy
  // of the label name).

  // Initialize the learner with the base hyperparameters.

  // Hyperparameters of the base learner.TrainWithStatus().

  // Build the effective space to optimize.

  // Select the best hyperparameters.

  // TODO: Record the logs.

    // Train a model on the entire train dataset using the best hyperparameters.
        auto mdl, base_learner->TrainWithStatus(train_dataset, valid_dataset));

absl::Status HyperParameterOptimizerLearner::GetEffectiveConfiguration(
  // Apply the default values.

  // Apply the default values.

  // Solve the symbols in the configuration.


    // Load the dataset in memory and run the in-memory training.


  // The effective configuration is the user configuration + the default value +
  // the automatic configuration (if enabled) + the copy of the non-specified
  // training configuration field from the learner to the sub-learner (e.g. copy
  // of the label name).

  // Initialize the remote workers.

  // Initialize the learner with the base hyperparameters.

  // Hyperparameters of the base learner.TrainWithStatus().

  // Build the effective space to optimize.

  // Select the best hyperparameters.

  // TODO: Record the logs.

    // Train a model on the entire train dataset using the best hyperparameters.









  // Configure the working directory.


  // Initialize the distribute manager.
  auto distribute_config = deployment_.distribute();

  // Number of model training request executed in parallel on each worker.






  // Initialize the hyperparameter logs.

  // Instantiate the optimizer.

  // The "async_evaluator" evaluates candidates in parallel using
  // multi-threading.
  struct Output {



  // Number of candidate being evaluated.
  int pending_evaluation = 0;

  // Number of evaluated and processed candidates.
  int round_idx = 0;

  // True iff. the optimizer is done generating new candidates.
  bool exploration_is_done = false;



    // Generate as many candidates as possible.
        // The optimization can stop.
        // Wait for some evaluation to finish.
        // Start evaluating this new candidate.

    auto maybe_output = async_evaluator.GetResult();
      // Stop the optimization.


    // Record the hyperparameter + evaluation.







  // Initializer the hyperparameter logs.

  // Instantiate the optimizer.

  // Mapping between request_id and hyperparameter of the currently running
  // evaluations.

  // Number of evaluated and processed candidates.
  int round_idx = 0;

  // Id of the next request.
  int next_request_id = 0;

  // True iff. the optimizer is done generating new candidates.
  bool exploration_is_done = false;


  // Path to the best model so far.


    // Generate as many candidates as possible.
        // The optimization can stop.
        // Wait for some evaluation to finish.
        // Start evaluating this new candidate.







        auto evaluator_result,


    auto it_hyperparameter =


    // Record the hyperparameter + evaluation.





    // If the best model is needed, load it.




      // Simple training + get the model self evaluation.


  // Extract the metric to optimize from the evaluation.


  // Slip the metric value into a score.











// ===== FILE: yggdrasil_decision_forests/learner/hyperparameters_optimizer/hyperparameters_optimizer.h =====


// Find the hyper-parameters that maximize/minimize a specific metric of a
// sub-learner. For example, find the hyper-parameters of the
// GradientBoostedTrees learner that maximize the accuracy of its model on a
// given dataset.





class HyperParameterOptimizerLearner : public AbstractLearner {

  // Unique identifier of the learning algorithm.



  // Sets the hyper-parameters of the learning algorithm from "generic hparams".
  absl::Status SetHyperParametersImpl(

  // Gets the available generic hparams.

  // Gets a description of what the learning algorithm can do.

  // Trains from file (needed for distributed training) from a dataset in
  // memory.

  // Aggregates the user inputs and automated logic output and returns the
  // effectively training configuration effectively used for training.
  absl::Status GetEffectiveConfiguration(

  // Assembles the effective search space.

  // Instantiates a base learner.

  // Searches for the best hyperparameter in process from a dataset loaded in
  // memory. The dataset object is shared among the trials.

  // Searches for the best hyperparameter using distributed training from a disk
  // dataset.

  // If true, the metric needs to be maximized. If false, the metric needs to be
  // minimized.

  // Evaluates the quality of a candidate locally i.e. train and evaluate the
  // model locally.

  // Creates an initialized distribute manager with "GENERIC_WORKER" workers.

  // Trains a model remotely.

  // Extracts the score from an evaluation. For scores, larger is always better.
  // Scores can be negative.



// Gets the default metric to evaluate in the list:
// loss > auc > accuracy > rmse ndcg > qini. Fails if none of those metrics
// are defined.




// ===== FILE: yggdrasil_decision_forests/learner/hyperparameters_optimizer/hyperparameters_optimizer_test.cc =====





class OnAdult : public utils::TrainAndTestTester {
  void SetUp() override {


  void SetTrainConfig(const absl::string_view optimizer_key = "RANDOM",











  void SetLocalTraining() {

  void SetDistributedTraining(const int num_workers = 2) {
    // Note: This test effectively using local multi-threaded training. However,
    // because we use the "distribute" API, this would behave the same way as
    // with distributed training.


















// ===== FILE: yggdrasil_decision_forests/learner/hyperparameters_optimizer/optimizer_interface.h =====


// Interface for the optimizers.
//
// Usage example of an optimizer:
//
// AbstractOptimizer* optimizer = ...;
// while(true) {
//   GenericHyperParameters candidate;
//   auto status = optimizer->NextCandidate(&candidate);
//   if(status==kExplorationIsDone){
//     // No more parameters to evaluate.
//     break;
//   } else if(status==kWaitForEvaluation){
//     // The optimizer expected at least one evaluation result before
//     generating a new candidate. In this example, the candidates are evaluated
//     one-by-one sequentially. At this point in the code, there are no pending
//     evaluation running, so this message is not possible.
//     LOG(FATAL) << "Should not append. As no evaluation pending.";
//   }
//   double evaluation = Evaluate(candidate);
//   optimizer->ConsumeEvaluation(candidate,evaluation);
//   }
// }
// GenericHyperParameters best_params;
// double best_score;
// tie(best_params, best_score) = optimizer.BestParameters();
//
// The goal is always to MAXIMIZE the score.
//
// An optimizer is not (unless specified otherwise in specific implementations)
// thread safe.





// Status result of "AbstractOptimizer::NextCandidate()" method.
  // The exploration is done. No new candidate will be generated and no new
  // evaluation is expected.
  // The optimizer waits for existing evaluation results before proposing a new
  // candidate or before ending the exploration. Only possible if at least
  // one evaluation result is pending.
  // A new candidate was generated in "candidate".

class OptimizerInterface {
  // Arguments
  //   config: The configuration of the optimizer (e.g. random optimizer).
  //   space: The space of hyper-parameter to optimize. For example, list the
  //      possible values of parameters.
  //   space_spec: The specification of the space to optimize. For example, list
  //     the type and default values of parameters.

  // Queries a new candidate hyperparameter set. "candidate" is only populated
  // if the returned value is "kNewCandidateAvailable".

  // Consumes a result previously generated by the last "NextCandidate" call.
  // A Nan value indicates that the evaluation failed i.e. the hyper-parameter
  // is not valid.

  // Returns the best parameters found so far. Can be called at any moment.

  // Total expected number of candidates to evaluate before the exploration is
  // done. This value is non-contractual and can change.





// ===== FILE: yggdrasil_decision_forests/learner/hyperparameters_optimizer/optimizers/random.cc =====








int RandomOptimizer::NumExpectedRounds() { return config_.num_trials(); }

absl::Status RandomOptimizer::BuildRandomSet(
  // Create the field.

  // Select a random value.

  // Call the children matching this parent value.
    bool match_child = false;




  int tries_left = num_tries_per_candidates_;






absl::Status RandomOptimizer::ConsumeEvaluation(
    // Unfeasible trial.


absl::Status UpdateWeights(model::proto::HyperParameterSpace* space) {


    // Allocate the weights.

  // Recursively update the weight of the children, and collect the number of
  // combinations in each children.

  // Number of combination for "field".

    // Number of combinations for this specific field value.

    // Count the number of combination in the children.
      // Check if the child is compatible with this specific value.
      bool match_child = false;





// ===== FILE: yggdrasil_decision_forests/learner/hyperparameters_optimizer/optimizers/random.h =====






// Randomly test a given number of hyper-parameters.
class RandomOptimizer : public OptimizerInterface {
  // Unique identifier of the optimizer algorithm.



  absl::Status ConsumeEvaluation(


  int NumExpectedRounds() override;

  // Build recursively a set of random hyper-parameter values.
  absl::Status BuildRandomSet(

  // Configuration of the optimizer.

  // Search space.

  // Best hyper-parameter found so far.

  // Score of the best hyper-parameter found so far.

  // Random generator.

  // Number of generated candidates without yet an evaluation.
  int pending_evaluations_ = 0;

  // Number of generated candidates with an evaluation.
  int observed_evaluations_ = 0;

  // Set of already generated candidates (to avoid duplicates).
  // Note: We use the string representation of the proto as a unique identifier.

  // Number of tries to generate a unique new candidate. If no new candidate
  // could be generated, the optimizer considers that no new candidates are
  // available.

  absl::Status constructor_status_;



// Computes the "weight" (set in the "weight" field) of each field in the
// hyper-parameter space. If "weight" is not already specified, all the
// hyper-parameter combination have the same probability of sampling. If
// "weight" is specified, it is applied as a coefficient factor over the uniform
// sampling.
absl::Status UpdateWeights(model::proto::HyperParameterSpace* space);





// ===== FILE: yggdrasil_decision_forests/learner/hyperparameters_optimizer/optimizers/random_test.cc =====









  int trial_idx = 0;
    auto status = optimizer.NextCandidate(&candidate).value();
      // No more parameters to evaluate.









    int sample = internal::Sample(weights, &random).value();

    int sample_2 = internal::Sample(weights, &random).value();


// ===== FILE: yggdrasil_decision_forests/learner/isolation_forest/isolation_forest.cc =====








// Assembles and checks the configuration.









// Check if this feature can be split.
template <typename T>
  float first_example = value_container->IsNa(selected_examples[0])
    float current_example = value_container->IsNa(example_idx)

// Compute the indices of the features that can be split, i.e. where not all
// values are equal. The indices of the nontrivial features are stored, by
// column type, in `nontrivial_features`. The total number of nontrivial
// features is returned.
    bool can_split;


  // TODO: Consider getting rid of this heap allocation

  // Sample a non-trivial feature type uniformly among the features. Note that
  // the number of column types is a small constant, so this algorithm is
  // reasonable.
  // For oblique numerical splits, this will not be used.
  int selected_feature_idx;

absl::Status SetRandomSplitNumericalSparseOblique(
  // An oblique split can be invalid even if all the features involved in the
  // split are nontrivial, if the weights are chosen so that features cancel
  // each other out. This is unlikely to happen. This function sets a
  // high number of trials and fails if it consistently fails to find a split
  // (which is likely indicative of an issue with the splitter).


    // Pre-compute the result of the current_projection.

    // Find minimum and maximum value.
    int num_valid_examples = 0;
    float min_value = std::numeric_limits<float>::infinity();
    float max_value = -std::numeric_limits<float>::infinity();
      // Invalid split, try again.

    // Randomly select a threshold in (min_value, max_value).

    // Count the number of positive examples.


    // Set split.

absl::Status SetRandomSplitNumericalAxisAligned(


  // Check if this feature can be split.
  float min_value = std::numeric_limits<float>::infinity();
  float max_value = -std::numeric_limits<float>::infinity();
    auto value = values[example_idx];
  // `feature_idx` must be a nontrivial feature.
  // Randomly select a threshold in (min_value, max_value).

  // Count the number of positive examples.
    auto value = values[example_idx];


  // Set split.

absl::Status FindSplitBoolean(

  // Positive values go to the positive branch, negative go to the negative
  // branch, NA goes to the majority side.


  // Set split.

absl::Status FindSplitCategorical(


  // if num_unique_feature_values is very large (likely because of a user
  // feeding pre-integerized values), the following might be memory-hungry.
  int num_active_feature_values = 0;
    auto value = values[example_idx];
  int ensure_chosen_idx = absl::Uniform(*rnd, 0, num_active_feature_values);
  int ensure_not_chosen_idx =

  int selected_values_idx = 0;
  int num_pos_examples = 0;
  bool na_is_chosen = false;
  // Flip a fair coin for every value in the selected examples.
      bool choose;
      // Randomly pick from the non-observed values for a more balanced split.

  // Set split.

// Grows recursively a node.
absl::Status GrowNode(


  // Set node value

  // Stop growth

  // Look for a split

    // No split found

  // Turn the node into a non-leaf node

  // Branch examples to children
      auto example_split,

  // Split children

// Grows and return a tree.
  auto tree = std::make_unique<decision_tree::DecisionTree>();

  auto selected_examples_rb =


int DefaultMaximumDepth(UnsignedExampleIdx num_examples_per_trees) {

    // If the number of examples is not too large, use Floyd's algorithm for
    // sampling `num_examples_to_sample` examples from range [0, num_examples).
    // https://doi.org/10.1145/30401.315746


      auto subsample_count = if_config.subsample_count();



absl::Status IsolationForestLearner::SetHyperParametersImpl(

  // Decision tree specific hyper-parameters.





  // Remove not yet implemented hyperparameters
  // TODO: b/345425508 - Implement more hyperparameters for isolation forests.











absl::Status FinalizeModel(IsolationForestModel* mdl) {
  // Cache the structural variable importance in the model data.



  auto model = std::make_unique<IsolationForestModel>();



  absl::Status global_status;
        auto selected_examples = internal::SampleExamples(
        auto tree_or =


// ===== FILE: yggdrasil_decision_forests/learner/isolation_forest/isolation_forest.h =====


// Isolation Forest learner.




class IsolationForestLearner : public AbstractLearner {

  // TODO: Add all hyper-parameters.


  absl::Status SetHyperParametersImpl(





struct Configuration {
  // "if_config" is a non-owning pointer to a sub-component of
  // "training_config".

// Gets the number of examples used to grow each tree.

// Sample examples to grow a tree.

// Default maximum depth hyper-parameter according to the number of examples
// used to grow each tree.
int DefaultMaximumDepth(UnsignedExampleIdx num_examples_per_trees);

// Finds a split (i.e. condition) for a node.
//
// A valid split always branches on training examples in each branch. If no
// valid split can be generated, "FindSplit" returns false and not split is set.
// If a valid split is sampled, the condition of "node" is set and the function
// returns true.
//
// TODO: Add support for more feature types.

// Sample an oblique split on the features in `nontrivial_features`.
//
// This function implements the oblique splits as described in
// "Sparse Projection Oblique Random Forests" 2020 JMLR paper by Tomita et al
// (https://www.jmlr.org/papers/volume21/18-664/18-664.pdf), adapted to
// isolation forests. In particular, this includes splits as performed for
// extended isolation forests.
//
// There is a small probability that the sampled split is not valid. The
// function will retry to sample a valid split many times before failing,
// reducing the probability to a negligible size.
absl::Status SetRandomSplitNumericalSparseOblique(

// Create an axis-aligned split on nontrivial numerical feature `feature_idx`.
//
// This function implements the original isolation forest algorithm: Only splits
// of the form "X >= threshold" are generated. The threshold is uniformly
// sampled between the minimum and maximum values observed in the training
// examples reaching this node.
absl::Status SetRandomSplitNumericalAxisAligned(
    int feature_idx, const Configuration& config,

// Create a split on nontrivial boolean feature `feature_idx`.
absl::Status FindSplitBoolean(
    int feature_idx, const Configuration& config,

// Create a split on nontrivial categorical feature `feature_idx`.
absl::Status FindSplitCategorical(
    int feature_idx, const Configuration& config,



// ===== FILE: yggdrasil_decision_forests/learner/isolation_forest/isolation_forest_test.cc =====







class IsolationForestOnGaussians : public utils::TrainAndTestTester {

  void SetUp() override {

  // Confirmed with Scikit-learn.



  auto if_model = dynamic_cast<const IsolationForestModel*>(model_.get());



  int max_depth = -1;

  // Confirmed with Scikit-learn.

class IsolationForestOnAdult : public utils::TrainAndTestTester {

  void SetUp() override {

  // Confirmed with Scikit-learn.

  // Confirmed with Scikit-learn.


  // Confirmed with Scikit-learn.


  // A very small model for the smoke test

class IsolationForestOnMammographicMasses : public utils::TrainAndTestTester {

  void SetUp() override {
    // Warning: For this dataset, evaluate on the training data in the test.

  // Confirmed with Scikit-learn.










  // Look for duplicates





    // This is a stochastic test.





    // This is a stochastic test.





    // This is a stochastic test.




        bool found_condition,

    // The NA replacement is the more common value.

    // This is a stochastic test.




    auto values_in_condition =






      bool is_default = field.second.mutual_exclusive().is_default();
        auto other_param_it =


// ===== FILE: yggdrasil_decision_forests/learner/multitasker/multitasker.cc =====








// Extracts the examples of "src" with non-missing value for the column
// "label_idx" into "dst".
absl::Status ExtractExamplesWithLabels(const int label_idx,


absl::Status AddPredictionToDataset(const dataset::VerticalDataset& src,
  auto examples = engine->AllocateExamples(1);





  auto model = std::make_unique<MultitaskerModel>();

  // Initialize the model. Use the first task for meta-data.

  // List the user specified input features.

  // Remove all the labels (and other special columns) from the set of the input
  // features.

  absl::Status status;

  // Index the primary and secondary tasks.

  // Placeholder for primary datasets.


    // Add the input features.

      // Add the output of the secondary models as input features.


    // Extract the training / validation dataset

    // Train





    // Use the secondary models to generated the training dataset for the
    // primary tasks.





absl::Status MultitaskerLearner::SetHyperParameters(









  // Build sub-learner




// ===== FILE: yggdrasil_decision_forests/learner/multitasker/multitasker.h =====


// The multitask learner trains multiple models in parallel.
// The model trained by the multitask learner is a multitask model that contains
// all models it trained.





class MultitaskerLearner : public AbstractLearner {



  absl::Status SetHyperParameters(




  // Hyper-parameters for the sub-learner.




// ===== FILE: yggdrasil_decision_forests/learner/multitasker/multitasker_test.cc =====







class MultitaskerOnAdult : public utils::TrainAndTestTester {
  void SetUp() override {
    // For the util tester.










    auto eval = submodel->Evaluate(test_dataset_, eval_options, &rnd);

    auto eval = submodel->Evaluate(test_dataset_, eval_options, &rnd);

    auto eval = submodel->Evaluate(test_dataset_, eval_options, &rnd);

    // Test serialization with prefix.

    // Loading the multitasker model.

    // Loading a submodel directly.



  auto t1 = tt1->mutable_train_config();
  auto t2 = tt2->mutable_train_config();
  auto t3 = tt3->mutable_train_config();












  auto t1 = tt1->mutable_train_config();
  auto t2 = tt2->mutable_train_config();
  auto t3 = tt3->mutable_train_config();














// ===== FILE: yggdrasil_decision_forests/learner/random_forest/random_forest.cc =====








absl::Status RandomForestLearner::SetHyperParametersImpl(

  // Decision tree specific hyper-parameters.








  // Note: We don't optimize the number of tree as it is always beneficial
  // metric wise, and we don't optimize the inference time or model size (yet).



















absl::Status RandomForestLearner::CheckConfiguration(
  // Check that the decision tree will contain prediction weighting is needed.

  // TODO: Divide function into smaller blocks.

  // Timeout in the tree training.



  auto config_with_default = training_config();

  // If the maximum model size is limited, "keep_non_leaf_label_distribution"
  // defaults to false.



  auto mdl = std::make_unique<RandomForestModel>();


  // Determines if the training code supports `weights` to be empty if
  // all the examples have the same weight. This triggers special handling for
  // improved performance.
  //
  // This feature is not supported for uplifting.
  bool use_optimized_unit_weights = true;




  // Individual seeds for each tree.


  // OOB (out-of-bag) predictions.

  // Prediction accumulator for each example in the training dataset
  // (oob_predictions.size()==training_dataset.nrow()).

  // Time of the last display of OOB metrics in the console. Expressed in
  // seconds from an arbitrary referential. Protected by "oob_metrics_mutex".
  // Number of trees the last time the OOB metrics was computed and displayed in
  // the console.
  int last_oob_computation_num_trees = 0;

  // Prediction accumulator for each example in the training dataset and
  // shuffled according to each input feature:
  // "oob_predictions_per_input_features[i][j]" is the prediction accumulator,
  // for the example "j" (i.e. row "j" in training_dataset), where the value of
  // the input feature "i" has been shuffled. "shuffled" means that, during
  // inference, the value of feature "i" for the example "j" is replaced by the
  // value of the example "k" (of the same feature), where "k" is uniformly
  // sampled in [0, dataset.nrow()[.

  // OOB Performance and variable importance are only computed when training is
  // bootstrapped.


  // If true, only a subset of trees will have been trained.


  // Various fields modified by the various training workers.
  struct {
    // Number of nodes in the "completed" trees.

    // Number of nodes in the accounted trees i.e. the first
    // "next_tree_idx_to_account" trees.

    // Index of the next tree to account.
    int next_tree_idx_to_account GUARDED_BY(mutex) = 0;


    absl::Status status GUARDED_BY(mutex);


  // Initialize the concurrent_fields.

  // Note: "num_trained_trees" is defined outside of the following brackets so
  // to make use it is not released before "pool".

        // The user interrupted the training.

        float bootstrap_size_ratio_factor = 1.f;

        // Maximum training time.
          // Stop the training if it lasted too long.

        // Check if the training should be stopped.
            // Some other thread already failed.

              // Reached the model size limit.

              // Reached the total number of nodes limit.


        // Examples selected for the training.
        // Note: This in the inverse of the Out-of-bag (OOB) set.

        // TODO: Cache.



        auto status_train = decision_tree::Train(

        int current_num_trained_trees;
            // The training of the tree has failed.

            // Note: A model should contain at least one tree.
              // Remove the tree that was just trained.



        // Note: Since the batch size is only impacting the training time (i.e.
        // the oob computation), and since the adaptive work manager assumes a
        // linear relation between work and time, we only measure the duration
        // of the training step.
        //
        // Note: The OOB computation does not impact the quality of the model
        // (only the computation of model metrics). Disabling OOB computation
        // will make the work manager inference more accurate.

        // General logging


        // OOB Metrics.
          // Update the prediction accumulator.
          auto update_oob_status = internal::UpdateOOBPredictionsWithNewTree(

          // Evaluate the accumulated predictions.
          // Compute OOB if one of the condition is true:
          //   - This is the last tree of the model.
          //   - The last OOB was computed more than
          //     "oob_evaluation_interval_in_seconds" ago.
          //   - This last OOB was computed more than
          //     "oob_evaluation_interval_in_trees" trees ago.

            auto evaluation_or = internal::EvaluateOOBPredictions(



            // Print progress in the console.
            auto snippet = build_common_snippet();

          // Variable importance.

    // Remove the non-trained trees.

    // Note: At this point, there are not concurrent workers running.

    // Check for any pending failure during the training.

      // Keep the first trees such that the maximum number of nodes constraint
      // is satisfied.
      int num_trees_to_keep = 0;




  // Cache the structural variable importance in the model data.




void InitializeOOBPredictionAccumulators(


      // Note: -2 because: The value 0 is reserved for the out-of-vocab item,
      // and there is one less predictions than treatments.

      // Nothing to do.

absl::Status UpdateOOBPredictionsWithNewTree(
  // "next_non_oob_example_idx" is the index in "sorted_non_oob_example_indices"
  // of the example, with the smallest index which is greater or equal to the
  // index of the example being iterator on in the following "for loop".


    // Skip the example_idx in "sorted_non_oob_example_indices".

    // Apply the decision tree.

    // Accumulate the decision prediction to the oob accumulator.

  // Configure the evaluation options.
  // Disable the computation of expensive metrics that are not needed for the
  // monitoring of training.
    // Note: The "weights" of "eval_options" won't be used, but "has_weights()"
    // needs to be true.


      // Not decision tree has been trained (yet) with this example in the oob
      // set i.e. all the trees have been trained using this example for
      // training.


absl::Status ComputeVariableImportancesFromAccumulatedPredictions(
  // Note: "for_permutation_importance=true" allows to compute AUC, PR-AUC and
  // other expensive evaluation metrics.



void InitializeModelWithTrainingConfig(

void SampleTrainingExamples(const UnsignedExampleIdx num_examples,

    // Sampling with replacement.
    // Sampling without replacement.
      // The probability of selection is p/n where p is the remaining number of
      // items to sample, and n the remaining number of items to test.

absl::Status ExportOOBPredictions(
  // Create the dataspec that describes the exported prediction dataset.

  // Buffer example.

  // Number of classification classes. Unused if the label is not categorical.
  int num_label_classes = -1;







  // Write the predictions one by one.






absl::Status SetDefaultHyperParameters(





// ===== FILE: yggdrasil_decision_forests/learner/random_forest/random_forest.h =====


// Random Forest learner.
//
// Note: "OOB" stands for "out of bag". The OOB examples of a tree (within a
// random forest) are the examples that are NOT used to trained this tree.
//




class RandomForestLearner : public AbstractLearner {


  // Generic hyper parameter names.




  // Detects configuration errors and warnings.

  absl::Status SetHyperParametersImpl(







void InitializeModelWithTrainingConfig(

// Accumulator of individual tree predictions. Can then be combined to compute
// the random forest predictions.
struct PredictionAccumulator {
  // Number of tree predictions being accumulated.
  int num_trees = 0;

// Initialize a vector of accumulators to support the task specified in
// "config*".
void InitializeOOBPredictionAccumulators(

// Add the predictions of a decision tree to a set of predictor accumulators.
// The tree is applied only on the example indices NOT contained in
// "sorted_non_oob_example_indices".
//
// If "shuffled_attribute_idx" is set, the decision tree will be applied while
// simulating the random shuffling of the value of the attribute
// "shuffled_attribute_idx.value()" using "rnd" as source of randomness.
absl::Status UpdateOOBPredictionsWithNewTree(

// Evaluates the OOB predictions. Examples without any tree predictions are
// skipped.
    int uplift_treatment_col_idx,
    bool for_permutation_importance = false);

// Update the variable importance of the model with set of oob predictions.
absl::Status ComputeVariableImportancesFromAccumulatedPredictions(

// Selects the examples to train one tree. Selects "num_samples" integers in [0,
// num_examples[ with replacement.
void SampleTrainingExamples(const UnsignedExampleIdx num_examples,

// Exports the Out-of-bag predictions of a model to disk.
absl::Status ExportOOBPredictions(

absl::Status SetDefaultHyperParameters(




// ===== FILE: yggdrasil_decision_forests/learner/random_forest/random_forest_hparams_templates.cc =====


// This files defines the pre-configured hyper-parameter templates.



      auto field = config.mutable_parameters()->add_fields();
      auto field = config.mutable_parameters()->add_fields();
      auto field = config.mutable_parameters()->add_fields();
      auto field = config.mutable_parameters()->add_fields();
      auto field = config.mutable_parameters()->add_fields();
      auto field = config.mutable_parameters()->add_fields();


// ===== FILE: yggdrasil_decision_forests/learner/random_forest/random_forest_test.cc =====







void SetExpectedSortingStrategy(Internal::SortingStrategy expected,

// Build a forest with two decision trees as follow:
// [a>1]
//   â”œâ”€â”€ [b=0] (pos)
//   â””â”€â”€ [b=1] (neg)
// [a>3]
//   â”œâ”€â”€ [b=2] (pos)
//   â””â”€â”€ [b=1] (neg)
//
// Build the dataset:
// "a" : {0, 2, 4}
// "b" : {1, 2, 1}
void BuildToyModelAndToyDataset(const model::proto::Task task,



  // Create a tree of the form.
  // [a> alpha]
  //   â”œâ”€â”€ [b=beta]
  //   â””â”€â”€ [b=gamma]
  auto create_tree = [&task](const float alpha, const int beta,
    auto tree = std::make_unique<decision_tree::DecisionTree>();





// Generate a dataset similar as the figure 10 in the Extremely Randomized Trees
// paper.
void ExtremelyRandomizeTreesFigure10Dataset(const int num_examples,
    float y = 0;

// Helper for the training and testing on two non-overlapping samples from the
// adult dataset.
class RandomForestOnAdult : public utils::TrainAndTestTester {
  void SetUp() override {



// Train and test a model on the adult dataset.



  // The top and worst variables have been computed using the "randomForest" R
  // package. YDF and the R randomForest implementation work differently for
  // categorical attributes. Since this dataset has a lot of categorical
  // attributes, the reported orders of variable importance are not exactly the
  // same for the two libraries. However, the overall ranking is still close.

  // Top 3 variables.


  // Worst 2 variables.




  // Check the oob predictions.

  // Check that the label is not in any feature importance table.



// Separate the examples used for the structure and the leaves of the model.


// Extremely Randomize Trees on Adult.

// Equal width histogram split on Adult.


  // Disabling winner take all reduce the logloss (as expected).

  // Disabling winner take all reduce the logloss (as expected).

// Use the one-hot splitter for the categorical features.

  // Add the categorical features.


  // Note: Enabling "mutable_one_hot, increases the average tree depth
  // from 10.5 to 13.2, and reduce the accuracy from 0.825 to 0.817.

// Train a Random Forest model using only the categorical features, and using
// the Random Categorical splitter.

  // Add the categorical features.


  // Similar (should be slightly better) to the dataset sampling.







  // Training and evaluating with the same weight reduce the logloss (as
  // expected).

// Train and test a model on the adult dataset for a maximum given duration.

  // Note: The "TrainAndEvaluateModel" function last a bit more because it is
  // also preparing the dataset and evaluating the final model.


// Train a model with a maximum size in RAM.

  // Note: Each tree takes ~200k of RAM; the majority caused by proto overhead
  // and pointers. The serialized model will be ~5x smaller.

  // Add an extra 3kB to help with the test flakiness.
  // Note: the model can be slightly larger than the
  // "set_maximum_model_size_in_memory_in_bytes" directive.


  // Would take a very long time without a timeout inside of the tree building.


  // Note: The "TrainAndEvaluateModel" function last a bit more because it is
  // also preparing the dataset and evaluating the final model.


  // Train for 5 seconds.

  // Note: The "TrainAndEvaluateModel" function last a bit more because it is
  // also preparing the dataset and evaluating the final model.
  // Note: the model trained with a sanitizer might be small / poor.

// Train and test a model on the adult dataset for a maximum given duration.

  // Note: The "TrainAndEvaluateModel" function last a bit more because it is
  // also preparing the dataset and evaluating the final model.



  // Disabling winner take all reduce the logloss (as expected).


  // Prevent timeouts with a smaller model.






// Helper for the training and testing on two non-overlapping samples from the
// Abalone dataset.
class RandomForestOnAbalone : public utils::TrainAndTestTester {
  void SetUp() override {




  // Check the oob predictions.




















  // Baseline

  // Shuffled

  // Compute importance.

  // Ground truth: 1, 1
  // Baseline prediction: 1, 0
  //
  // Baseline has 0.5 accuracy.
  // Shuffled has 1.0 accuracy (per chance).

// We train a 100-trees regressive RF and ERT on 20 examples. The RF predictions
// are expected to be very "stairy" while the ERT predictions smoothly
// interpolate the training examples.
//
// This test is similar to the figure 10 in the Extremely Randomized Trees
// paper.
//
// The test also checks the RMSE between the model predictions and a linear
// interpolation.
  // Generate the training and testing datasets.


  // Train the models.



  // Generate the predictions.

  // Evaluate the RMSE between the predictions and a linear interpolation.

  // List of <input,output> of the training examples sorted by increasing input
  // value (so we can use a binary search).

  // Computes the RMSE between a model predictions and a linear interpolation
  // (of the training examples).
  auto get_rmse = [&](const std::vector<float>& predictions) -> double {
    // Only the testing examples in between two training examples (i.e. not
    // outside the bounds) are valid.
    int num_valid_examples = 0;
      // The input feature value of the  example "example_idx" is between
      // "(it_upper_bound-1).first" and "it_upper_bound.first".
      // Linear interpolation between "it_lower_bound" and "it_upper_bound".

  // Compute the RMSE between the model prediction and a linear interpolation.

  // ERT's RMSE to the linear interpolation model is significantly lower than
  // RF's RMSE (~5x less).

  // Export the predictions for the plotting.

  // The plot can then be generated by running the following commands in R:
  //  test = read.csv("ert_figure10_test.csv")
  //  train = read.csv("ert_figure10_train.csv")
  //  test = test[order(test$x), ]
  //  train = train[order(train), ]
  //  plot(train$x, train$y, xlab = "input", ylab = "output", ylim = c(0, 1))
  //  lines(test$x, test$rf, col = "blue")
  //  lines(test$x, test$ert, col = "red")
  //  lines(test$x, test$y, col = "green")
  //  legend( 2.3, 0.6, legend = c("RF", "ERT", "True"), col = c("blue", "red",
  //    "green"), lty = c(1, 1, 1))

class RandomForestOnIris : public utils::TrainAndTestTester {
  void SetUp() override {


  // Note: R RandomForest has an OOB (out-of-bag) accuracy of 0.9467.

// Helper for the training and testing on the DNA dataset.
class RandomForestOnDNA : public utils::TrainAndTestTester {
  void SetUp() override {



  // For comparison, the RF model learned in R yields an accuracy of 0.909.

class RandomForestOnSyntheticClassification : public utils::TrainAndTestTester {
  void SetUp() override {


  // This test has a lot of variance because of the small number of examples
  // , large size of the feature space, and procedurally generated dataset.




class RandomForestOnSimPTE : public utils::TrainAndTestTester {
  void SetUp() override {





  // Note: A Qini of ~0.1 is expected with a simple Random Forest model.

  // Export the labels+predictions for external evaluation.

  // Check the oob predictions.





  // Values are unique.

class RandomForestOnRegressiveSimPTE : public utils::TrainAndTestTester {
  void SetUp() override {




  // Note: The labels of this dataset are in {1,2}. Therefore, regressive
  // uplift (this test) is not perfectly equal to categorical uplift
  // (RandomForestOnSimPTE.Base) test, and the Qini score of this test is not
  // exactly the same as the Qini score of categorical uplift test. If the
  // labels of this dataset were to be replaced with {0,1}, the scores would be
  // equal (tested).

class AutotunedRandomForestOnAdult : public utils::TrainAndTestTester {
  void SetUp() override {










  auto model_1 = std::move(model_);

  auto model_2 = std::move(model_);


  auto model_1 = std::move(model_);

  auto model_2 = std::move(model_);


class RandomForestOnSyntheticVectorSequence : public utils::TrainAndTestTester {
  void SetUp() override {

  // Build synthetic dataset.

  // Configure learner

  // Runs checks

  // Build synthetic dataset.
  // Configure learner

  // Runs checks


// ===== FILE: yggdrasil_decision_forests/learner/random_forest/random_forest_tuner_test.cc =====





class AutotunedRandomForestOnAdult : public utils::TrainAndTestTester {
  void SetUp() override {










// ===== FILE: yggdrasil_decision_forests/model/abstract_model.h =====


// Abstract classes for models and model builders (called learners).
//
// FutureWork(gbm): Make this file and the "AbstractModel" minimalistic. Move
// the help methods in a separate file e.g. "abstract_model_utils".





struct ModelIOOptions {
  // The prefix of files used by the model.
  //
  // For saving a model, if `file_prefix` is not set, an empty string is used.
  // For loading a model, if `file_prefix` is not set, the model prefix is
  // auto-detected (if possible) based on the existing files in the given
  // directory.

class AbstractModel {

  // It is likely that you want to use the function "SaveModel" from
  // "model_library.h" instead of this function.
  //
  // Save the model into a directory. The model controls the format of the model
  // (i.e. what file is written and what they contains) but it should not create
  // files called "header.pb" nor "data_spec.pb" (see kModelHeaderFileName and
  // kModelDataSpecFileName) as these filenames are reserved for the model meta
  // information.
  // If no file prefix is given through `io_options`, the empty string is used.

  // It is likely that you want to use the function "LoadModel" from
  // "model_library.h" instead of this function.
  //
  // Load the model from a directory. Should match the format created by "Save".
  // Derived classes may require a file prefix be given through `io_options`.

  // Creates an inference engine able to run the model more efficiently
  // than by calling "Predict". Once the inference engine created, the model
  // can be discarded. If no inference engine is available for the model,
  // an error is returned. If multiple inference engines are available,
  // the faster one will be selected.
  //
  // Inference engines are added as separate dependencies. For example,
  // ../serving/decision_forest:register_engines contains multiple basic
  // inference engines for decision forest models.
  //
  // Because "BuildFastEngine" uses virtual calls, this solution is slower
  // than selecting directly the inference engine at compile time.
  //
  // If specified, "force_engine_name" is the name of the created engine.
  // If "force_engine_name" is not specified, create the fastest compatible
  // engine.

  // Lists the fast engines compatible with the model.
  // Engines are sorted by decreasing expected speed i.e., for the fastest
  // inference, use the first one.

  // Lists the names of fast engines compatible with the model.
  // Engines are sorted by decreasing expected speed i.e., for the fastest
  // inference, use the first one.

  // If set to "False", "BuildFastEngine" won't return an engine, even if one if
  // available.
  void SetAllowFastEngine(const bool allow_fast_engine) {

  // Check that the model is valid. The inference on a non-valid model is non
  // defined.
  //
  // This function is called implicitly when importing and exporting a model.

  // Set the dataspec of the model.
  void set_data_spec(const dataset::proto::DataSpecification& v) {

  // Get the dataspec in the model.

  // Get the mutable dataspec in the model.

  // Set the model's task.
  void set_task(const proto::Task task) { task_ = task; }

  // Get the task of the model.

  // Set the model target column.
  void set_label_col_idx(int label_col_idx) { label_col_idx_ = label_col_idx; }

  // Get the model target column.
  int label_col_idx() const { return label_col_idx_; }

  // Tests if the model has a label.
  bool has_label() const { return label_col_idx_ != -1; }

  // Name of the label column. Should only be called if "has_label()" is true.

  // Set the model ranking group column (e.g. query id).
  void set_ranking_group_col(int ranking_group_col_idx) {

  // Get the model ranking group column.
  int ranking_group_col_idx() const { return ranking_group_col_idx_; }

  // Set the model uplift treatment column.
  void set_uplift_treatment_col(int uplift_treatment_col_idx) {

  // Get the model uplift treatment column.
  int uplift_treatment_col_idx() const { return uplift_treatment_col_idx_; }

  // Column spec of the label.

  // Get the weights used during training..

  // Set training weights.
  void set_weights(const dataset::proto::LinkedWeightDefinition& weights) {

  // Column spec of the label.


  // Export an abstract model to a proto.

  // Load an abstract model from a proto.

  // Evaluates the model on a dataset. Returns a finalized EvaluationResults.
  //
  // If specified, "predictions" will be populated with the predictions.



  // Similar to "EvaluateWithStatus", but allow to override the evaluation
  // objective.
      int override_label_col_idx, int override_group_col_idx,

  // Similar to "EvaluateWithEngine", but allow to override the evaluation
  // objective.
      int override_label_col_idx, int override_group_col_idx,

  // Evaluates the model and appends the results to an initialized and
  // non-finalized EvaluationResults proto.
  //
  // If specified, "predictions" will be populated with the predictions.
  absl::Status AppendEvaluation(

  // Similar as "AppendEvaluation" above. But operates on a dataset stored on
  // disk. This method is preferable when the number of examples is large since
  // they do not have to be all loaded in memory as the same time.
  absl::Status AppendEvaluation(const absl::string_view typed_path,

  // Similar to "AppendEvaluation", but allows to override the evaluation
  // objective.
  absl::Status AppendEvaluationOverrideType(

  // Generates the predictions of the model.
  absl::Status AppendPredictions(

  // Apply the model on an example defined as a VerticalDataset and a row
  // index. Requires for the dataset to have the same structure as the training
  // dataset. The model representation is expected to be generic and the
  // inference code is expected to be slower than the optimized serving code
  // available in "serving:all".
  //
  // Does not set the ground truth and the weight fields in "prediction".
  //
  // TODO: Add status.

  // Apply the model on a proto::Example. The model representation is expected
  // to be generic and the inference code is expected to be slower than the
  // optimized serving code available in "serving:all".
  //
  // "proto::Example" is the native generic example format for YDF. This
  // is different from the "tensorflow::Example". Conversion from
  // "tensorflow::Example" to "proto::Example" can be done with the function
  // "TfExampleToExample".
  //
  // Does not set the ground truth and the weight fields in "prediction".
  //
  // TODO: Add status.

  // Set the ground truth values in a Prediction proto. Ground truth values
  // can be defined by fields like label or example weight. This depends on the
  // model "type" (see Prediction proto). This step is required to evaluate the
  // prediction of one example (done in metric::AddPrediction in "metric.h").
  //
  // Both version requires that either the example (this version) or dataset
  // (next version) contains the label value.
  absl::Status SetGroundTruth(const dataset::proto::Example& example,

  // Set the ground truth values (see description above) of one Prediction
  // proto from the specified row in the given dataset.
  absl::Status SetGroundTruth(const dataset::VerticalDataset& dataset,

  // Generates a human readable description of the statistics and structure of
  // the model. If "full_definition" is true, the entire model definition is
  // printed. In case of large model, this can represent a lot of data.

  // Simplified syntax to "AppendDescriptionAndStatistics".

  // Returns the list of the variable importance according to the model.
  //
  // When derived and in most cases, this function should merge the results
  // with its parent implementation.

  // Returns a sorted list of variable importances (the most important first).
  // "key" should be an element of the result of "AvailableVariableImportances".
  //
  // Note: The model does not have to return a variable importance for all the
  // input features available at training time. If the model does not use a
  // feature, it does not have to return a variable importance for this feature.
  //
  // When derived, this function should also call its parent implementation.

  // Create a user readable description of all the variable importance metrics
  // of the model.
  void AppendAllVariableImportanceDescription(std::string* description) const;

  // Evaluation of the performance of the model estimated during training.
  // Depending on the machine learning algorithm, the semantic of this
  // estimation can change.
  //
  // This evaluation (often called "validation") can be used to guide the
  // training and tuning of the model. For this reason, this evaluation is only
  // indicative and should not be used to compare models.

  // Estimates the memory usage of the model in RAM. The serialized or the
  // compiled version of the model can be much smaller.
  //
  // This value should not be relied upon in tests.
  // May be empty if the model does not support this operation.

  // Estimates the memory usage of the attributes defined in the "AbstractModel"
  // object. Returns {} if the model size is not available.

  // List of input features of the model.

  // Copy the meta data of the model i.e. the attributes common to all models.
  void CopyAbstractModelMetaData(AbstractModel* dst) const;



  bool classification_outputs_probabilities() const {

  void set_classification_outputs_probabilities(bool value) {

  // Computes a set of variable importances available in
  // "AvailableVariableImportances", and store the result in the model. Querying
  // those variable importances will return the cached values (instead of
  // possibly re-computing those variables from the model structure).
  // If the variable is already cached, it will be ignored.
  absl::Status PrecomputeVariableImportances(

  // Metadata accessors.
  //
  // Note: The use of "MetaData" (instead of "proto::MetaData") is a temporary
  // change. Do not depend on it.

  // Hyperparameter tuning logs.

  // Feature selection logs

  // Clear the model from any information that is not required for model
  // serving. This function is called when the model is trained with
  // "pure_serving_model=true", or when using the "--pure_serving" operation in
  // the ":edit_model" tool.
  //
  // Warning: Sub implementation of this methods are expected to call the parent
  // implementation.

  // Creates a plot showing the training logs e.g. quality of the model during
  // training.

  // Compares two models. If the models are equal (except for the meta-data),
  // return an empty string. Otherwise, returns a description of the difference.

  // WARNING: Don't use this function directly. Instead use
  // "model::SerializeModel".
  //
  // Serializes the virtual part of the AbstractModel. This function should not
  // serialize the fields defined in "AbstractModel". The model data can be
  // serialized in an extension in the "dst_proto" or in the "dst_raw".
  // Note that protos are limited to 2GB of data. Therefore, part of the model
  // that is expected to be larger than 2GB should be serialized in the
  // "dst_raw".

  // WARNING: Don't use this function directly. Instead use
  // "model::DeserializeModel".
  //
  // Deserializes the virtual part of the model.

  absl::Status AppendEvaluationWithEngine(

  absl::Status AppendEvaluationWithEngineOverrideType(
      int override_label_col_idx, int override_group_col_idx,


  // Prints information about the hyper-parameter optimizer logs.
  void AppendHyperparameterOptimizerLogs(std::string* description) const;

  // Prints information about the feature selection logs.
  void AppendFeatureSelectionLogs(std::string* description) const;

  // Checks if the ModelIOOptions are sufficient to load the model.
  //
  // At this time, this function checks if a prefix if given.

  // A string uniquely identifying the model type . Used to determine
  // model types during serialization. This should match the registered names in
  // ":model_library".

  // Dataset specification.

  // Modeling task (e.g. Classification, regression).

  // Column idx of the label.
  int label_col_idx_ = -1;

  // Column index of groups (e.g. queries) in ranking.
  int ranking_group_col_idx_ = -1;

  // Column index of uplift treatment.
  int uplift_treatment_col_idx_ = -1;

  // Example weight used during training. If not specified, all the examples
  // have the same weight.

  // Input features of the model sorted by index.


  // Allow for fast engine to run.
  bool allow_fast_engine_ = true;

  // If true, the output of a task=CLASSIFICATION model is a probability and can
  // be used accordingly (e.g. averaged, clamped to [0,1]). If false, the output
  // of the task=CLASSIFICATION model might not be a probability.
  bool classification_outputs_probabilities_ = true;

  // TODO: Use proto::Metadata.
  // Note: Cannot use proto::Metadata with the version of protobuf linked by TF.



  // Indicate if a model is pure for serving i.e. the model was stripped of all
  // information not required for serving.
  bool is_pure_model_ = false;

  // Note: New fields should be registered in:
  // - The proto serialization functions.
  // - The "CopyAbstractModelMetaData" method.



// Set the ground truth in a "proto::Prediction".
//
// See the definition of AbstractModel::SetGroundTruth(...) for mode details.
// Unlike AbstractModel::SetGroundTruth, the two following SetGroundTruth
// functions do not require for a model to exist. When possible use
// "model.SetGroundTruth(...)" instead.

// In case of a non-ranking task (e.g. regression), `ranking_group_col_idx`
// should be set to  `kNoRankingGroup`.
// In case of a non-uplift task (e.g. regression), `uplift_treatment_col_idx`
// should be set to  `kNoUpliftTreatmentGroup`.

// Indices of the columns needed to set the ground truth.
struct GroundTruthColumnIndices {
  // These fields correspond to the fields defined in AbstractModel.


// Note: The "task" defines how the label are interpreted and how the
// predictions are evaluated. The task should correspond to the model emitting
// the predictions, or be compatible with it (e.g. a ranking model can be
// evaluated with task=REGRESSION).

// See comments above.
absl::Status SetGroundTruth(const dataset::VerticalDataset& dataset,

// See comments above.
absl::Status SetGroundTruth(const dataset::proto::Example& example,

// Create a user readable description of the set of the variable importances of
// a model as returned by "GetVariableImportance".
void AppendVariableImportanceDescription(

// Merge the variable importance of "src" to the variable importances of "dst".
// The final variable importance is: src * weight_src + dst * (1 - weight_src).
// If an item is not present in "src" or "dst", its importance is assumed to be
// 0 for this container. The output "dst" is sorted in decreasing order of
// importance.
void MergeVariableImportance(const std::vector<proto::VariableImportance>& src,

// Content accumulator for predictions.
// The final prediction is defined as \sum_i src_factor_i * src_i, where "i"
// correspond to the successive calls to "Add".
class PredictionMerger {
  // Initialize the merged with a target prediction.

  // Add a prediction to dst. Note: "dst" should not be used before "Merge" is
  // called.
  void Add(const proto::Prediction& src, float src_factor);

  // Finalize the addition of the predictions. Should be called before "dst" is
  // used.
  void Merge();

  // "Scales" the predictions. This is similar to multiply all the "src_factor"
  // of the "Add" method by the "scale" parameter.
  //
  // Scaling all the predictions have a different semantic for different tasks
  // but can always be understood as the "accumulation" of the predictions from
  // different sub-predictions.
  //
  //   Classification: Has no effect (multiply the numerator and denominator
  //     used to compute the final probabilities).
  //   Regression: Multiplies the prediction value by "scale".
  //   Ranking:  Multiplies the prediction value by
  //   "scale". Does not impact the predicted ranking.


// Converts a prediction generated by a fast engine into a proto Prediction.
void FloatToProtoPrediction(const std::vector<float>& src_prediction,
                            int example_idx, const proto::Task task,
                            int num_prediction_dimensions,

// Converts a proto Prediction to a list of float predictions.
void ProtoToFloatPrediction(const proto::Prediction& src_prediction,



// ===== FILE: yggdrasil_decision_forests/model/abstract_model_test.cc =====








class FakeModelWithEngine : public FakeModel {

class Engine1 : public serving::FastEngine {
      int num_examples) const override {

  void Predict(const serving::AbstractExampleSet& examples, int num_examples,

  int NumPredictionDimension() const override {



class EngineFactory1 : public model::FastEngineFactory {

  bool IsCompatible(const AbstractModel* const model) const override {




class Engine2 : public serving::FastEngine {
      int num_examples) const override {

  void Predict(const serving::AbstractExampleSet& examples, int num_examples,

  int NumPredictionDimension() const override {



class EngineFactory2 : public model::FastEngineFactory {

  bool IsCompatible(const AbstractModel* const model) const override {




class FakeModelWithoutEngine : public FakeModel {


































  // example_idx = 0

  // example_idx = 1

  // example_idx = 2


  // example_idx = 0

  // example_idx = 0

  // example_idx = 1













  // The model size is compiler+arch dependent.








// ===== FILE: yggdrasil_decision_forests/model/describe.cc =====






void AddKeyValue(utils::html::Html* dst, const absl::string_view key,

void AddKeyMultiLinesValue(utils::html::Html* dst, const absl::string_view key,

bool HasTuner(const AbstractModel& model) {

bool HasFeatureSelector(const AbstractModel& model) {

















  // Index the possible fields and scores.

  struct Trial {
    int step_idx;
    float score;



  // Set table header

  // Fill the table

    // Values ordered by field index.



    // TODO: Don't compare hps by string.


// Creates a plot of the feature selector logs.
  // Creates a plot for each metric, for the score and for the number of
  // features.



          // Adds a 10% extra
          float margin = (max_value - min_value) / 10;


  // Score plot
    float min_value = +std::numeric_limits<float>::infinity();
    float max_value = -std::numeric_limits<float>::infinity();

  // Number of features plot
    float min_value = +std::numeric_limits<float>::infinity();
    float max_value = -std::numeric_limits<float>::infinity();

    float min_value = +std::numeric_limits<float>::infinity();
    float max_value = -std::numeric_limits<float>::infinity();



// Creates a table of the feature selector logs.

    // Create header

  // Fill table content




// Creates a HTML section for the feature selector logs.




  // Find the metric keys.

  // Plot with the results

  // Table with the results.








  // TODO: Plot the trees.




















  // Adds a block of content.
  bool first_entry = true;

  // Sort Variable Importances by key

    // Export VI plot
    // TODO: Use an html plot instead of ascii-art.
















// ===== FILE: yggdrasil_decision_forests/model/model_library.h =====


// Abstract classes for model and model builder (called learner).





// Creates an empty model (the semantic depends on the model) from a model name.
absl::Status CreateEmptyModel(absl::string_view model_name,

// Returns the list of all registered model names.

// Saves a model into a directory for later re-use.
//
// If the directory exists and already contains a model, make sure to empty
// it first as to avoid unnecessary residual files.
absl::Status SaveModel(absl::string_view directory,

// Saves a model into a directory for later re-use.
absl::Status SaveModel(absl::string_view directory, const AbstractModel& mdl,

// Load a model from a directory previously saved with "SaveModel".
absl::Status LoadModel(absl::string_view directory,

// Loads a model in memory.

// Serializes a model to a string.
//
// "SerializeModel" is suited for small models. For large models, using
// "SaveModel" is more efficient.
//
// The returned string is not compressed (e.g. a serialized proto).

// Deserializes a model from a string.

// Checks if a model exist i.e. if the "done" file (see kModelDoneFileName) is
// present.

// If exactly one model exists in the given directory, returns the prefix of the
// given model. Returns absl::StatusCode::kFailedPrecondition if zero or
// multiple models exist in the given directory.

// Checks if a given model is a TensorFlow SavedModel.



// ===== FILE: yggdrasil_decision_forests/model/decision_tree/builder.cc =====







    float threshold) {



void TreeBuilder::LeafRegression(const float value) {

void TreeBuilder::LeafAnomalyDetection(const int num_examples_without_weight) {


// ===== FILE: yggdrasil_decision_forests/model/decision_tree/decision_tree.cc =====









// Prefix added in front of a node when printing it with AppendModelStructure.


// Append "num_tabs x tab_width" spaces to the string.
void AppendMargin(const int num_tabs, std::string* dst,

// Append a human readable description of a node value (i.e. output).
void AppendValueDescription(const dataset::proto::DataSpecification& data_spec,


      // Should the label values be quoted?










// For each path "p" and for each feature "i", adds to
// "min_depth_per_feature[i]" the minimum depth of feature "i" along the path
// "p".
//
// For a given path, "stack[j] = i" with "j \in [0, depth)" indicates that the
// "j-th" node along the path is the "i-th" feature.
//
// "min_depth_per_feature" should be already initialized (its size should be
// set).
//
// If a feature is effectively seen in the tree, set feature_used[feature_idx]
// to be true.
void AddMinimumDepthPerPath(const NodeWithChildren& node, const int depth,
      int min_depth = 0;


// Returns a human readable representation of the anchor. "max_items" is the
// maximum number of anchor values to print. If more values are available,
// "..." is used.




float SquaredDistance(const absl::Span<const float> a,
  float acc = 0;

float DotProduct(const absl::Span<const float> a,
  float acc = 0;

// Converts a map of variable importance into a vector of variable importance
// sorted in decreasing order.

  // Sorts the variable importances in decreasing order.

    int vocab_size, const proto::Condition& condition) {


void AppendConditionDescription(
    // Oblique conditions print the attribute name themself.












bool NodeWithChildren::CheckStructure(




      // There is currently not logic to train oblique condition that don't
      // follow global imputation.



















absl::Status DecisionTree::Validate(

void DecisionTree::CreateRoot() {

absl::Status DecisionTree::WriteNodes(

absl::Status DecisionTree::ReadNodes(

absl::Status NodeWithChildren::WriteNodes(

absl::Status NodeWithChildren::ReadNodes(

void NodeWithChildren::CreateChildren() {

void NodeWithChildren::ClearChildren() {

void NodeWithChildren::ClearLabelDistributionDetails() {

void NodeWithChildren::FinalizeAsLeaf(

void NodeWithChildren::FinalizeAsNonLeaf(

void NodeWithChildren::TurnIntoLeaf() {

struct EvalConditionTrueValue {

struct EvalConditionDiscretizedHigher {



struct EvalConditionHigher {


  float threshold;

struct EvalConditionContainsCategorical {

    // TODO: For small masks, should we use linear search instead?

struct EvalConditionContainsCategoricalSet {


struct EvalConditionContainsBitmapCategorical {


struct EvalConditionContainsBitmapCategoricalSet {


struct EvalConditionOblique {
  struct Data {



    float sum = 0.f;
      float value = (*data.attribute_data[item_idx])[example_idx];

  float threshold;

struct EvalConditionVectorSequenceCloserThan {



  float threshold2_;

struct EvalConditionVectorSequenceProjectedMoreThan {



  float threshold_;

template <typename EvalFn, typename T>
absl::Status EvalConditionTemplate(EvalFn eval_fn,



  // TODO: Populate directly in the right direction, and only use the
  // "reverse option" is the expected number of positive examples does not match
  // the evaluation. Alternatively, we can remove the reverse. The results will
  // be the same (modulo rounding error variation from summing float values in a
  // different order).


absl::Status EvalConditionIsNaTemplate(

  struct EvalConditionIsNaValue {


absl::Status EvalConditionOnDataset(const dataset::VerticalDataset& dataset,












  // Handle NA values.

      // The NA value have been filtered already.







      float sum = 0;
        float value = numerical_column->values()[example_idx];





  // Handle NA values.



  // Handle NA values. Numerical attribute is the only attribute type than has
  // two representation for NA.

      // The NA value have been filtered already.







      float sum = 0;
        auto value = sub_attribute.numerical();





void NodeWithChildren::CountFeatureUsage(


void DecisionTree::CountFeatureUsage(

  // Go down the tree according to an observation attribute values.


  // Go down the tree according to an observation attribute values.

  // Go down the tree according to an observation attribute values.


void DecisionTree::GetPath(const dataset::VerticalDataset& dataset,


void DecisionTree::IterateOnNodes(

void DecisionTree::IterateOnMutableNodes(

void NodeWithChildren::IterateOnNodes(

void NodeWithChildren::IterateOnMutableNodes(

absl::Status NodeWithChildren::Validate(
        // Compatible with all the dataspec types.



void DecisionTree::AppendModelStructure(

void NodeWithChildren::AppendModelStructure(
  auto children_prefix = prefix;


  // The condition.

  // The node value.


  // The children.

void SetLeafIndices(DecisionForest* trees) {


// Number of nodes in a list of decision trees.

bool CheckStructure(const CheckStructureOptions& options,

void AppendModelStructure(

void AppendModelStructureHeader(
    // Print the label values.



int DecisionTree::MaximumDepth() const {
  int max_depth = -1;

void DecisionTree::SetLeafIndices() {
  int next_leaf_idx = 0;

bool DecisionTree::CheckStructure(

void DecisionTree::ScaleRegressorOutput(const float scale) {



  struct Importance {
    bool used = false;




  // Do the three following operations:
  //   - Copy the importance from the vector to the map.
  //   - Compute the **inverse** mean min depth.
  //   - Skip non used features.





// Gets the leaf index for each example and each tree.
//
// The returned values "leaves" is defined as follow: "leaves[i+j *
// trees.size()]" is the leaf index of the j-th example in "dataset" for the
// i-th tree.


absl::Status Distance(









    // Print the trees.



// ===== FILE: yggdrasil_decision_forests/model/decision_tree/decision_tree.h =====


// Generic decision tree model and learning.
// Support multi-class classification and regression.
//
// For classification, the information gain is used as the split score. For
// regression, the standard deviation reduction is used as the split score.






// Variable importance names common for decision tree based models.
// "INV_MEAN_MIN_DEPTH" is 1/(1+d) where "d" is the average depth of the feature
// in the forest.

// Return an identifier of a type of condition.

// Evaluate a condition on an example contained in a vertical dataset.



// A list of selected examples, and a related buffer used to do some
// computation.
struct SelectedExamplesRollingBuffer {

  bool empty() const { return active.empty(); }


struct ExampleSplitRollingBuffer {


absl::Status EvalConditionOnDataset(const dataset::VerticalDataset& dataset,
                                    bool dataset_is_dense,

// Argument to the "CheckStructure" method that tests various aspects of the
// model structure. By default, "CheckStructureOptions" checks if the model
// was trained with global imputation.
struct CheckStructureOptions {
  // "global_imputation_*" tests if the model structure looks as if it was
  // trained with global imputation. That is, the "na_value" values of the
  // conditions (i.e. the value of the condition when a value is missing) are
  // equal to the condition applied with global imputation feature replacement
  // (i.e. replacing the value by the global mean or median; see
  // GLOBAL_IMPUTATION strategy for more details).

  // For the "is higher" conditions.
  bool global_imputation_is_higher = true;

  // For all the other conditions.
  bool global_imputation_others = true;

  // Check if the model does not contain any IsNA condition. Should not be
  // combined with other options.
  bool check_no_na_conditions = false;



// A node and its two children (if any).
class NodeWithChildren {
  // Approximate size in memory (expressed in bytes) of the node and all its
  // children.

  // Exports the node (and its children) to a RecordIO writer. The nodes are
  // stored sequentially with a depth-first exploration.
  absl::Status WriteNodes(

  // Imports the node (and its children) to a RecordIO reader.
  absl::Status ReadNodes(utils::ProtoReaderInterface<proto::Node>* reader);

  // Indicates the node is a leaf i.e. if the node DOES NOT have children.
  bool IsLeaf() const { return !children_[0]; }

  // Clear the detailed label distribution i.e. we only keep the top category
  // (in case of classification) or the mean (in case of regression).
  void ClearLabelDistributionDetails();

  // See definition of "CountFeatureUsage" in Random Forest.
  void CountFeatureUsage(



  // The "positive" child i.e. the child that is responsible for the prediction
  // when the condition evaluates to true.

  // The "negative" child.

  // Instantiate the children.
  void CreateChildren();

  // Removes the children.
  void ClearChildren();

  // Number of nodes.

  // Check the validity of a node and its children.
  absl::Status Validate(

  // Call "call_back" on the node and all its children.
  void IterateOnNodes(const std::function<void(const NodeWithChildren& node,
                      int depth = 0) const;

  void IterateOnMutableNodes(
      bool neg_before_pos_child, int depth);

  // Append a human readable semi-graphical description of the model structure.
  void AppendModelStructure(const dataset::proto::DataSpecification& data_spec,
                            int label_col_idx, int depth,

  // Convert a node that was previously "FinalizeAsNonLeaf" into a leaf.
  void TurnIntoLeaf();

  // Finalize the node structure as a leaf. After this function is called, this
  // node is guaranteed to be a leaf.
  void FinalizeAsLeaf(bool store_detailed_label_distribution);

  // Finalize the node structure as a non-leaf. After this function is called,
  // this node is guaranteed not to be a leaf.
  void FinalizeAsNonLeaf(bool keep_non_leaf_label_distribution,
                         bool store_detailed_label_distribution);

  // Tests if the model satisfy the condition defined in
  // "CheckStructureOptions".
  bool CheckStructure(const CheckStructureOptions& options,

  void set_leaf_idx(const int32_t v) { leaf_idx_ = v; }

  void set_depth(const int32_t v) { depth_ = v; }

  // Compare a tree to another tree. If they are equal, return an empty string.
  // If they are different, returns an explanation of the differences.

  // Node content (i.e. value and condition).

  // Children (if any).

  // Index of the leaf (if the node is a leaf) in the tree in a depth first
  // exploration. It is set by calling "SetLeafIndices()".

  // Depth of the node. Assuming that the root node has depth 0. It is set by
  // calling "SetLeafIndices()".

// A generic decision tree. This class is designed for cheap modification (by
// opposition to fast serving).
class DecisionTree {
  // Estimates the memory usage of the model in RAM. The serialized or the
  // compiled version of the model can be much smaller.

  // Number of nodes in the tree.

  // Number of leafs in the tree.

  // Exports the tree to a RecordIO writer. Cannot export a tree without a root
  // node.
  absl::Status WriteNodes(

  // Imports the tree from a RecordIO reader.
  absl::Status ReadNodes(utils::ProtoReaderInterface<proto::Node>* reader);

  // Creates a root node. Fails if the tree is not empty (i.e. if there is
  // already a root node).
  void CreateRoot();


  // Check the validity of a tree.
  absl::Status Validate(

  // See definition of "CountFeatureUsage" in Random Forest.
  void CountFeatureUsage(

  // Apply the decision tree on an example and returns the leaf.




  // Apply the decision tree on an example and returns the path.
  void GetPath(const dataset::VerticalDataset& dataset,

  // Apply the decision tree similarly to "GetLeaf". However, during inference,
  // simulate the replacement of the value of the attribute
  // "selected_attribute_idx" with the value of the example
  // "row_id_for_selected_attribute" (instead of "row_idx" for the other
  // attributes).

  // Call "call_back" on all the nodes of the model.
  void IterateOnNodes(

  void IterateOnMutableNodes(
      bool neg_before_pos_child = false);

  // Append a human readable semi-graphical description of the model structure.
  void AppendModelStructure(const dataset::proto::DataSpecification& data_spec,

  // Maximum depth of the forest. A depth of "0" means a stump i.e. a tree with
  // a single node.  A depth of "-1" is an empty tree.
  int MaximumDepth() const;

  // Scales the output of a regressor tree. If the tree is not a regressor, an
  // error is raised.
  void ScaleRegressorOutput(float scale);

  // Tests if the model satisfy the condition defined in
  // "CheckStructureOptions".
  bool CheckStructure(const CheckStructureOptions& options,

  // Set the "leaf_idx" field for all the leaves. The index of a leaf is
  // assigned in the depth first iteration over all the nods (negative before
  // positive).
  void SetLeafIndices();

  // Compare a tree to another tree. If they are equal, return an empty string.
  // If they are different, returns an explanation of the differences.
                           int label_idx, const DecisionTree& other) const;

  // Root of the decision tree.

// A list of trees without specific semantic.

// Sets the leaf indices of all the trees.
void SetLeafIndices(DecisionForest* trees);

// Estimate the size (in bytes) of a list of decision trees.
// Returns 0 if the size cannot be estimated.

// Number of nodes in a list of decision trees.

// Tests if the model satisfy the condition defined in
// "CheckStructureOptions".
bool CheckStructure(const CheckStructureOptions& options,

// Append a human readable semi-graphical description of the model structure.
void AppendModelStructure(const DecisionForest& trees,
                          int label_col_idx, std::string* description);

// Append the header of AppendModelStructure.
void AppendModelStructureHeader(


// Gets the number of time each feature is used as root in a set of trees.

// Gets the number of time each feature is used in a set of trees.

// Gets the average minimum depth of each feature.

// Gets the weighted sum of the score (the semantic of the score depends on the
// loss function) of each feature.

// Append a human readable description of a node condition (i.e. split).
void AppendConditionDescription(

// Checks if two sets represented by sorted containers intersect i.e. have at
// least one element in common.
template <typename Iter1, typename Iter2>
bool DoSortedRangesIntersect(Iter1 begin1, Iter1 end1, Iter2 begin2,

// Extracts the list of positive elements from a "contains" type conditions.
    int vocab_size, const proto::Condition& condition);

// Computes the pairwise distance between examples in "dataset1" and
// "dataset2".
//
// The distance is computed as one minus the ratio of common active leaves
// between two examples.
//
// "distances[i * dataset2.nrows() +j]" will be the distance between the i-th
// example of "dataset1" and the j-th example of "dataset2".
//
// "tree_weights" are optional tree weights. If specified, the size of
// "tree_weights" should be the same as "trees".
absl::Status Distance(

// Lists the input features used by the trees. The input features are given as
// sorted column indices.

// Compare two forests. If they are equal, return an empty string. If they are
// different, returns an explanation of the differences.

// Square of the euclidean distance between two vectors.
float SquaredDistance(absl::Span<const float> a, absl::Span<const float> b);

// A dot product between two vectors.
float DotProduct(absl::Span<const float> a, absl::Span<const float> b);



// ===== FILE: yggdrasil_decision_forests/model/decision_tree/decision_tree_test.cc =====











  // We get the first positive child.



  // Switch the 0 and 1 rows.

  // Replace the row 0 and 1 by themself i.e. equivalent to "GetLeaf".

  // Switch the row 0 and 1 for an unused attribute.

class EvalConditions : public ::testing::Test {
  void SetUp() override {

  // Check the evaluation of a condition on VerticalDataset and proto::Example.
  // Returns the string representation of the condition.

    // Evaluate on single example on dataset

    // Evaluate on single proto example

    // Evaluate on full dataset
    auto selected_example_rb = SelectedExamplesRollingBuffer::Create(




  // The tested value is NA.

  // The tested value is not NA.

  // The tested value is 2.


  // The tested value is NA.


  // The tested value is false.

  // The tested value is true.

  // The tested value is A=1.


  // The tested value is B=2.

  // The tested value is {X=1}.

  // The tested value is the *empty* set.

  // The tested value is 1=A.


  // The tested value is B=2.

  // The tested value is {X=1}. The condition is {1, 2}.

  // The tested value is the *empty* set. The condition is {1, 2}.







































  auto vi = StructureNumberOfTimesAsRoot(trees);








  auto vi = StructureNumberOfTimesInNode(trees);








  auto vi = StructureMeanMinDepth(trees, /*num_features=*/4);










  auto vi = StructureSumScore(trees);




  // Builds a decision tree with a single condition and two leaf nodes.
  //
  // attribute >= threshold
  //     â”œâ”€(neg)â”€ leaf #0
  //     â””â”€(pos)â”€ leaf #1
    auto tree = std::make_unique<DecisionTree>();

  // Build a forest with two trees.






  // Builds a decision tree with a single condition and two leaf nodes.
  //
  // attribute >= threshold
  //     â”œâ”€(neg)â”€ leaf #0
  //     â””â”€(pos)â”€ leaf #1
    auto tree = std::make_unique<DecisionTree>();

  // Build a forest with two trees.






  // Builds a decision tree with a single higherThan condition and two leaf
  // nodes.
  //
  // attribute >= threshold
  //     â”œâ”€(neg)â”€ leaf #0
  //     â””â”€(pos)â”€ leaf #1
    auto tree = std::make_unique<DecisionTree>();
  // Builds a decision tree with a two conditions
  //
  // attribute>=0.5
  //     â”œâ”€(pos)â”€ attribute is NA
  //     |        â”œâ”€(pos)
  //     |        â””â”€(neg)
  //     â””â”€(neg)
    auto tree = std::make_unique<DecisionTree>();


  // Build a forest with one trees.





  auto tree = std::make_unique<DecisionTree>();


  auto [pos, l1] = builder.ConditionIsGreater(1, 1);
  auto [l2, l3] = pos.ConditionOblique({2, 3}, {0.5f, 0.5f}, 1);





    auto [pos, l1] = builder1.ConditionIsGreater(1, 1);
    auto [l2, l3] = pos.ConditionIsGreater(1, 2);

    // Same as builder 1
    auto [pos, l1] = builder2.ConditionIsGreater(1, 1);
    auto [l2, l3] = pos.ConditionIsGreater(1, 2);

    auto [pos, l1] = builder3.ConditionIsGreater(1, 1);
    auto [l2, l3] = pos.ConditionIsGreater(1, 3);  // Replace value 2 with 3.

  // Tree 1 and tree 2 are equal.



  auto [pos, l1] = builder.ConditionIsGreater(1, 1);
  auto [l2, l3] = pos.ConditionIsGreater(1, 2);



// ===== FILE: yggdrasil_decision_forests/model/decision_tree/structure_analysis.cc =====







  // List of max depths that we care about. "-1" means no max depth.

  // Allocates the counters.


  // Fills the histograms and counters.






void StrAppendForestStructureStatistics(
  // Display statistics.




  // Given a vector of integer "counts", returns the index and value sorted by
  // decreasing value. Excludes the zeros.



void StrAppendForestStructureStatistics(


// ===== FILE: yggdrasil_decision_forests/model/decision_tree/structure_analysis.h =====






// Number of type of conditions.
// Should be 1 + the higher field in proto::Condition.

// Statistics about a forest.
struct ForestStructureStatistics {

  // Distribution of the depth of the leafs.

  // Distribution of the number of training examples that reaches the leafs.


  int num_trees = 0;

  // "condition_attribute_sliced_by_max_depth[i].second[j]" is the number of
  // nodes, of depth equal of below
  // "condition_attribute_sliced_by_max_depth[i].first" , with attribute index
  // "j".

  // "condition_type_sliced_by_max_depth[i].second[j]" is the number of
  // nodes, of depth equal of below
  // "condition_type_sliced_by_max_depth[i].first" , with condition type
  // "j" (when casted as "proto::Condition::TypeCase").

// Extracts statistics.

// Append statistics in a text human readable form.
void StrAppendForestStructureStatistics(

// Utility function that combines ComputeForestStructureStatistics and
// StrAppendForestStructureStatistics.
void StrAppendForestStructureStatistics(



// ===== FILE: yggdrasil_decision_forests/model/gradient_boosted_trees/gradient_boosted_trees.cc =====








// Basename for the shards containing the nodes.
// Filename containing the gradient boosted trees header.



void GradientBoostedTreesModel::ApplyHeaderProto(const proto::Header& header) {

absl::Status GradientBoostedTreesModel::Save(

  // Format used to store the nodes.

  int num_shards;

  auto header = BuildHeaderProto();


absl::Status GradientBoostedTreesModel::Load(absl::string_view directory,


absl::Status GradientBoostedTreesModel::SerializeModelImpl(

absl::Status GradientBoostedTreesModel::DeserializeModelImpl(

absl::Status GradientBoostedTreesModel::Validate() const {




  int expected_initial_predictions_size = -1;



void GradientBoostedTreesModel::set_loss(

bool GradientBoostedTreesModel::CheckStructure(

// Add a new tree to the model.
void GradientBoostedTreesModel::AddTree(

void GradientBoostedTreesModel::CountFeatureUsage(

absl::Status GradientBoostedTreesModel::PredictGetLeaves(

void GradientBoostedTreesModel::Predict(


      float proba_true;

      // Zero initial prediction for the MULTINOMIAL_LOG_LIKELIHOOD.

        int accumulator_cell_idx = 0;


      // Top class.
        float sum_logit = 0;
        int highest_cell_idx = 0;
        float highest_cell_value = 0;
          auto value = accumulator[accumulator_idx];
        // Sum logits.
        float sum_exp = 0;
          // The offset of 1 between the class idx and the accumulator_idx is to
          // skill the special OOD value with index 0.
        // Softmax
        int highest_cell_idx = 0;
        float highest_cell_value = 0;

        float clamped_accumulator =


void GradientBoostedTreesModel::Predict(

      // Zero initial prediction for the MULTINOMIAL_LOG_LIKELIHOOD.

        int accumulator_cell_idx = 0;

      // Note: Why the "+1"? : "prediction" reserves the first value for the out
      // of vocabulary which is not taken into account in "accumulator'.


      float sum_exp = 0;


      float highest_cell_value = 0;
      int highest_cell_idx = 0;



        float clamped_accumulator =


void GradientBoostedTreesModel::CallOnAllLeafs(

void GradientBoostedTreesModel::CallOnAllLeafs(

void GradientBoostedTreesModel::IterateOnNodes(

void GradientBoostedTreesModel::IterateOnMutableNodes(

void GradientBoostedTreesModel::AppendModelStructure(


    // The log entry corresponding to the final model is identified with the
    // number of trees in the final model.
    // `log` is the training log that corresponds to the final model.
      // Some classical metric names.


void GradientBoostedTreesModel::AppendDescriptionAndStatistics(
    bool full_definition, std::string* description) const {




  // Training logs.
    int entry_idx = 0;
        // Metric name.

        // Metric values.
        float train_value = std::numeric_limits<float>::quiet_NaN();
        float valid_value = std::numeric_limits<float>::quiet_NaN();


      // Print the first 5 entries, and then, print once every 10 entries.


  auto variable_importances = AbstractModel::AvailableVariableImportances();

  // Remove possible duplicates.



    // Tree structure variable importances.

absl::Status GradientBoostedTreesModel::MakePureServing() {
            // Remove the unused information.
            // Remove the label information from the non-leaf nodes.



  // One plot for the loss, and one plot for each metric.
      auto placer,

  // Setup loss plot

  // Fill loss plot

  // Metric plots


      // Training loss

      // Validation loss


absl::Status GradientBoostedTreesModel::Distance(
  // Normalize the tree weights.



  auto loss_name = proto::Loss_Name(loss_);


float WeightedMeanAbsLeafValue(const decision_tree::DecisionTree& tree) {





// ===== FILE: yggdrasil_decision_forests/model/gradient_boosted_trees/gradient_boosted_trees.h =====


// Implementation of Gradient Boosted Trees.






class GradientBoostedTreesLearner;

// A GBT model i.e. takes as input an example and outputs a prediction.
// See the file header for a description of the GBT learning algorithm/model.
class GradientBoostedTreesModel : public AbstractModel,

  // The prediction of a model trained with a poisson loss is computed as:
  // p := e^clamp(acc, -kPoissonLossClampBounds, kPoissonLossClampBounds)

  absl::Status Save(absl::string_view directory,
  absl::Status Load(absl::string_view directory,

  absl::Status Validate() const override;

  // Computes the indices of the active leaves.
  absl::Status PredictGetLeaves(const dataset::VerticalDataset& dataset,

  void Predict(const dataset::VerticalDataset& dataset,

  void Predict(const dataset::proto::Example& example,

  // Number of nodes in the model.

  bool CheckStructure(

  // Number of trees in the model. Note that this value can be different from
  // the "num_trees" parameter used during training for two reasons: (1) Early
  // stopping was triggered, (2) the model is trained with multiple trees per
  // iteration.
  int num_trees() const override { return NumTrees(); }

  // Number of times each feature is used in the model. Returns a map, indexed
  // by feature index, and counting the number of time a feature is used.
  void CountFeatureUsage(



  void set_loss(proto::Loss loss, const proto::LossConfiguration& loss_config);

  int num_trees_per_iter() const { return num_trees_per_iter_; }
  void set_num_trees_per_iter(const int num_trees_per_iter) {


  // Sets and gets the initial predictions of the model.
  //
  // The initial prediction is the constant part of the model i.e. the
  // prediction of the model if all the trees are removed.
  //
  // For people familiar with Neural Nets, it can be seen as the "bias" weight
  // of a neuron.
  void set_initial_predictions(std::vector<float> initial_predictions) {



  // List the variable importances that can be computed from the model
  // structure.


  float validation_loss() const { return validation_loss_; }
  void set_validation_loss(const float loss) { validation_loss_ = loss; }

  bool output_logits() const { return output_logits_; }
  void set_output_logits(bool value) { output_logits_ = value; }

  // Updates the format used to save the model on disk. If not specified, the
  // recommended format `model::decision_tree::RecommendedSerializationFormat()`
  // is used.
  void set_node_format(const std::optional<std::string>& format) override {

  // Adds a new tree to the model.
  void AddTree(std::unique_ptr<decision_tree::DecisionTree> decision_tree);

  absl::Status MakePureServing() override;

  // Fields related to unit testing.
  struct Testing {
    // If true, the "CheckStructure" method will fail if
    // "global_imputation_is_higher"=True.
    bool force_fail_check_structure_global_imputation_is_higher = false;


  // The distance between two examples is computed at the ratio of shared
  // leaves, weighted by the number of training examples in the leaf and the
  // average absolute value of the leaves in the tree.
  absl::Status Distance(const dataset::VerticalDataset& dataset1,


  void PredictClassification(const dataset::VerticalDataset& dataset,

  void PredictRegression(const dataset::VerticalDataset& dataset,

  void PredictClassification(const dataset::proto::Example& example,

  void PredictRegression(const dataset::proto::Example& example,

  // Estimates the memory usage of the model in RAM. The serialized or the
  // compiled version of the model can be much smaller.

  // Call the function "callback" on all the leafs in which the example (defined
  // by a dataset and a row index) is falling.
  void CallOnAllLeafs(

  void CallOnAllLeafs(

  void AppendDescriptionAndStatistics(bool full_definition,

  // Append a human readable semi-graphical description of the model structure.
  void AppendModelStructure(std::string* description) const;

  // Call "call_back" on all the nodes of the model.
  void IterateOnNodes(

  void IterateOnMutableNodes(

  // The decision trees.
  // Prediction constant of the model.

  // Loss evaluated on the validation dataset. Only available is a validation
  // dataset was provided during training.
  float validation_loss_ = std::numeric_limits<float>::quiet_NaN();

  // Number of trees extracted at each gradient boosting operation.
  int num_trees_per_iter_;

  // Evaluation metrics and other meta-data computed during training.
  // If true, call to predict methods return logits (e.g. instead of probability
  // in the case of classification).
  bool output_logits_ = false;

  // Format used to stored the node on disk.
  // If not specified, the format `RecommendedSerializationFormat()` will be
  // used. When loading a model from disk, this field is populated with the
  // format.

  struct Testing testing_;

  absl::Status SerializeModelImpl(model::proto::SerializedModel* dst_proto,

  absl::Status DeserializeModelImpl(


  void ApplyHeaderProto(const proto::Header& header);
  // Full name of the loss, including NDCG truncation where applicable.
  // Loss used to train the model.
  // Options of the loss.


// Average of the absolute value of the leafs weighted by the number of training
// examples.
float WeightedMeanAbsLeafValue(const decision_tree::DecisionTree& tree);




// ===== FILE: yggdrasil_decision_forests/model/isolation_forest/isolation_forest.cc =====







// Basename for the shards containing the nodes.
// Filename containing the isolation forest header.

  struct ImportancePerFeature {
    int num_usage = 0;
  bool has_training_information = true;

              // Missing training set information, abort.

struct DiffiInlierOutlier {

  // Add a constructor for C++17 compatibility.

struct DiffiIIC {

struct DiffiCFI {

// Score the training examples in `node` as inliers and outliers. The result is
// stored in pre-order in `inlier_outlier_counter`.
    // Use 0.5 as the hard cutoff for inlier and outlier.
    auto pos_counter = PredictTrainingExamples(
    auto neg_counter = PredictTrainingExamples(



// Compute the cumulative feature importances recursively by traversing the
// paths to the leaves. Corresponds to most of Algorithm 2 in the paper.
absl::Status ComputeCFIs(
      // The paper doesn't make it clear if the counter should increase for
      // nodes with zero IIC. In the official implementation, these node are not
      // counted.

// DIFFI score. See https://arxiv.org/abs/2007.11117.
  struct ImportancePerFeature {
  // Compute the Induced Imbalance Coefficient (IIC) of a node according to
  // equations (4) -- (6) in the paper.
      // Do not assign an IIC for trivial splits. This condition is not explicit
      // in the paper, but it makes sense and exists in the reference
      // implementation.
  bool has_training_information = true;

  // For each feature, stores its CFI.
    // For each tree node (in pre-order), record the number of
    // inliers and outliers and, in a second step, the corresponding IICs.
    int depth_first_index = 0;
    // For each internal node, compute the IICs (Algorithm 1 in the paper).
    // This code assumes that IterateOnNodes follows pre-order.

    int node_idx = 0;


  // Convert the CFIs into feature importances by normalizing between inliers
  // and outliers.


float PreissAveragePathLength(UnsignedExampleIdx num_examples) {

  // Harmonic number
  // This is the approximation proposed in "Isolation Forest" by Liu et al.


float IsolationForestPredictionFromDenominator(const float average_h,

float IsolationForestPrediction(const float average_h,


void IsolationForestModel::ApplyHeaderProto(const proto::Header& header) {

absl::Status IsolationForestModel::Save(

  // Format used to store the nodes.

  int num_shards;

  auto header = BuildHeaderProto();


absl::Status IsolationForestModel::Load(absl::string_view directory,


absl::Status IsolationForestModel::SerializeModelImpl(

absl::Status IsolationForestModel::DeserializeModelImpl(

absl::Status IsolationForestModel::Validate() const {


void IsolationForestModel::PredictLambda(
  float sum_h = 0.0;


void IsolationForestModel::Predict(const dataset::VerticalDataset& dataset,

void IsolationForestModel::Predict(const dataset::proto::Example& example,

absl::Status IsolationForestModel::PredictGetLeaves(

bool IsolationForestModel::CheckStructure(

// Add a new tree to the model.
void IsolationForestModel::AddTree(

void IsolationForestModel::AppendDescriptionAndStatistics(
    bool full_definition, std::string* description) const {



absl::Status IsolationForestModel::MakePureServing() {
            // Remove the label information from the non-leaf nodes.

absl::Status IsolationForestModel::Distance(


  auto variable_importances = AbstractModel::AvailableVariableImportances();

  // Remove possible duplicates.






// ===== FILE: yggdrasil_decision_forests/model/isolation_forest/isolation_forest.h =====







// Isolation-Forest specific variable importances

class IsolationForestModel : public AbstractModel,


  void Predict(const dataset::VerticalDataset& dataset,

  void Predict(const dataset::proto::Example& example,

  absl::Status PredictGetLeaves(const dataset::VerticalDataset& dataset,

  bool CheckStructure(

  void AddTree(std::unique_ptr<decision_tree::DecisionTree> decision_tree);


  void AppendDescriptionAndStatistics(bool full_definition,

  absl::Status MakePureServing() override;

  absl::Status Distance(const dataset::VerticalDataset& dataset1,

  int num_trees() const override { return decision_trees_.size(); }

  // For the serving engines.
  // TODO: Move in DecisionForestInterface.
  void CountFeatureUsage(



  void set_node_format(const std::optional<std::string>& format) override {

  void set_num_examples_per_trees(int64_t value) {



  absl::Status Save(absl::string_view directory,

  absl::Status Load(absl::string_view directory,

  absl::Status Validate() const override;

  // List the variable importances that can be computed from the model
  // structure.

  void PredictLambda(std::function<const decision_tree::NodeWithChildren&(

  // The decision trees.

  // Node storage format.

  absl::Status SerializeModelImpl(model::proto::SerializedModel* dst_proto,

  absl::Status DeserializeModelImpl(

  void ApplyHeaderProto(const proto::Header& header);



  // Number of examples used to grow each tree.

// Analytical expected number of examples in a binary tree trained with
// "num_examples" examples. Called "c" in "Isolation-Based Anomaly Detection" by
// Liu et al.
float PreissAveragePathLength(UnsignedExampleIdx num_examples);

// Isolation forest prediction.
float IsolationForestPrediction(float average_h,

// Isolation forest prediction, from the pre-computed denominator.
float IsolationForestPredictionFromDenominator(float average_h,
                                               float denominator);



// ===== FILE: yggdrasil_decision_forests/model/multitasker/multitasker.h =====


// A multitasker model is a model containing multiple sub-models solving
// different tasks. Individual models of a multitasker model are generally
// trained in parallel using the multitasker learner.
//
// Warning: The "Predict" methods of a multitasker model is calling the
// "Predict" method on the first sub-model.
//




class MultitaskerLearner;

class MultitaskerModel : public AbstractModel {


  absl::Status Save(absl::string_view directory,

  absl::Status Load(absl::string_view directory,

  absl::Status Validate() const override;

  // Generate a predictions with the first model. To make predictions with the
  // other models, use "models(model_idx)->Predict(...)".
  void Predict(const dataset::VerticalDataset& dataset,

  void Predict(const dataset::proto::Example& example,

  void AppendDescriptionAndStatistics(bool full_definition,








// ===== FILE: yggdrasil_decision_forests/model/random_forest/random_forest.cc =====







// Basename for the shards containing the nodes.
// Filename containing the random forest header.




void RandomForestModel::ApplyHeaderProto(const proto::Header& header) {

absl::Status RandomForestModel::Save(absl::string_view directory,

  // Format used to store the nodes.

  int num_shards;

  auto header = BuildHeaderProto();


absl::Status RandomForestModel::Load(absl::string_view directory,


absl::Status RandomForestModel::SerializeModelImpl(

absl::Status RandomForestModel::DeserializeModelImpl(

absl::Status RandomForestModel::Validate() const {














bool RandomForestModel::CheckStructure(

// Add a new tree to the model.
void RandomForestModel::AddTree(

void RandomForestModel::CountFeatureUsage(

absl::Status RandomForestModel::PredictGetLeaves(

void RandomForestModel::Predict(const dataset::VerticalDataset& dataset,

void RandomForestModel::Predict(const dataset::proto::Example& example,

void RandomForestModel::PredictClassification(

void RandomForestModel::PredictRegression(

void RandomForestModel::PredictUplift(

void RandomForestModel::PredictClassification(


void RandomForestModel::PredictRegression(

void RandomForestModel::PredictUplift(

void RandomForestModel::CallOnAllLeafs(

void RandomForestModel::CallOnAllLeafs(

void RandomForestModel::AppendDescriptionAndStatistics(
    bool full_definition, std::string* description) const {








void RandomForestModel::IterateOnNodes(

void RandomForestModel::IterateOnMutableNodes(

void RandomForestModel::AppendModelStructure(std::string* description) const {

  auto variable_importances = AbstractModel::AvailableVariableImportances();
      // TODO: Add uplift variable importances.

  // Remove possible duplicates.





int RandomForestModel::MaximumDepth() const {
  int max_depth = -1;

int RandomForestModel::MinNumberObs() const {
  int min_num_obs = std::numeric_limits<int>::max();

absl::Status RandomForestModel::MakePureServing() {
            // Remove the label information from the non-leaf nodes.

absl::Status RandomForestModel::Proximity(





  // Select displayed evaluation metric.




void AddClassificationLeafToAccumulator(

void FinalizeClassificationLeafToAccumulator(

void AddRegressionLeafToAccumulator(const decision_tree::proto::Node& node,

void AddUpliftLeafToAccumulator(const decision_tree::proto::Node& node,




// ===== FILE: yggdrasil_decision_forests/model/random_forest/random_forest.h =====


// Random Forest model.
//





class RandomForestModel : public AbstractModel, public DecisionForestInterface {

  // Variable importance introduced by Breiman
  // (https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf).


  absl::Status Save(absl::string_view directory,
  absl::Status Load(absl::string_view directory,

  absl::Status Validate() const override;

  // Compute a single prediction of a model on a VerticalDataset. See the
  // documentation of "AbstractModel" for mode details.
  void Predict(const dataset::VerticalDataset& dataset,

  // Compute a single prediction for a classification random forest.
  void PredictClassification(const dataset::VerticalDataset& dataset,

  // Compute a single prediction for a regression random forest.
  void PredictRegression(const dataset::VerticalDataset& dataset,

  // Compute a single prediction for an uplift random forest.
  void PredictUplift(const dataset::VerticalDataset& dataset,

  void Predict(const dataset::proto::Example& example,

  // Compute a single prediction for a classification random forest.
  void PredictClassification(const dataset::proto::Example& example,

  // Compute a single prediction for a regression random forest.
  void PredictRegression(const dataset::proto::Example& example,

  // Compute a single prediction for an uplift random forest.
  void PredictUplift(const dataset::proto::Example& example,

  // Computes the indices of the active leaves.
  absl::Status PredictGetLeaves(const dataset::VerticalDataset& dataset,

  // Add a new tree to the model.
  void AddTree(std::unique_ptr<decision_tree::DecisionTree> decision_tree);

  // Estimates the memory usage of the model in RAM. The serialized or the
  // compiled version of the model can be much smaller.

  // Number of nodes in the model.

  bool CheckStructure(

  // Number of trees in the model.

  int num_trees() const override { return NumTrees(); }

  // Number of times each feature is used in the model. Returns a map, indexed
  // by feature index, and counting the number of time a feature is used.
  void CountFeatureUsage(



  // Call the function "callback" on all the leafs in which the example (defined
  // by a dataset and a row index) is falling.
  void CallOnAllLeafs(

  void CallOnAllLeafs(

  void AppendDescriptionAndStatistics(bool full_definition,

  // Append a human readable semi-graphical description of the model structure.
  void AppendModelStructure(std::string* description) const;

  // Call "call_back" on all the nodes of the model.
  void IterateOnNodes(

  void IterateOnMutableNodes(

  void set_winner_take_all_inference(bool value) {

  bool winner_take_all_inference() const { return winner_take_all_inference_; }




  // List the variable importances that can be computed from the model
  // structure.



  // Maximum depth of the model. A depth of "0" means a stump i.e. a tree with a
  // single node. A depth of -1 is only possible for an empty forest.
  int MaximumDepth() const;

  // Minimum number of training observations in a node.
  int MinNumberObs() const;

  // Updates the format used to save the model on disk. If not specified, the
  // recommended format `model::decision_tree::RecommendedSerializationFormat()`
  // is used.
  void set_node_format(const std::optional<std::string>& format) override {

  // Number of nodes pruned during training.

  void set_num_pruned_nodes(int64_t value) { num_pruned_nodes_ = value; }

  absl::Status MakePureServing() override;

  // The proximity between examples is one minus the ratio of shared leaves
  // between the examples. This metric distance is defined by Breiman:
  // https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#prox
  //
  // TODO - b/306591749 - Understand, validate and possibly document the
  // definition of Breiman's proximity as a metric distance.
  absl::Status Proximity(const dataset::VerticalDataset& dataset1,

  absl::Status Distance(const dataset::VerticalDataset& dataset1,

  // Fields related to unit testing.
  struct Testing {
    // If true, the "CheckStructure" method will fail if
    // "global_imputation_is_higher"=True.
    bool force_fail_check_structure_global_imputation_is_higher = false;



  // The decision trees.

  // Whether the vote of individual trees are distributions or winner-take-all.
  bool winner_take_all_inference_ = true;

  // Evaluation of the model computed during training on the out of bag
  // examples.

  // Variable importance.

  // Format used to stored the node on disk.
  // If not specified, the format `RecommendedSerializationFormat()` will be
  // used. When loading a model from disk, this field is populated with the
  // format.

  // Number of nodes trained and then pruned during the training.
  // The classical random forest learning algorithm does not prune nodes.

  absl::Status SerializeModelImpl(model::proto::SerializedModel* dst_proto,

  absl::Status DeserializeModelImpl(

  // Fields related to unit testing.
  struct Testing testing_;

  void ApplyHeaderProto(const proto::Header& header);


// Create a single line string containing the result of the evaluation computed
// by "EvaluateOOBPredictions".

void AddClassificationLeafToAccumulator(

void FinalizeClassificationLeafToAccumulator(

// Add a node prediction to a prediction accumulator for regression.
void AddRegressionLeafToAccumulator(const decision_tree::proto::Node& node,

// Add a node prediction to a prediction accumulator for uplift.
void AddUpliftLeafToAccumulator(const decision_tree::proto::Node& node,




// ===== FILE: yggdrasil_decision_forests/model/random_forest/random_forest_test.cc =====








// Build a forest with two decision trees as follow:
// [a>1]
//   â”œâ”€â”€ [b=0] (pos)
//   â””â”€â”€ [b=1] (neg)
// [a>3]
//   â”œâ”€â”€ [b=2] (pos)
//   â””â”€â”€ [b=1] (neg)
//
// Build the dataset:
// "a" : {0, 2, 4}
// "b" : {1, 2, 1}
void BuildToyModelAndToyDataset(const model::proto::Task task,



  // Create a tree of the form.
  // [a> alpha]
  //   â”œâ”€â”€ [b=beta]
  //   â””â”€â”€ [b=gamma]
  auto create_tree = [&task](const float alpha, const int beta,
    auto tree = std::make_unique<decision_tree::DecisionTree>();





      // This test uses the same column as input and label. Note that this might
      // no longer work in the future, at which point this test will be
      // updated.



  int num_calls = 0;













































// ===== FILE: yggdrasil_decision_forests/cli/compute_variable_importances.cc =====


// Computes various variable importances, and add the result to the model
// metadata. The results can be seen with the ":show_model" tool.
//
// See the documentation for the definition of variable importances:
// https://ydf.readthedocs.io/en/latest/cli_user_manual.html#variable-importances
//
// This tool is useful for programmatic analysis of pre-trained model on large
// datasets.
//
// Usage example:
//
// :train ... --dataset=csv:train.csv --output=model
// :compute_variable_importances --input_model=model \
//   --output_model=model_with_vi --dataset=csv:test.csv
// :show_model --model=model
//





void PermutationVI() {
  // Check required flags.






int main(int argc, char** argv) {

// ===== FILE: yggdrasil_decision_forests/cli/edit_model.cc =====


// Edit a model.
//
// Usage example:
//
//   # Change the name of the label
//   bazel run -c opt :edit_model -- \
//     --input=/path/to/original_model \
//     --output=/path/to/final_model \
//     --new_label_name="NEW_LABEL"
//
// The available edit actions are:
//
//   If new_label_name is set:
//     Changes the label's name to --new_label_name. For example, this operation
//     is useful to set a specific label name in a TensorFlow Decision Forests
//     model (as in TF-DF, the label name is always "__LABEL").
//
//   If new_weights_name is set:
//     Changes the weight name to --new_weights_name. For example, this
//     operation is useful to set a specific label name in a TensorFlow Decision
//     Forests model (as in TF-DF, the label name is always "__WEIGHTS").
//
//   If new_file_prefix is set:
//     Changes the model filename prefix (i.e. the prefix string added to all
//     the model filenames) with "new_file_prefix". Set "new_file_prefix" to the
//     empty string (i.e. --new_file_prefix=) to remove the model prefix.
//

// Default string flag values. Used to detect if a flag is set by the user for
// flag where the empty value is possible.





void EditModel() {
  // Check required flags.




  // Change the name of the label.

  // Change the name of the weights.
    auto weights = model->weights();

  // Pure serving

  // Change how the model is exported.


int main(int argc, char** argv) {

// ===== FILE: yggdrasil_decision_forests/cli/infer_dataspec.cc =====


// Infers the dataspec of a dataset.
//
// A dataspec is the list of features, their type and meta data. A dataspec is
// used for all dataset IO operations.
//
// Usage example:
//   bazel run -c opt :infer_dataspec -- \
//     --dataset=csv:data.csv \
//     --output=spec.pbtxt
//
//   You can then visualize "spec.pbtxt" directly or using :show_dataspec.
//






void InferDataspec() {





int main(int argc, char** argv) {

// ===== FILE: yggdrasil_decision_forests/cli/train.cc =====


// Train a ML model and export it to disk.











void Train() {
  // Check required flags.

  // Load configuration protos and the dataspec.





  auto model = learner



int main(int argc, char** argv) {

// ===== FILE: yggdrasil_decision_forests/cli/monitoring/benchmark_training.cc =====


// Benchmark the training speed of a set of models.
//
// Usage example:
// yggdrasil_decision_forests/cli/monitoring/run_benchmark_training.sh







struct Result {

struct SyntheticParams {
  // Number of training examples.
  int num_examples;
  // Number of features.
  // Note: Currently, only numerical features are generated.
  int num_features;
  // Number of label classes
  int num_classes = 2;
  // Resolution of the numerical features. If -1, features have infinite
  // resolution (this is not allowed for DISCRETIZED_NUMERICAL features).
  int resolution = -1;
  // If true, create DISCRETIZED_NUMERICAL features. If false, create NUMERICAL
  // features.
  bool use_discretized_numerical_features = false;


// Utility class for the training of a model.
class Trainer {
    // By default, train on 12 threads.
    // Note: Some experiments will change the number of threads.

  // Loads the dataset in memory, and prepare the training.
  absl::Status PrepareTraining() {

      // Infer the dataspec.


      // Load the dataset in memory.

    // Configure the trainer.


  // Trains a model. Calling this function multiple times will train multiple
  // models.
  absl::Status Run(const std::string& name, std::vector<Result>* results) {
    int idx = results->size() + 1;




    // Record results.



  // Create a synthetic dataset.
  //
  // TODO: Replace with dataset/synthetic_generator.
  absl::Status GenSyntheticDataset(const SyntheticParams& option) {
    // Set dataspec.




    // Link to dataset data.

    // Fill dataset.
        float value = unif_01(random);


    // Config trainer.


  // Accessors to the configuration fields. Should be set before calling
  // "PrepareTraining".

  // Training configuration fields.


// Variations for GBT models.
         // Leave all defaults.
         // Note: Use sorting_strategy=PRESORTED which use PRESORTED except when
         // note interesting.
         // Note: Ideally, PRESORTED should be renamed to AUTO.

      // In node training is the naive way to train individual trees. It is
      // generally very slow (this is why is it not tested by default).

      // Make sure PRESORTED is always used.
      // Note: Ideally, FORCE_PRESORTED should be renamed to PRESORTED.

// Variations for RF models.
         // Leave all defaults.
         // Note: Use sorting_strategy=PRESORTED.

absl::Status Benchmark_RandomForest_Adult(std::vector<Result>* results) {
    // Configure benchmark.



    // Run benchmark.

absl::Status Benchmark_RandomForest_Adult_1Thread(
    // Configure benchmark.


    // Run benchmark.

absl::Status Benchmark_RandomForest_Synthetic(std::vector<Result>* results) {
           // A small dataset with few features.
           // A small dataset with some features.
           // A small dataset with some features and many classes.
           // Having classes is expensive for YDF's GBT implementation.
           // Large datasets with some features.
           // Note: A 2-3 millions examples (depending on the parameters), we
           // observe a point of inflection of the training speed.
      // Configure benchmark.



      // Run benchmark.

absl::Status Benchmark_GBT_Adult(std::vector<Result>* results) {
    // Configure benchmark.



    // Run benchmark.

absl::Status Benchmark_GBT_Adult_NoEarlyStop(std::vector<Result>* results) {
    // Configure benchmark.


    // Run benchmark.


absl::Status Benchmark_GBT_Adult_NoEarlyStop_Oblique(
    // Configure benchmark.


    // Run benchmark.

absl::Status Benchmark_GBT_Adult_Hessian(std::vector<Result>* results) {
    // Configure benchmark.



    // Run benchmark.

absl::Status Benchmark_GBT_Adult_Hessian_NoEarlyStop(
    // Configure benchmark.


    // Run benchmark.

absl::Status Benchmark_GBT_Adult_Discretized(std::vector<Result>* results) {
    // Configure benchmark.


    // Run benchmark.

absl::Status Benchmark_GBT_Adult_Discretized_NoEarlyStop(
    // Configure benchmark.



    // Run benchmark.

absl::Status Benchmark_GBT_Adult_1Thread(std::vector<Result>* results) {
    // Configure benchmark.



    // Run benchmark.

absl::Status Benchmark_GBT_Synthetic(std::vector<Result>* results) {
           // A small dataset with few features.
           // A small dataset with some features.
           // A small dataset with some features and many classes.
           // Having classes is expensive for YDF's GBT implementation.
           // Large datasets with some features.
           // Note: A 2-3 millions examples (depending on the parameters), we
           // observe a point of inflection of the training speed.
           //  {.num_examples = 4'000'000, .num_features = 200},
           //  {.num_examples = 4'000'000, .num_features = 200, .resolution =
           //  1000},
           //  {.num_examples = 4'000'000,
           //   .num_features = 200,
           //   .resolution = 1000,
           //   .use_discretized_numerical_features = true},
      // Configure benchmark.



      // Run benchmark.

// TODO: Generating examples to tfexamples is *very* slow (it takes order
// of magnitude more time that training). Generate examples as YDF dataset to
// YDF examples instead.
absl::Status Benchmark_GBT_SyntheticV2(std::vector<Result>* results) {

  // Disable most features.

    auto synthetic = default_synthetic;

    auto synthetic = default_synthetic;

    auto synthetic = default_synthetic;


      // Configure benchmark.



      // Run benchmark.

absl::Status Benchmark_GBT_Synthetic_Oblique(std::vector<Result>* results) {
      // Configure benchmark.



      // Run benchmark.

absl::Status Benchmark_GBT_Iris(std::vector<Result>* results) {
    // Configure benchmark.



    // Run benchmark.

absl::Status Benchmark_GBT_Abalone(std::vector<Result>* results) {
    // Configure benchmark.



    // Run benchmark.

absl::Status Benchmark_GBT_DNA(std::vector<Result>* results) {
    // Configure benchmark.



    // Run benchmark.


absl::Status ResultsToCsv(const std::vector<Result>& results,


absl::Status Benchmark() {




  // TODO: Enable by default when fast enough.
  // RETURN_IF_ERROR(Benchmark_GBT_SyntheticV2(&results));







int main(int argc, char** argv) {


// ===== FILE: yggdrasil_decision_forests/cli/utils/synthetic_dataset.cc =====


// Create a synthetic dataset.
//
// See yggdrasil_decision_forests/dataset/synthetic_dataset.cc for a description
// of the algorithm used to generate the dataset.
//
// Basic usage example:
//
//   bazel run -c opt :synthetic_dataset -- \
//     --alsologtostderr \
//     --train=csv:/path/to/my/train.csv \
//     --test=csv:/path/to/my/test.csv \
//     --ratio_test=0.2
//
// Advanced usage example:
//
//   // Content of "config.pbtxt"
//   num_examples: 10000
//   missing_ratio: 0.20
//   categorical_vocab_size: 100
//   num_multidimensional_numerical: 10
//
//   bazel run -c opt :synthetic_dataset -- \
//     --alsologtostderr \
//     --options=config.pbtxt\
//     --train=csv:/path/to/my/train.csv \
//     --valid=csv:/path/to/my/valid.csv \
//     --test=csv:/path/to/my/test.csv \
//     --ratio_valid=0.2 \
//     --ratio_test=0.2
//











void SyntheticDataset() {


int main(int argc, char** argv) {

// ===== FILE: yggdrasil_decision_forests/dataset/data_spec.h =====


// This library contains utility functions for the manipulation of dataspecs.






// The "out of dictionary" (OOD) item is a special item used to represent
// infrequent or unknown categorical values.

// Format used to represent discretized numerical values.
// Special value reserved to represent missing value.

// How to represent a Na values in a csv (apart from leaving the field empty).

// Build the mapping from col idx to a given vector of field names.
//
// If "required_columns" is not provided, all the columns are required.
// If "required_columns" is provided, only the columns in "required_columns" are
// required. Missing and non-required column receive the field index -1.
//
// For example, if "required_columns={1,2}" and if column "3" is not in the
// fields (i.e. column  "3" is missing), the column "3 will be considered to be
// filled with missing values. However, if column "1" is not in the fields, and
// error will be raised.
//
// Another example, if "required_columns={}", andy column can be missing. Such
// missing column will be filled with missing values.
absl::Status BuildColIdxToFeatureLabelIdx(

// Returns a sorted list (in increasing order of column idx) of column idxs from
// a list of regular expressions on the column name.
void GetMultipleColumnIdxFromName(

// Returns a column idx from a regular expression on the column name. If none or
// several columns are matching the regular expression, the function fails with
// its error message starting with `error_message_prefix` (if specified).
absl::Status GetSingleColumnIdxFromName(

// Converts a single row from a csv into an Example.
// If col_idx_to_field_idx[i] == -1, all the values of the i-th column are
// replaced by empty values.
absl::Status CsvRowToExample(const std::vector<std::string>& csv_fields,

// Converts a proto::Example into an array of string that can be saved in a csv
// file. The output "csv_fields[i]" is the string representation of the "i-th"
// column of "example".
absl::Status ExampleToCsvRow(const proto::Example& example,

// Returns the index of the column with the corresponding name. Raise an error
// if the column does not exist.
int GetColumnIdxFromName(absl::string_view name,

// Returns the index of the column with the corresponding name. Returns  an
// error if the column does not exist.

// Test if the dataspec contains this column.
bool HasColumn(absl::string_view name,

// Test if a particular attribute value is NA -- "Not Available", also
// referred as "missing".

// Returns a human readable representation of the dataspec. Easier to read and
// more informative than proto's DebugString() method.
                               bool sort_by_column_names = false);

// Returns the integer representation of a categorical value provided as a
// string.
//
// TODO: Remove this version when external protobuffer will support
// map query with absl::string_view.


// Similar to "CategoricalStringToValueWithStatus" above, but only work for
// non-integerized values.

// Tokenize a string. "tokens" is cleared before being filled.
absl::Status Tokenize(const absl::string_view text,

// Extract a ngrams of tokens from a list of token i.e. extracts all the
// sub-sequences of length "n" from "tokens". Append "separator" in between the
// items.
void ExtractNGrams(const std::vector<std::string>& tokens, const int n,

// Returns a string representation of a categorical value.
                                           bool add_quotes = false);

// Returns a string representation of a list of categorical values. If more than
// "max_values" are available, only print the first "max_values" and "..[xyz
// left]". Example: "b, c,...[1 left]". If "max_values==-1", all the elements
// are printed.
    int max_values, const absl::string_view separator = ", ");

// Add a column with specified name and specified type to a dataspec.

// Adds a numerical column.

// Adds a boolean column.

// Adds a categorical column with a vocabulary.
// The Out-of-vocabulary item is added automatically i.e. the created column has
// "vocab.size() + 1" items.

// Adds a categorical-set column with a vocabulary.
// The Out-of-vocabulary item is added automatically i.e. the created column has
// "vocab.size() + 1" items.

// Is this column type multidimensional?
bool IsMultiDimensional(proto::ColumnType type);

// Is this column categorical?
bool IsCategorical(proto::ColumnType type);

// Is this column numerical?
bool IsNumerical(proto::ColumnType type);

// Converts a discretized numerical value into a numerical value.

// Determines a set of histogram-like boundaries of a discretized numerical
// features.
//
// For example, if "candidates" is a list of more-of-less uniform values between
// 0 and 50, and if "maximum_num_bins"=5, the results will be the following 4
// boundaries: 10, 20, 30, 40 representing the 5 bins: ]-inf, 10[, [10,20[, [20,
// 30[, [30, 40[, [40, +inf[.
//
// The function is mostly an heuristic that assigns bins proportionally to the
// density of values. See the arg for details on the other logics.
//
// Args:
//  candidates: Pairs of <value,count> sorted by value. Values should be unique.
//  maximum_num_bins: Maximum number of bins.
//  min_obs_in_bins: Minimum number of observations (sum of counts) in each
//    bins.
//  special_values: Special values (not necessarily present in "candidates")
//    that require their own bin. E.g. if 5 is in "special_values", and if
//    "candidates" contains values both greater and smaller than 5 (e.g. -3, 1,
//    6, 10) there will be a bin [5-eps, 5+eps[.
    int min_obs_in_bins, const std::vector<float>& special_values);

// Converts a numerical value into a discretized numerical value.

// Escape a feature name to be consumed as a "feature" in a training config.

// Hashing function for HASH columns.


// Name of an unrolled column.

// Generate the unrolled column names following the naming convention used in
// PYDF. This is the convention used in all new codes.
// See "unrolled_feature_names" in "port/python/ydf/dataset/io/dataset_io.py"
                                                int num_dims);



// ===== FILE: yggdrasil_decision_forests/dataset/synthetic_dataset.cc =====


// Generation of synthetic datasets suited for Yggdrasil Decision Forests.
//
// Features
//   - Classification and regression.
//   - Numerical, categorical {str, int}, categorical-set {str,int}, and boolean
//     features.
//   - Conditional independence of the label and features e.g. patters such as
//     the label and features {F1} are independent under a condition
//     controlled by features {F2}.
//   - Deterministic generation (if using a fixed seed).
//   - Each type of feature contributes more-or-less equally to the label. This
//     impact can be see by looking at the variable importance of a correctly
//     configured learner.
//   - The contribution of a feature is indicated by its index. For example,
//     feature_0 contributes more than feature_1, that contributes more that
//     feature_2.
//
// Algorithm
//   - Each feature is attached to one of few (e.g. 5) accumulators.
//   - The value of an accumulator (for a given example) is defined as the
//     weighted sum of the features attached to it. For categorical and
//     categorical-set features, a random (or fixed) numerical value is attached
//     to each element in the dictionary.
//   - While a feature is used in an accumulator, it can be randomly masked from
//     being in the final examples to simulate missing values.
//   - For a given example, the i-th accumulator rank is the rank of accumulator
//     compared to all the others examples. This rank is stored in a
//     "noisy_accumulator_rank" variable to indicate that noise is added to this
//     rank computation.
//   - One of the accumulator rank define the label value. The label is directly
//     the rank in the case of regression. For classification, each class is
//     assigned a contiguous set of rank value e.g. ranks{1..4}->Class1,
//     ranks{4..7}->Class2, etc. For ranking, the relevance is the label^2 with
//     a random bias and scale constant within each group.
//   - The selection of the accumulator used for the label is defined by a
//     binary decision tree using the other accumulator ranks.
//




// Short string representation by type.

// Scaling applied to numerical values stored as integer.

// Example in creation.
struct Example {
  // Example that will be serialized ultimately.

  // Numerical accumulators, each defined as a weighted sum of a random subset
  // of features values (with or without an activation function).

  // Rank of the "accumulators" compared to the other examples.
  //
  // For example, without noise, noisy_accumulator_ranks[i] is the number of
  // examples with accumulators[i] smaller that this object accumulators[i].

  // Group of the example. Only used in ranking. -1 iif. not used.
  int group_idx = -1;

// Internal information managed by the generator.
struct GeneratorState {
  // Accumulator where each feature contributes.
  //
  // For example, the i-th numerical feature contributes to the
  // "accumulators[numerical_accumulator_idxs[i]]" accumulator.

// Creates and initializes the random generator.

// Hashing of an integer that is stable for two executions of the library.

// Creates a decreasing weight for a list of features.
float DecreasingWeight(const int feature_idx, const int num_features) {

// Creates a symbol "{base}_{index}".

// Name of a feature.


void SetNumericalFeature(const absl::string_view name, const float value,

void SetCategoricalStringFeature(const absl::string_view name,

void SetCategoricalIntFeature(const absl::string_view name, const int value,

void SetNumericalIntFeature(const absl::string_view name, const int value,

void SetCategoricalSetStringFeature(const absl::string_view name,

void SetCategoricalSetIntFeature(const absl::string_view name,

void AddNumericalFeatures(const proto::SyntheticDatasetOptions& options,
  auto uniform = std::uniform_real_distribution<float>();
    // Generate a random value.

    // Add to accumulator.

    // Record feature value.

void AddMultidimensionalNumericalFeatures(
  auto uniform = std::uniform_real_distribution<float>();
      // Generate a random value.

      // Add to accumulator.

      // Record feature value.

void AddBooleanFeatures(const proto::SyntheticDatasetOptions& options,
  auto uniform = std::uniform_real_distribution<float>();
    // Generate a random value.

    // Add to accumulator.

    // Record feature value.

void AddCategoricalFeatures(const proto::SyntheticDatasetOptions& options,
  auto uniform = std::uniform_real_distribution<float>();
    // Select the accumulator.

    // Generate a random value.

    // Add to accumulator.

    // Record feature value.

void AddCategoricalSetFeatures(const proto::SyntheticDatasetOptions& options,
  auto uniform = std::uniform_real_distribution<float>();
    // Select the accumulator.


    // Generate a random set of values.
      // Is this value included?


      // Add to accumulator.

    // Record feature value.



    // Numerical features.

    // Multidimensional numerical features.

    // Categorical features.

    // Boolean features.

    // Categorical-set features.



absl::Status ComputeAccumulatorRanks(
  auto raw_label_rank_noise =

    // Gather and sort all the accumulator values.

    // Update the rank field of each example.
      // Add some noise to the rank.
      int noisy_rank = rank_idx + raw_label_rank_noise(*rnd);

      // Record the rank.

// Gets the value as a number in [0,1) from the accumulator ranks and values.
float ComputeLabelValue(const Example& example, const int num_examples) {
  // Transverse the accumulators as a heap binary tree.
  int accumulator_idx = 0;


absl::Status CreateLabels(const proto::SyntheticDatasetOptions& options,

  auto raw_label_rank_noise =







// Writes the list of examples to a tensorflow.Example container.
absl::Status WriteTFEExamples(const std::vector<Example>& examples,

// Writes a list of examples to a CSV file.
absl::Status WriteCsvExamples(const std::vector<Example>& examples,
  // Open the output file.

  // List the input features i.e. the csv header.

  // Write header.


  // Write rows.


absl::Status WriteExamples(const std::vector<Example>& examples,
  // Note: We don't use the generic example writer (CreateExampleWriter) because
  // we don't have (and do not want to create) a dataspec.
    // Will fail if the format is not based on tensorflow.Example protos.

  auto accumulator_dist =









absl::Status GenerateSyntheticDataset(
  auto rnd = CreateRandomGenerator(options);

absl::Status GenerateSyntheticDatasetTrainValidTest(



  auto rnd = CreateRandomGenerator(options);

  auto uniform = std::uniform_real_distribution<float>();

  // Random generator seeded with the example group. Only used for ranking.

    // Destination of the example among train, valid and test.
    float dst;

    // Ensure that examples from the same group are written to the same
    // destination.






  // Create dataspec.


  // Index raw data.


  // Populate columns.
  int num_positive_labels = 0;
    int label = 0;

    // Generate between 0 and 5 random vectors for each feature.

    // Set the label.




// ===== FILE: yggdrasil_decision_forests/dataset/synthetic_dataset.h =====






// Generates a synthetic dataset.
// See "synthetic_dataset.cc" header.
absl::Status GenerateSyntheticDataset(

// Generates three datasets with the same underlying patters. The terms "train",
// "valid" and "test" are indicative as there are no differences between those
// datasets. "typed_path_valid" can be empty iif. "ratio_valid==0".
//
// The ratio of examples in the train dataset is "1 - ratio_valid - ratio_test".
absl::Status GenerateSyntheticDatasetTrainValidTest(

// Methods used for testing.

// Creates a synthetic dataset with vector sequence input features. Used for
// unit testing.
struct VectorSequenceSyntheticDatasetOptions {
  int seed = 1234;
  int num_features = 1;
  int vector_dim = 5;
  int num_examples = 10000;

  // Note: The label is true when there is a point within "distance_limit" unit
  // of (0.5, 0.5) in any of the vector-sequence features.
  float distance_limit = 0.5;





// ===== FILE: yggdrasil_decision_forests/dataset/types.cc =====





absl::Status CheckNumExamples(size_t num_examples) {


// ===== FILE: yggdrasil_decision_forests/dataset/types.h =====






// "ExampleIdx" is a signed integer able to store the number of examples in a
// training dataset.
//
// ExampleIdx is controlled by the --define=ydf_example_idx_num_bits={32,64}
// flag. See the documentation of this flag for more details.

// Checks at runtime that the number of examples is compatible with
// "SignedExampleIdx".
absl::Status CheckNumExamples(size_t num_examples);


// Alias in "model" namespace.



// ===== FILE: yggdrasil_decision_forests/dataset/weight.cc =====







absl::Status GetLinkedWeightDefinition(

      auto cat_value_idx_2_weight =

      // The OOD item has a weight of 1 if not specified by the user.

float GetWeight(const VerticalDataset& dataset, VerticalDataset::row_t row,

float GetWeight(const proto::Example& example,


      // "weight_col" is the data about the categorical attribute that controls
      // the weighting of example i.e. the vector of value used for weighting
      // indexed by the row index. These values are categorical and they are
      // indexed with integers. the mapping from these integer to their value is
      // done with the column_spec dictionary.
      //
      // "categorical_value_idx_2_weight" maps the categorical value indices
      // (i.e. what "weight_col" contains) to the float weight values.
      //
      // Example:
      //   std::vector<int> weight_col = {1,2,1}
      //   column_spec_dictionary = {0 : "riri", 1: "fifi", 2: "loulou"}
      //   std::vector<float> categorical_value_idx_2_weight = {2.f,3.f,4.f}
      //
      // In this example, the weight of the first example is 3, and the second
      // example is 4.

absl::Status GetWeights(const VerticalDataset& dataset,

absl::Status GetWeights(
    // Check if all values are identical.


// ===== FILE: yggdrasil_decision_forests/dataset/weight.h =====






// Obtains the "linked weight definition" from a "weight definition". A  "weight
// definition" is a user input. The "linked weight definition" is an equivalent
// representation, attached to a given dataspec, and optimized for fast code
// inference (e.g. categorical values stored as string a converted into
// indices).
absl::Status GetLinkedWeightDefinition(

// Reverses "GetLinkedWeightDefinition".

// Get the weight of an example from a vertical dataset.

// Get the weight of a proto::Example.

// TODO: Update.
float GetWeight(const VerticalDataset& dataset, VerticalDataset::row_t row,
float GetWeight(const proto::Example& example,

// Get the weights of all the examples in a vertical dataset.
absl::Status GetWeights(const VerticalDataset& dataset,

// Helper function on top of GetWeights(weight_definition). If
// "weight_definition" is not set, all the weights are set to 1.
//
// If `use_optimized_unit_weights` is set, `weights` becomes an empty vector if
// and only if no weight definition is given or all given weights are equal
// to 1.
absl::Status GetWeights(
    bool use_optimized_unit_weights = false);



// ===== FILE: yggdrasil_decision_forests/utils/fold_generator.cc =====






absl::Status GenerateFoldsTrainTest(const proto::FoldGenerator& generator,

  // Assign the examples to either the training or the testing fold.

absl::Status GenerateFoldsCrossValidationWithoutGroups(

absl::Status GenerateFoldsCrossValidationWithGroups(
  // Group the rows per group.





  // Shuffle the groups.
  // Note: The shuffle is deterministic i.e. running it twice on the same data
  // should produce the same result. std and absl maps have no deterministic
  // iteration (in practice std map are often implemented deterministically).


  // Randomly assign the examples in each group to a fold.
    // The fold indices should remain sorted.

absl::Status GenerateFoldsCrossValidation(

// Merge the training dataset (already in "dataset") and the test dataset
// specified by the proto.
//
// The test dataset is set to fold 0. The training dataset is set to fold 1.
// The final dataset is composed of the testing dataset examples followed by the
// training dataset examples.
absl::Status GenerateFoldsTestOnOtherDataset(







// Generates folds from a .csv file containing the fold index of each example.
absl::Status GenerateFoldsPrecomputedCrossValidation(
  int num_folds = *std::max_element(fold_values.begin(), fold_values.end()) + 1;

absl::Status GenerateFoldsNoTraining(const proto::FoldGenerator& generator,


int NumberOfFolds(const proto::FoldGenerator& generator,
    // By default, we performs a cross-validation.


absl::Status GenerateFolds(const proto::FoldGenerator& generator,
      // By default, we performs a cross-validation.

absl::Status GenerateFoldsConstDataset(const proto::FoldGenerator& generator,
      // By default, we performs a cross-validation.



absl::Status ExportFoldsToCsv(const FoldList& folds, absl::string_view path) {


  // For each fold, contains the index to the next example index to iterate
  // i.e. folds[j][next_examples[j]] is the next examples to iterate in the
  // fold "j".

    int next_fold_idx = -1;

  // Ensure that all the examples have been scanned.


// ===== FILE: yggdrasil_decision_forests/utils/fold_generator_test.cc =====








class FoldGenerator : public ::testing::Test {
  void SetUp() override { LoadAdultDataset(&dataset_); }

  void LoadAdultDataset(dataset::VerticalDataset* dataset) {

  void CheckFoldValidity() {
      // Test if sorted.
        // Test row idx validity.
        // Test uniqueness.
    // Test coverage.


    // Test of grouping
      // Ensure that each group only appear in only one fold.
      // All the groups.
      // Groups present in each fold.

        int number_of_folds_with_group = 0;

    // Test the exported csv
      int fold_idx;





// Fold grouping on the "education" attribute.

// Incorrect fold grouping on a non existing attribute.

// Incorrect fold grouping on a non-categorical attribute.

// Incorrect fold grouping on a categorical attribute without enough unique
// values.








// ===== FILE: yggdrasil_decision_forests/utils/hyper_parameters.cc =====










bool HyperParameterIsBoolean(


// ===== FILE: yggdrasil_decision_forests/utils/hyper_parameters.h =====






// Helper function to consume generic hyper-parameters.
class GenericHyperParameterConsumer {

  // Returns a hparam if present.

  // Ensures that all the fields have been consumed.
  // Returns OK if all the hyper-parameters have been consumed.
  // Returns a InvalidArgumentError if at least one of the hyper-parameter has
  // not been consumed.
  absl::Status CheckThatAllHyperparametersAreConsumed() const;

  // Set of hyper-parameters.
  // Already consumed hyper-parameters.

// Tests if the default value of a field satisfy a condition.

// Tests if a field is boolean.
bool HyperParameterIsBoolean(



// ===== FILE: yggdrasil_decision_forests/utils/test_utils.cc =====







// Shuffles a dataset randomly. Does not rely on a static seed.
void ShuffleDataset(dataset::VerticalDataset* dataset) {

// Generates a random seed. Does not rely on a static seed.

// Generates a deterministic sequence of boolean value approximating poorly a
// binomial distribution sampling.
class DeterministicBinomial {

  bool Sample() {
      // Always return false first, unless the rate is 1.


  float rate_;
  int num_pos_ = 0;
  int num_total_ = 0;


void TrainAndTestTester::ConfigureForSyntheticDataset() {
  auto cat_int = guide_.add_column_guides();

  auto cat_set_int = guide_.add_column_guides();




void TrainAndTestTester::TrainAndEvaluateModel(
  // TODO: Remove "emulate_weight_with_duplication" argument.


void TrainAndTestTester::PrepareDataset(
  // Path to dataset(s).

  // Build dataspec.

  // Check and update the configuration.
  float max_numerical_weight_value;

  // Instantiate the train and test datasets.

void TrainAndTestTester::TrainModel(
  // Configure the learner.




  // Apply custom loss if necessary.
    auto gbt_config = train_config_.GetExtension(




  // Train the model.
    // Export the training dataset into a set of sharded files.
      // Export the validation dataset into a set of sharded files.





absl::Status TrainAndTestTester::PostTrainingChecks() {
  // Evaluate the model.

  // Print the model evaluation.
  // LOG(INFO) << "Evaluation:\n" << evaluation_description;

  // Export the evaluation to a html file.


  // Save model to disk.

            // Note: On small dataset, the accuracy can change if the prediction
            // value for one example is near the decision boundary.
            //
            // Note: In the next test (see "TestGenericEngine"), we ensure that
            // predictions are equal with a margin of 0.0002.
            // No metrics

  // Evaluate the model saved to disk.

  // Test that the exported model evaluation is the same as the original model
  // evaluation.

  // Model serialization / deserialization.

  // Ensure that the predictions of the semi-fast engine are similar as the
  // predictions of the generic engine.

  // Evaluation with disabled semi-fast engine.

  // Test that the pure version of the model is equal to the non-pure version.

absl::Status TrainAndTestTester::TestModelSerialization() {




  // Infer the dataspec.
  // LOG(INFO) << "Dataspec:\n"
  //               << dataset::PrintHumanReadable(data_spec, false);

void TrainAndTestTester::FixConfiguration(


void TrainAndTestTester::BuildTrainValidTestDatasets(

        // Split deterministically, 1/2 of the examples in training, 1/2 of the
        // examples for validation.




  // Split the dataset in two folds: training and testing.


  // TODO: Make deterministic.

    // Down-sampling of examples.

    // Down-sampling of examples according of a numerical attribute.







void TestGenericEngine(const model::AbstractModel& model,
  auto engine_or = model.BuildFastEngine();
  auto engine = std::move(engine_or.value());





void ExpectEqualPredictions(const dataset::VerticalDataset& dataset,

    // Extract a set of examples.


    // Generate the predictions of the engine.

    // Check the predictions against the ground truth inference code.

void ExpectEqualPredictions(const model::proto::Task task,






void ExpectEqualPredictions(
  // Maximum difference between two supposedly equal values.

  // Container of predictions from the ground truth engine.


  // Current prediction being checked.
  int prediction_idx = 0;


    // Compute the prediction with the generic engine.

        // Determine the format of the predictions.
        bool compact_format;

          // Generic predictions.
          // Precomputed predictions.



        // Precomputed predictions.



void TestPredefinedHyperParameters(

  // Retrieve the preconfigured parameters.


    // Configure a learner

    // Train a model.

    // Evaluate the model.

void TestPredefinedHyperParametersAdultDataset(
  // Create a dataspec.



void TestPredefinedHyperParametersRankingDataset(


  // Create a dataspec.




  // Down-sample the number of examples.

  // If "create_empty_shard=true", shard #4 is empty, and shard #5 contains
  // twice the number of examples of other shards.


      // Empty shard

absl::Status ExportUpliftPredictionsToTFUpliftCsvFormat(




void InternalExportMetricCondition(const absl::string_view test,
  // Margin of error when comparing golden metric values.


  bool success_margin = abs_diff_margin < margin;
  bool success_golden = abs_diff_golden < kGoldenMargin;

    // Export metric to csv file.


int GetVariableImportanceRank(

absl::Status ExpectEqualGoldenModelWithStatus(



void ExpectEqualGoldenModel(const model::AbstractModel& model,




// ===== FILE: yggdrasil_decision_forests/utils/test_utils.h =====


// Utility functions for end-to-end testing of the model training, evaluation,
// serialization and fast-engine-inference.
//
// One such test should be applied for each learning algorithm and learning
// algorithm variant (e.g. a new hyper-parameter).
//
// Usage example:
//
//   class MyLearningAlgorithm : public utils::TrainAndTestTester {
//     void SetUp() override {
//       train_config_.set_learner(MyLearningAlgorithm::kRegisteredName);
//       train_config_.set_task(model::proto::Task::CLASSIFICATION);
//       train_config_.set_label("LABEL");
//       dataset_filename_ = "dna.csv";
//     }
//   };
//
//   TEST_F(MyLearningAlgorithm, MyConfiguration) {
//     TrainAndEvaluateModel();
//     EXPECT_NEAR(metric::Accuracy(evaluation_), 0.9466, 0.01);
//     EXPECT_NEAR(metric::LogLoss(evaluation_), 0.2973, 0.04);
//   }
//





// Train, test and run many checks on a model (e.g., check equality of
// predictions of various engines, save and restore a model from disk).
// This utility class can also be used on a pre-trained model.
class TrainAndTestTester : public ::testing::Test {
  // Runs the full checking.
  // Prepare the dataset, trains, evaluates, serialized & deserialization (save
  // and load a model to disk [directory format], or save and load a model from
  // a sequence of bytes [byte sequence format]) + tests predictions, and check
  // the equality of the predictions from the different inference
  // implementations (e.g., slow engine, all available fast engines).
  //
  // This method should be called after "train_config_" is set. Once this
  // function returns, "evaluation_" contains the result of the evaluation,
  // "training_duration_" contains the duration of the training, and "model_"
  // contains the model.
  //
  // TrainAndEvaluateModel := PrepareDataset + TrainModel + PostTrainingChecks.
  void TrainAndEvaluateModel(
      bool emulate_weight_with_duplication = false,

  // Prepare the dataset.
  void PrepareDataset(

  // Train the model.
  void TrainModel(

  // Run checks on an already trained model.
  absl::Status PostTrainingChecks();

  // Configure the test to run on the synthetic dataset generator.
  void ConfigureForSyntheticDataset();

  // Directory containing the dataset used in the test.


  // Filename of the dataset. The full dataset path will be
  // Join(dataset_root_directory_, dataset_filename_). If empty, generates a
  // synthetic dataset.

  // Filename of the test dataset. If not specified, the dataset
  // "dataset_filename_" is split into a training and a testing dataset.
  // If "dataset_test_filename_" is specified, all of "dataset_filename_" is
  // used for training, and "dataset_test_filename_" is used for testing.

  // Options to generate a syntheteic dataset when "dataset_filename_" is empty.

  // Filename of the dataspec guide. The full guide path will be
  // Join(dataset_root_directory_, guide_filename_). If empty, no guide will
  // be used.

  // Training configuration to train the model.

  // Generic hyper-parameters to train the model.

  // Deployment configuration to train the model.

  // Result of the evaluating the model on the test dataset.

  // If set, overrides the type used in the model evaluation.

  // Learner.

  // Options of the model evaluation.

  // Percentage of the dataset used for the train/test.
  float dataset_sampling_ = 1.f;

  // Dataspec.

  // The trained model.

  // Duration of training of the model.

  // Directory name containing the model, evaluation and training logs.

  // Train and testing datasets.

  // Ratio of the original dataset going into the training fold. The remaining
  // examples are uniformly split between the test and valid dataset (is
  // "pass_validation_dataset_=true").
  //
  // If the value if 0.5f (default), the examples are split deterministically
  // according to their index : even examples are used for training, odd
  // examples are used for test/validation.
  float split_train_ratio_ = 0.5f;

  // Format and extension used to store the temporary dataset generated during
  // the test. The reader and writer of this format need to be registered.

  // If true, the dataset is passed to the learner as a path. If false, the
  // dataset is passed to the learner as a VerticalDataset.
  bool pass_training_dataset_as_path_ = false;

  // Number of shards to use if "pass_training_dataset_as_path_" is set to true.
  int num_shards_ = 3;

  // If set, interrupts the training after "interrupt_training_after".

  // If true, the model is checked and the implementation is checked for
  // potential issues e.g. serializing+deserializing, creation of serving
  // engines.
  bool check_model = true;

  // If true, the training method is called with a validation dataset (either a
  // path or a VerticalDataset; depending on "pass_training_dataset_as_path_");
  bool pass_validation_dataset_ = false;

  // If true, show the entire model structure (e.g. show the decision trees) in
  // the logs.
  bool show_full_model_structure_ = false;

  // If true, shuffle the datasets in unit tests.
  bool inject_random_noise_ = false;

  // If true, randomize learner seeds in unit tests.
  bool change_random_seed_ = false;

  // If set, specifies the custom loss used.

  // If true, tests if the model can be serialized / deserialized.
  bool test_model_serialization_ = true;



  void FixConfiguration(

  void BuildTrainValidTestDatasets(

  // Serialize the model to std::string, deserialize it, and check the equality
  // of the original and deserialized model.
  absl::Status TestModelSerialization();

// Tests the prediction of the (slow) generic engine and the fast generic
// engine. If the model does not implement a fast generic engine, the model
// succeed.
void TestGenericEngine(const model::AbstractModel& model,

// Checks the predictions of an engine vs the slow generic engine.
void ExpectEqualPredictions(const dataset::VerticalDataset& dataset,

// Checks the predictions of the slow generic engine vs "predictions".
void ExpectEqualPredictions(const dataset::VerticalDataset& dataset,

// Checks the predictions of a templated engine vs the slow generic engine.
template <typename Engine,
          void (*PredictCall)(const Engine&, const typename Engine::ExampleSet&,
void ExpectEqualPredictionsTemplate(const dataset::VerticalDataset& dataset,

// Checks the predictions of a templated engine with the old API vs the slow
// generic engine.
template <typename Engine,
          void (*PredictCall)(const Engine&,
void ExpectEqualPredictionsOldTemplate(

// Checks that two predictions are equivalent.
void ExpectEqualPredictions(const model::proto::Task task,

template <typename Engine,
          void (*PredictCall)(const Engine&, const typename Engine::ExampleSet&,
void ExpectEqualPredictionsTemplate(const dataset::VerticalDataset& dataset,


    // Extract a set of examples.


    // Generate the predictions of the engine.

    // Check the predictions against the ground truth inference code.

template <typename Engine,
          void (*PredictCall)(const Engine&,
void ExpectEqualPredictionsOldTemplate(


    // Extract a set of examples.


    // Generate the predictions of the engine.

    // Check the predictions against the ground truth inference code.

// Trains and tests a model for each possible predefined hyper-parameters
// values.
void TestPredefinedHyperParameters(

// Runs "TestPredefinedHyperParameters" on the adult dataset.
void TestPredefinedHyperParametersAdultDataset(

// Runs "TestPredefinedHyperParameters" on the synthetic ranking dataset.
void TestPredefinedHyperParametersRankingDataset(
    int expected_num_preconfigured_parameters,

// Randomly shards a dataset. Returns the sharded path in the temp directory.
                         int num_shards, float sampling,

// Exports the predictions of a binary treatment uplift model to a csv file with
// the columns: "uplift", "response", "weight", "group".
absl::Status ExportUpliftPredictionsToTFUpliftCsvFormat(

// Internal implementation of "YDF_TEST_METRIC".
void InternalExportMetricCondition(absl::string_view test, double value,

// Gets the name of the current test.
template <typename T>
  int status;
    auto last_idx = result.find_last_of("::");

// Returns the rank of importance of an attribute.
int GetVariableImportanceRank(


// Checks that "value" is in [center-margin, center+margin] (margin test) and
// equal to "golden". If "kYdfTestMetricCheckGold=False" or if "golden=NaN",
// only check the margin test.
//
  ::yggdrasil_decision_forests::utils::InternalExportMetricCondition(        \
      ::yggdrasil_decision_forests::utils::InternalGetTestName(this), value, \

// TODO: Simplify protocol.
//
// The following block allows to export unit-test evaluation metrics to csv
// files, to then analyse the distribution of metrics in a notebook, and
// possibly update valid margins.
//
// If "kYdfTestMetricDumpDir" is set, the result of unit test metrics
// tested with "YDF_TEST_METRIC" are exported to csv files in the
// directory specified by "kYdfTestMetricDumpDir" (Note: The directory
// should already exist) and the tests become non-failing (i.e., if a
// metric is not in a valid range, the test does not fail).
//
// YDF training is deterministic modulo changes in implementation of the random
// generator (or equivalent, e.g. change of default random seed, change of query
// order of the random generator) and floating point compiler optimizations.
// Stability of unit tests to random seeds can be tested with
// "change_random_seed_=True" in conjunction with value for "--runs_per_test"
// e.g. "--runs_per_test=100".
//

// If set, export metrics to disk, and disable metric unit tests.
// To enable logging of unit test metrics.
// constexpr char kYdfTestMetricDumpDir[] = "/tmp/metric_condition";



// If kYdfTestMetricCheckGold=true, checks that "model" is equal to the model
// stored in
// "yggdrasil_decision_forests/test_data/golden/<model_name>". The
// model meta-data is not compared. If kYdfTestMetricCheckGold=false, does
// nothing.
void ExpectEqualGoldenModel(const model::AbstractModel& model,



// ===== FILE: yggdrasil_decision_forests/utils/time.h =====






// Converts a duration into a string compatible with the training logs (i.e.,
// limited precision on the seconds, only use hours, minutes and seconds).



// ===== FILE: yggdrasil_decision_forests/utils/usage.h =====


// Tracks learner and model usage for accounting.
//
// Those methods are called whenever the corresponding action occurs. For
// example, "OnTrainingStart" is called whenever the training of a new model
// starts.





// Start a new model training.
// Should be called at the start of the "Train" methods of learners.
void OnTrainingStart(const dataset::proto::DataSpecification& data_spec,

// Complete a model training.
// Should be called at the end of the "Train" methods of learners.
void OnTrainingEnd(const dataset::proto::DataSpecification& data_spec,

// Inference of a model.
//
// The inference on model containing other models (e.g. the ensembler or the
// calibrator) might or might not be counted multiple times depending on the
// specific model implementation.
//
// TODO: Merge the two functions when model::Metadata is removed.
void OnInference(int64_t num_examples, const model::proto::Metadata& metadata);
void OnInference(int64_t num_examples, const model::MetaData& metadata);

// When a dataset is loaded for training, inference, or other operations.
void OnLoadDataset(absl::string_view path);

// When a dataset is saved.
void OnSaveDataset(absl::string_view path);

// When a model is loaded for training, inference, or other operations.
void OnLoadModel(absl::string_view path);

// When a model is saved.
void OnSaveModel(absl::string_view path);

// Enables / disable usage tracking.
void EnableUsage(bool usage);



// ===== FILE: yggdrasil_decision_forests/utils/usage_default.cc =====





void OnTrainingStart(const dataset::proto::DataSpecification& data_spec,
  // Add usage tracking here.

void OnTrainingEnd(const dataset::proto::DataSpecification& data_spec,
  // Add usage tracking here.

void OnInference(const int64_t num_examples,
  // Add usage tracking here.

void OnInference(int64_t num_examples, const model::MetaData& metadata) {
  // Add usage tracking here.

void OnLoadDataset(absl::string_view path) {}

void OnSaveDataset(absl::string_view path) {}

void OnLoadModel(absl::string_view path) {}

void OnSaveModel(absl::string_view path) {}

void EnableUsage(bool usage) {}
